{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>深度学习框架——多层神经网络实践</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 课程内容\n",
    "\n",
    "* 1.深度学习初探\n",
    "* 2.多层神经网络构建\n",
    "* 3.多层神经网络实践\n",
    "* 4.Keras使用介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 1.深度学习初探"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 可视化初识神经网络\n",
    "\n",
    "PlayGround是一个在线演示、实验的神经网络平台，是一个入门神经网络非常直观的网站，将神经网络的训练过程直接可视化。\n",
    "\n",
    "PlayGround的网址：http://playground.tensorflow.org/\n",
    "\n",
    "* PlayGround的页面如图所示，主要分为DATA（数据），FEATURES（特征），HIDDEN LAYERS（隐含层），OUTPUT（输出层）。\n",
    "\n",
    "![](./images/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 深度学习框架选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习框架入门选择\n",
    "![](./images/dl_frameworks_power_scores.png)\n",
    "<center>Deep learning 框架排名，由 Jeff Hale 基于 7 个分类的 11 个数据源计算得出</center>\n",
    "\n",
    "Keras和PyTorch是深度学习的开源框架，深受数据科学家的欢迎。其中：\n",
    "\n",
    "* Keras是一种能够在TensorFlow、CNTK、Theano或MXNet（或者在TensorFlow中运行tf.contrib）上运行的高级API。自2015年3月首次发布以来，它因易于使用和语法简单而备受青睐，使开发更加快速。由Google的支持。\n",
    "* PyTorch于2016年10月发布，是一款专注于直接使用数组表达式的低级API。它在过去的一年里备受瞩目，成为学术研究的首选解决方案，以及需要优化自定义表达式的深度学习应用。由Facebook支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras对比PyTorch：易用性和灵活性\n",
    "Keras和PyTorch在操作抽象级别方面不同。\n",
    "\n",
    "* Keras是一个更高级的框架，将常用的深度学习层和操作包装到简洁的积木式的构建块中，将深度学习的复杂性从数据科学家的眼前抽象出来。\n",
    "\n",
    "* PyTorch为实验提供了一个相对低级的环境，使用户可以更自由地编写自定义层并查看数值优化任务的底层。当你可以使用Python的全部功能并访问所使用的所有函数的内核时，可以更直接的开发更复杂的架构。当然，代价是代码冗长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们思考一下，在Keras和PyTorch中定义一个简单的卷积网络的详细对比："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "```py\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "```py\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 6 * 6)\n",
    "        x = F.log_softmax(self.fc1(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码片段展示了两种框架之间的差异。至于模型训练本身 – 在PyTorch中需要大约20行代码，而Keras只需一行代码。\n",
    "\n",
    "如果你是初学者，Keras的这种高级可能看起来是一个明显的优势。Keras确实更具可读性和简洁性，使你可以更快地构建自己的第一个端到端深度学习模型，同时跳过实现细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么要用框架实现神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用Numpy实现两层神经网络\n",
    "\n",
    "![](./images/2ln.jpg)\n",
    "\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias，用来从x预测y。\n",
    "- $h = W_1X + b_1$\n",
    "- $a = max(0, h)$\n",
    "- $y_{hat} = W_2a + b_2$\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络、loss和反向传播。\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造64组x（1000维）,y（10维）数据\n",
    "import numpy as np\n",
    "\n",
    "# N=64 输入个数，D_in=1000输入维度，D_out=10输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算步骤\n",
    "# h = w1x + b1\n",
    "# a = max(0,h)\n",
    "# y = w2 + b2\n",
    "\n",
    "# 初始化第1层权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "# 初始化第2层权重\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "# 定义学习速率（太大，收敛过慢；太小，可能发散）\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    h = x.dot(w1) # N * H\n",
    "    h_relu = np.maximum(h, 0) # N * H\n",
    "    y_pred = h_relu.dot(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用Keras实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# 定义模型\n",
    "model = Sequential()\n",
    "\n",
    "# 定义网络\n",
    "model.add(Dense(H, input_dim=D_in, activation='relu'))\n",
    "model.add(Dense(D_out, activation='sigmoid'))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# 训练模型\n",
    "model.fit(x, y, epochs=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.多层神经网络构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1、构建网络\n",
    "![](./images/keras-1.png)\n",
    "\n",
    "* 2、训练网络\n",
    "![](./images/keras-2.png)\n",
    "\n",
    "* 3、评估网络\n",
    "![](./images/keras-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras的神经网络API是在封装后与使用者直接进行交互的API组件，在使用时可以调用Keras的其它组件。使用者可以通过神经网络API实现机器学习任务中的常见操作，包括人工神经网络的构建、编译、训练、评估、测试等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、模型构建\n",
    "\n",
    "Keras Sequential**序贯模型**是多个网络层的线性堆叠。\n",
    "\n",
    "你可以通过将网络层实例的列表传递给 Sequential 的构造器，来创建一个 Sequential 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# 简单地使用 .add() 方法将各层添加到模型中\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784)) # Dense为全连接层, 32指的是输出个数,下一层节点个数\n",
    "model.add(Activation('relu')) # Activation为激活函数层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/dense.png)\n",
    "\n",
    "指定输入数据的尺寸\n",
    "\n",
    "模型需要知道它所期望的输入的尺寸。出于这个原因，顺序模型中的第一层（且只有第一层，因为下面的层可以自动地推断尺寸）需要接收关于其输入尺寸的信息。有几种方法来做到这一点：\n",
    "\n",
    "* 传递一个 input_shape 参数给第一层。它是一个表示尺寸的元组 (一个整数或 None 的元组，其中 None 表示可能为任何正整数)。在 input_shape 中不包含数据的 batch 大小。\n",
    "* 某些 2D 层，例如 Dense，支持通过参数 input_dim 指定输入尺寸，某些 3D 时序层支持 input_dim 和 input_length 参数。\n",
    "* 如果你需要为你的输入指定一个固定的 batch 大小（这对 stateful RNNs 很有用），你可以传递一个 batch_size 参数给一个层。如果你同时将 batch_size=32 和 input_shape=(6, 8) 传递给一个层，那么每一批输入的尺寸就为 (32，6，8)。\n",
    "\n",
    "因此，下面的代码片段是等价的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784,))) # model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary() # 查看网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、模型编译\n",
    "在训练模型之前，您需要配置学习过程，这是通过 **compile** 方法完成的。\n",
    "```\n",
    "compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n",
    "```\n",
    "用于配置训练模型。\n",
    "\n",
    "#### 参数\n",
    "\n",
    "* **optimizer**: 字符串（优化器名）或者优化器对象，如 rmsprop 或 adagrad。详见 optimizers。\n",
    "* **loss**: 字符串（目标函数名）或目标函数，如 categorical_crossentropy 或 mse。详见 losses。 如果模型具有多个输出，则可以通过传递损失函数的字典或列表，在每个输出上使用不同的损失。模型将最小化的损失值将是所有单个损失的总和。\n",
    "* **metrics**: 在训练和测试期间的模型评估标准。通常你会使用 metrics = ['accuracy']。 要为多输出模型的不同输出指定不同的评估标准，还可以传递一个字典，如 metrics = {'output_a'：'accuracy'}。\n",
    "* loss_weights: 指定标量系数（Python浮点数）的可选列表或字典，用于加权不同模型输出的损失贡献。 模型将要最小化的损失值将是所有单个损失的加权和，由 loss_weights 系数加权。 如果是列表，则期望与模型的输出具有 1:1 映射。 如果是张量，则期望将输出名称（字符串）映射到标量系数。\n",
    "* sample_weight_mode: 如果你需要执行按时间步采样权重（2D 权重），请将其设置为 temporal。 默认为 None，为采样权重（1D）。如果模型有多个输出，则可以通过传递 mode 的字典或列表，以在每个输出上使用不同的 sample_weight_mode。\n",
    "* weighted_metrics: 在训练和测试期间，由 sample_weight 或 class_weight 评估和加权的度量标准列表。\n",
    "* target_tensors: 默认情况下，Keras 将为模型的目标创建一个占位符，在训练过程中将使用目标数据。相反，如果你想使用自己的目标张量（反过来说，Keras 在训练期间不会载入这些目标张量的外部 Numpy 数据），您可以通过 target_tensors 参数指定它们。它应该是单个张量（对于单输出 Sequential 模型）。\n",
    "* **kwargs: 当使用 Theano/CNTK 后端时，这些参数被传入 K.function。当使用 TensorFlow 后端时，这些参数被传递到 tf.Session.run。\n",
    "\n",
    "#### 异常\n",
    "\n",
    "* ValueError: 如果 optimizer, loss, metrics 或 sample_weight_mode 这些参数不合法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二分类问题，输出层激活函数用sigmoid，一般损失函数选用binary_crossentropy。\n",
    "![](./images/class2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 二分类问题\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy', #损失函数的不同\n",
    "              metrics=['accuracy']) # 数组形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多分类问题，输出层激活函数用softmax，一般损失函数选用categorical_crossentropy。\n",
    "![](./images/classn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 多分类问题\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy', #损失函数的不同\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：损失函数loss的选择，是二分类还是多分类**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、模型训练\n",
    "Keras 模型在输入数据和标签的 Numpy 矩阵上进行训练。为了训练一个模型，你通常会使用 **fit** 函数。\n",
    "```\n",
    "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "```\n",
    "以固定数量的轮次（数据集上的迭代）训练模型。\n",
    "\n",
    "#### 参数\n",
    "\n",
    "* **x**: 训练数据的 Numpy 数组。 如果模型中的输入层被命名，你也可以传递一个字典，将输入层名称映射到 Numpy 数组。 如果从本地框架张量馈送（例如 TensorFlow 数据张量）数据，x 可以是 None（默认）。\n",
    "* **y**: 目标（标签）数据的 Numpy 数组。 如果模型中的输出层被命名，你也可以传递一个字典，将输出层名称映射到 Numpy 数组。 如果从本地框架张量馈送（例如 TensorFlow 数据张量）数据，y 可以是 None（默认）。\n",
    "* **batch_size**: 整数或 None。每次提度更新的样本数。如果未指定，默认为 32.\n",
    "* **epochs**: 整数。训练模型迭代轮次。一个轮次是在整个 x 或 y 上的一轮迭代。请注意，与 initial_epoch 一起，epochs 被理解为 「最终轮次」。模型并不是训练了 epochs 轮，而是到第  epochs 轮停止训练。\n",
    "* verbose: 0, 1 或 2。日志显示模式。 0 = 安静模式, 1 = 进度条, 2 = 每轮一行。\n",
    "* callbacks: 一系列的 keras.callbacks.Callback 实例。一系列可以在训练时使用的回调函数。详见 callbacks。\n",
    "* **validation_split**: 在 0 和 1 之间浮动。用作验证集的训练数据的比例。模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。验证数据是混洗之前 x 和y 数据的最后一部分样本中。\n",
    "* **validation_data**: 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights)，用来评估损失，以及在每轮结束时的任何模型度量指标。模型将不会在这个数据上进行训练。这个参数会覆盖 validation_split。\n",
    "* shuffle: 布尔值（是否在每轮迭代之前混洗数据）或者 字符串 (batch)。batch 是处理 HDF5 数据限制的特殊选项，它对一个 batch 内部的数据进行混洗。当 steps_per_epoch 非 None 时，这个参数无效。\n",
    "* class_weight: 可选的字典，用来映射类索引（整数）到权重（浮点）值，用于加权损失函数（仅在训练期间）。这可能有助于告诉模型 「更多关注」来自代表性不足的类的样本。\n",
    "* sample_weight: 训练样本的可选 Numpy 权重数组，用于对损失函数进行加权（仅在训练期间）。您可以传递与输入样本长度相同的平坦（1D）Numpy 数组（权重和样本之间的 1：1 映射），或者在时序数据的情况下，可以传递尺寸为 (samples, sequence_length) 的 2D 数组，以对每个样本的每个时间步施加不同的权重。在这种情况下，你应该确保在 compile() 中指定 sample_weight_mode=\"temporal\"。\n",
    "* initial_epoch: 开始训练的轮次（有助于恢复之前的训练）。\n",
    "* steps_per_epoch: 在声明一个轮次完成并开始下一个轮次之前的总步数（样品批次）。使用 TensorFlow 数据张量等输入张量进行训练时，默认值 None 等于数据集中样本的数量除以 batch 的大小，如果无法确定，则为 1。\n",
    "* validation_steps: 只有在指定了 steps_per_epoch时才有用。停止前要验证的总步数（批次样本）。\n",
    "\n",
    "#### 返回\n",
    "\n",
    "一个 History 对象。其 History.history 属性是连续 epoch 训练损失和评估值，以及验证集损失和评估值的记录（如果适用）。\n",
    "\n",
    "#### 异常\n",
    "\n",
    "* RuntimeError: 如果模型从未编译。\n",
    "* ValueError: 在提供的输入数据与模型期望的不匹配的情况下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成虚拟数据\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu')) #输入20，输出64\n",
    "# model.add(Dense(64, input_dim=20))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) #输入64，输出1\n",
    "# model.add(Dense(2, activation='softmax')) #二分类变成多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary() #神经网络的层数，从隐藏层开始计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "# 举个例子\n",
    "# 训练集有1000个样本，batchsize=10，那么训练1次epoch，需要100次迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、模型评估\n",
    "在模型完成学习后，Keras提供**evaluate**方法进行模型评估\n",
    "```\n",
    "evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
    "```\n",
    "在测试模式，返回误差值和评估标准值。\n",
    "\n",
    "计算逐批次进行。\n",
    "\n",
    "#### 参数\n",
    "\n",
    "* **x**: 训练数据的 Numpy 数组。 如果模型中的输入层被命名，你也可以传递一个字典，将输入层名称映射到 Numpy 数组。 如果从本地框架张量馈送（例如 TensorFlow 数据张量）数据，x 可以是 None（默认）。\n",
    "* **y**: 目标（标签）数据的 Numpy 数组。 如果模型中的输出层被命名，你也可以传递一个字典，将输出层名称映射到 Numpy 数组。 如果从本地框架张量馈送（例如 TensorFlow 数据张量）数据，y 可以是 None（默认）。\n",
    "* **batch_size**: 整数或 None。每次提度更新的样本数。如果未指定，默认为 32.\n",
    "* verbose: 0, 1。日志显示模式。0 = 安静模式, 1 = 进度条。\n",
    "* sample_weight: 训练样本的可选 Numpy 权重数组，用于对损失函数进行加权（仅在训练期间）。 您可以传递与输入样本长度相同的平坦（1D）Numpy 数组（权重和样本之间的 1：1 映射），或者在时序数据的情况下，可以传递尺寸为 (samples, sequence_length) 的 2D 数组，以对每个样本的每个时间步施加不同的权重。在这种情况下，你应该确保在 compile() 中指定 sample_weight_mode=\"temporal\"。\n",
    "* steps: 整数或 None。 声明评估结束之前的总步数（批次样本）。默认值 None。\n",
    "\n",
    "#### 返回\n",
    "\n",
    "标量测试误差（如果模型只有单个输出且没有评估指标）或标量列表（如果模型具有多个输出和/或指标）。 属性 model.metrics_names 将提供标量输出的显示标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('Test loss:',score[0])\n",
    "print('Test accuracy',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 返回值是一个数组，包括loss和accuracy\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5、模型测试\n",
    "Keras模型可以通过**predict**进行测试。\n",
    "```\n",
    "predict(x, batch_size=None, verbose=0, steps=None)\n",
    "```\n",
    "为输入样本生成输出预测。\n",
    "\n",
    "计算逐批次进行。\n",
    "\n",
    "#### 参数\n",
    "\n",
    "* **x**: 输入数据，Numpy 数组（或者如果模型有多个输入，则为 Numpy 数组列表）。\n",
    "* **batch_size**: 整数。如未指定，默认为 32。\n",
    "* verbose: 日志显示模式，0 或 1。\n",
    "* steps: 声明预测结束之前的总步数（批次样本）。默认值 None。\n",
    "\n",
    "#### 返回\n",
    "\n",
    "预测的 Numpy 数组。\n",
    "\n",
    "#### 异常\n",
    "\n",
    "* ValueError: 如果提供的输入数据与模型的期望数据不匹配，或者有状态模型收到的数量不是批量大小的倍数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=8)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 多层神经网络的二分类\n",
    "# 导入依赖包\n",
    "\n",
    "\n",
    "# 数据准备\n",
    "# 生成虚拟数据\n",
    "\n",
    "\n",
    "# 1、构造模型\n",
    "\n",
    "\n",
    "# 2、编译模型\n",
    "\n",
    "\n",
    "# 3、训练模型\n",
    "\n",
    "\n",
    "# 4、评测模型\n",
    "\n",
    "\n",
    "# 5、预测模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习\n",
    "# 多层神经网络的多分类\n",
    "# 提示：标签需要转换为one-hot编码，keras.utils.to_categorical(labels, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.多层神经网络实践\n",
    "\n",
    "![](./images/Keras2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Classifier分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST手写数字识别\n",
    "\n",
    "MNIST手写数字识别数据集，这是由**Yann LeCun**所搜集的，他也是Convolution Neuarl Networks的创始人。\n",
    "\n",
    "MNIST数据集的数据量并不大，而且是单色的图像，比较简单，很适合深度学习的初学者用来建立模型、训练、预测。\n",
    "\n",
    "MNIST数据集共有60000项训练数据，10000项测试数据。MNIST数据集中的每一项数据images(数字图像)与Labels(真实数字)所组成。\n",
    "\n",
    "每一个MNIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签，每一张图片包含28像素X28像素。\n",
    "![](./images/MNIST1.png)\n",
    "\n",
    "没有接触过图像处理的人可能会很纳闷，从一张图片识别出里面的内容似乎是件相当神奇的事情。其实，当你把图片当成一枚枚像素来看的话，就没那么神秘了。下图为手写体数字1的图片，它在计算机中的存储其实是一个二维矩阵，每个元素都是0~1之间的数字，0代表白色，1代表黑色，小数代表某种程度的灰色。\n",
    "![](./images/MNIST2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.1.1 多层神经网络实战  \n",
    "- 训练  \n",
    "MNIST训练数据集的训练数据总共有60000项，经过数据预处理后会产生Features(数字图像特征值)与Label（数字真实的值），然后输入多层感知器模型进行训练，训练完成后的模型可以作为下一个阶段预测使用。  \n",
    "- 预测  \n",
    "输入数字图像，预处理后会产生Features（数字图像特征值），使用训练完成的多层感知器模型进行预测，最后产生预测结果是0~9的数字。\n",
    "![](./images/neural_net1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先导入keras的各种模块\n",
    "\n",
    "keras.datasets 里面包含了多种常用数据集，如mnist，cifar10等等，可以实现自动下载和解析等等。\n",
    "\n",
    "keras.models 里面有最核心的模型结构，如顺序模型结构Sequential\n",
    "\n",
    "keras.layers 里面有一些常用的层结构，如全连接层Dense\n",
    "\n",
    "keras.optimizers 里面有一些常用优化函数，如adam等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入mnist数据，第一次会自动下载，之后运行会载入本地文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = mnist.load_data() #加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看一下数据格式，训练集一共有6万张，大小是28*28,单通道灰度图，测试集是1万张。标签是列向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看输入的数据内容及形状\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看输出的数据内容及形状\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化一些图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    im = plt.imshow(x_train[i],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于mlp的输入是一维向量，所以要转换\n",
    "\n",
    "将每一幅图像都转换为一个长向量，大小为28*28=784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000,784)\n",
    "x_test = x_test.reshape(10000,784)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看输入的数据内容及形状\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看输出的数据内容及形状\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化，将图像的像素归到0~1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看输入的数据内容及形状\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看输出的数据内容及形状\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将label也转换成One-hot标签，这里直接用keras的预置的一个函数 keras.utils.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_train[0:10])# 查看原始标签 0~9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train,10)\n",
    "y_test = keras.utils.to_categorical(y_test,10)\n",
    "\n",
    "print(y_train[0:10])#查看转换完毕的标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始构建模型，模型分包含一个隐层和一个输出层，都是全连接层，使用Sequential构建\n",
    "\n",
    "其中隐层输出采用ReLU激活函数，Sequential的第一层要指定input_shape，要注意，这里的input_shape 是不包含batch大小的，就只是后面几维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500,activation='relu',input_shape=(784,)))\n",
    "model.add(Dense(10,activation='softmax')) #多分类激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()#这一句用来输出网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编译模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置模型，主要包括\n",
    "\n",
    "* loss：loss计算方法（损失函数）\n",
    "\n",
    "* optimizer：优化函数\n",
    "\n",
    "* metrics：指定哪些量需要在训练及测试中关注，一般都会写accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', #二分类 binary_crossentropy\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练。这里使用的是model对象的fit方法。前两个参数分别是完整的训练数据和训练标签\n",
    "\n",
    "batch_size 表示每一次塞入多少张图片\n",
    "\n",
    "epochs 表示训练几轮\n",
    "\n",
    "verbose 表示用何种方式显示输出信息，0表示不输出，1表示在一直输出更新，2表示每一个epoch才输出一次。\n",
    "\n",
    "validation_data 表示验证集，格式和训练集一样，如果此参数不为空的话，每一个epoch过后就会输出验证集的loss和accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit(x_train,y_train,batch_size=128,epochs=5,verbose=1,\n",
    "         validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "训练过程可视化查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['acc']) #accuracy\n",
    "plt.plot(hist.history['val_acc'])# val_accuracy\n",
    "plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试结果，输出为loss以及其他之前compile模型时指定过的metrics的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test,y_test,verbose=1)\n",
    "print('Test loss:',score[0])\n",
    "print('Test accuracy',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.1.2 多层神经网络优化 \n",
    "\n",
    "##### 1、如何提高多层神经网络模型的准确率？\n",
    "\n",
    "* 1）增加模型深度\n",
    "![](./images/neural_net2.jpeg)\n",
    "\n",
    "* 2）更换损失函数\n",
    "  - mse、categorical_crossentropy\n",
    " \n",
    "* 3）更换激活函数\n",
    "  - sigmoid、relu、tanh\n",
    " \n",
    "![](./images/sigmoid.jpeg)\n",
    "<center> **Sigmoid Activation Function** </center>\n",
    "![](./images/relu.jpeg)\n",
    "<center> **Relu Activation Function** </center>\n",
    "\n",
    "* 4）更换优化器\n",
    "  - SGD、Adam\n",
    "![](./images/opt2.gif)\n",
    "\n",
    "* 5）增加训练次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习\n",
    "# 多层神经网络的调参优化\n",
    "# 1.增加模型深度，改成2个隐藏层，model.add(Dense(500,activation='tanh'))\n",
    "# 2.更换损失函数，mse、categorical_crossentropy\n",
    "# 3.更换激活函数，sigmoid、softmax、relu、tanh\n",
    "# 4.更换优化器，SGD、Adam、RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2、如何防止多层神经网络模型的过拟合？\n",
    "* 1）Dropout\n",
    "![](./images/dropout.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500,activation='relu',input_shape=(784,)))\n",
    "model.add(Dropout(0.2)) ###\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 小结\n",
    "\n",
    "本节主要写了一个最简单的多层感知机模型，目的是熟悉keras最基本的操作。\n",
    "\n",
    "知识点：\n",
    "\n",
    "1. 学习载入Keras中预置的数据库及数据库数据的基本变换\n",
    "1. Sequential模型的定义，以及如何添加层\n",
    "1. 如何对Dense层及Dropout层进行基本的配置\n",
    "1. 学习使用compile对网络进行配置\n",
    "1. 使用fit方法来对小数据库进行训练，这里的小数据库指的是所有数据可以一次性载入到内存\n",
    "1. 使用evaluate方法来对模型进行效果评估\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Regressor回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载模块**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入模块并创建数据 \n",
    "import numpy as np\n",
    "np.random.seed(1337)  # 固定一个随机数种子，保证每次数据一样\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt # 可视化模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create some data\n",
    "X = np.linspace(-1, 1, 200)\n",
    "np.random.shuffle(X)    # randomize the data\n",
    "Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (200, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = X[:160], Y[:160]     # train 前 160 data points\n",
    "X_test, Y_test = X[160:], Y[160:]       # test 后 40 data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构建模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#建立模型\n",
    "'''\n",
    "然后用 Sequential 建立 model， 再用 model.add 添加神经层，添加的是 Dense 全连接神经层。\n",
    "参数有两个，一个是输入数据和输出数据的维度，本代码的例子中 x 和 y 是一维的。\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=1, units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**编译模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#激活模型\n",
    "#参数中，误差函数用的是 mse 均方误差；优化器用的是 sgd 随机梯度下降法。\n",
    "model.compile(loss='mse', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,Y_train,batch_size=32,epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**评估模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#检验模型\n",
    "cost = model.evaluate(X_test, Y_test, batch_size=40)\n",
    "print('test cost:', cost)\n",
    "W, b = model.layers[0].get_weights()\n",
    "print('Weights=', W, '\\nbiases=', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#可视化结果\n",
    "Y_pred = model.predict(X_test)\n",
    "plt.scatter(X_test, Y_test)\n",
    "plt.plot(X_test, Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**保存模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save保存模型\n",
    "print('test before save: ', model.predict(X_test[0:2]))\n",
    "model.save('tmp/my_model.h5')   # 将模型保存到本地\n",
    "del model  # 将模型删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型已经删除，这时预测会报错\n",
    "model.predict(X_test[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load导入模型并应用\n",
    "from keras.models import load_model\n",
    "new_model = load_model('tmp/my_model.h5')\n",
    "print('test after load: ', new_model.predict(X_test[0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Keras流程\n",
    "* 1、定义模型 **Sequential()**\n",
    "* 2、堆叠模块 **.add()**\n",
    "* 3、模型编译 **.compile()**\n",
    "* 4、模型训练 **.fit()**\n",
    "* 5、模型评估 **.evaluate()**\n",
    "* 6、数据预测 **.predict()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Keras使用介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras的神经网络API是在封装后与使用者直接进行交互的API组件，在使用时可以调用Keras的其它组件。除数据预处理外，使用者可以通过神经网络API实现机器学习任务中的常见操作，包括人工神经网络的构建、编译、学习、评估、测试等。\n",
    "\n",
    "![](./images/Keras1.png)\n",
    "\n",
    "Keras中文文档：https://keras.io/zh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Keras网络层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 核心网络层\n",
    "* **Dense(全连接层)**:输入和输出都是n维张量.实现的运算是output=activation(dot(input,kernel)+bias).作为第一层时需要指定输出维度units和输入维度input_shape,后续层只用指定units即可.主要参数有激活函数activation以及对权值,偏置向量和输出的各种设置.\n",
    "* **Activation(激活层)**:对一个层的输出施加激活函数,参数只有activation,作为第一层时要指定input_shape.\n",
    "* **Dropout**:在训练过程中每次更新参数时按一定概率随机断开输入神经元,用于防止过拟合.主要参数有断开比例rate.\n",
    "* **Flatten**:把多维的输入一维化,常用在从卷积层到全连接层的过渡,不会影响batch的大小.\n",
    "* Reshape:把输入shape转换为特定的shape,参数只有target_shape.当shape的其余维度确定后,可以用-1来指代剩下一个未确定的值.\n",
    "* Permute:将输入的维度按照规定模式进行重排,例如将RNN和CNN网络连接时.参数只有dims,用整数tupel指定了重排的模式.\n",
    "* RepeatVector:将输入重复n次,参数只有n.\n",
    "* Lambda:对上一层的输入施加任何Theano/TensorFlow表达式.主要参数有函数function和output_shape\n",
    "* AvtivityRegularizer:不会改变数据,但是会基于激活值更新损失函数值.参数有l1正则因子l1和l2正则因子l2\n",
    "* Masking:根据给定的值对输入的序列信号进行屏蔽.如果输入张量在某个时间步上都等于给定值,则该时间步将在模型接下来所有支持masking的层被屏蔽.参数只有mask_value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense [全连接层]\n",
    "\n",
    "```py\n",
    "keras.layers.Dense(units, activation=None, use_bias=True,kernel_initializer='glorot_uniform', bias_initializer='zeros',kernel_regularizer=None, bias_regularizer=None,activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "参数\n",
    "* **units**: 正整数，输出空间维度。\n",
    "* **activation**: 激活函数(详见activations)。若不指定，则不使用激活函数(即，“线性” 激活:a(x) = x)。\n",
    "* use_bias: 布尔值，该层是否使用偏置向量。\n",
    "* kernel_initializer: kernel 权值矩阵的初始化器(详见initializers)。\n",
    "* bias_initializer: 偏置向量的初始化器(see initializers).\n",
    "* kernel_regularizer: 运用到kernel 权值矩阵的正则化函数(详见regularizer)。\n",
    "* bias_regularizer: 运用到偏置向的的正则化函数(详见regularizer)。\n",
    "* activity_regularizer: 运用到层的输出的正则化函数(它的“activation”)。(详见regularizer)。\n",
    "* kernel_constraint: 运用到kernel 权值矩阵的约束函数(详见constraints)。\n",
    "* bias_constraint: 运用到偏置向量的约束函数(详见constraints)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation [激活函数]\n",
    "```py\n",
    "keras.layers.Activation(activation)\n",
    "```\n",
    "将激活函数应用于输出。\n",
    "\n",
    "参数\n",
    "* activation: 要使用的激活函数的名称(详见: hyperref[activations]activations)，或者选择一个Theano 或TensorFlow 操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout \n",
    "```py\n",
    "keras.layers.Dropout(rate, noise_shape=None, seed=None)\n",
    "```\n",
    "将Dropout 应用于输入。\n",
    "Dropout 包括在训练中每次更新时，将输入单元的按比率随机设置为0，这有助于防止过拟合。\n",
    "\n",
    "参数\n",
    "\n",
    "* **rate**: 在0 和1 之间浮动。需要丢弃的输入比例。\n",
    "* noise_shape: 1D 整数张量，表示将与输入相乘的二进制dropout 掩层的形状。\n",
    "* seed: 一个作为随机种子的Python 整数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 卷积层\n",
    "* 1D一般指时域序列,2D一般指图像,3D一般指连续的图像序列.\n",
    "* **Conv1D**:一维卷积层(时域卷积),用于在一维输入信号上进行邻域滤波.\n",
    "* **Conv2D**:二维卷积层(空域卷积),对二维输入进行滑动窗卷积.\n",
    "* SeparableConv2D:深度可分离卷积层.\n",
    "* Conv2DTranspose:进行转置的卷积操作(反卷积).\n",
    "* Conv3D:对三维输入进行滑动窗卷积,例如连续帧图像.\n",
    "* Cropping1D(2D/3D):在时间轴上对1D(2D/3D)输入进行裁剪,需要指定首尾裁掉多少个元素.\n",
    "* UpSampling1D(2D/3D):在时间轴(行和列/三个维度)上将每个时间步重复size次.\n",
    "* ZeroPadding1D(2D/3D):对1D(2D/3D)输入的首尾端填充0,以控制卷积以后向量的长度/特征图的大小."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv1D [1D 卷积层]\n",
    "```py\n",
    "keras.layers.Conv1D(filters, kernel_size, strides=1,padding='valid', dilation_rate=1, activation=None,\n",
    "use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros', kernel_regularizer=None,\n",
    "bias_regularizer=None, activity_regularizer=None,kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "1D 卷积层(例如时序卷积)。\n",
    "\n",
    "该层创建了一个卷积核，该卷积核以单个空间（或时间）维上的层输入进行卷积，以生成输出张量。如果use_bias 为True，则会创建一个偏置向量并将其添加到输出中。最后，如果activation 不是None，它也会应用于输出。\n",
    "\n",
    "当使用该层作为模型第一层时，需要提供input_shape 参数（整数元组或None），例如，(10, 128) 表示10 个128 维的向量组成的向量序列，(None, 128) 表示128 维的向量组成的变长序列。\n",
    "\n",
    "参数\n",
    "\n",
    "* filters: 整数，输出空间的维度（即卷积中滤波器的输出数量）。\n",
    "* kernel_size: 一个整数，或者单个整数表示的元组或列表，指明1D 卷积窗口的长度。\n",
    "* strides: 一个整数，或者单个整数表示的元组或列表，指明卷积的步长。指定任何stride 值!=1 与指定dilation_rate 值!= 1 两者不兼容。\n",
    "* padding: \"valid\", \"causal\" 或\"same\" 之一(大小写敏感) \"valid\" 表示「不填充」。\"same\"表示填充输入以使输出具有与原始输入相同的长度。\"causal\" 表示因果（膨胀）卷积，例如，output[t] 不依赖于input[t+1:]，在模型不应违反时间顺序的时间数据建模时非常有用。在模型不应违反时间顺序的时间数据建模时非常有用。\n",
    "* dilation_rate: 一个整数，或者单个整数表示的元组或列表，指定用于膨胀卷积的膨胀率。当前，指定任何dilation_rate 值!= 1 与指定stride 值!= 1 两者不兼容。\n",
    "* activation: 要使用的激活函数(详见activations)。如果你不指定，则不使用激活函数(即线性激活：a(x) = x)。\n",
    "* use_bias: 布尔值，该层是否使用偏置向量。\n",
    "* kernel_initializer: kernel 权值矩阵的初始化器(详见initializers)。\n",
    "* bias_initializer: 偏置向量的初始化器(详见initializers)。\n",
    "* kernel_regularizer: 运用到kernel 权值矩阵的正则化函数(详见regularizer)。\n",
    "* bias_regularizer: 运用到偏置向量的正则化函数(详见regularizer)。\n",
    "* activity_regularizer: 运用到层输出（它的激活值）的正则化函数(详见regularizer)。\n",
    "* kernel_constraint: 运用到kernel 权值矩阵的约束函数(详见constraints)。\n",
    "* bias_constraint: 运用到偏置向量的约束函数(详见constraints)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv2D [2D 卷积层]\n",
    "```py\n",
    "keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid',data_format=None, dilation_rate=(1, 1), activation=None,\n",
    "use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros', kernel_regularizer=None,\n",
    "bias_regularizer=None, activity_regularizer=None,kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "2D 卷积层(例如对图像的空间卷积)。\n",
    "\n",
    "该层创建了一个卷积核，该卷积核对层输入进行卷积，以生成输出张量。如果use_bias 为True，则会创建一个偏置向量并将其添加到输出中。最后，如果activation 不是None，它也会应用于输出。\n",
    "\n",
    "当使用该层作为模型第一层时， 需要提供input_shape 参数（整数元组， 不包含样本表示的轴） ， 例如，input_shape=(128, 128, 3) 表示128x128 RGB 图像， 在data_format=\"channels_last\" 时。\n",
    "\n",
    "参数\n",
    "* filters: 整数，输出空间的维度（即卷积中滤波器的输出数量）。\n",
    "* kernel_size: 一个整数，或者2 个整数表示的元组或列表，指明2D 卷积窗口的宽度和高度。可以是一个整数，为所有空间维度指定相同的值。\n",
    "* strides: 一个整数，或者2 个整数表示的元组或列表，指明卷积沿宽度和高度方向的步长。可以是一个整数，为所有空间维度指定相同的值。指定任何stride 值!= 1 与指定dilation_rate 值!= 1 两者不兼容。\n",
    "* padding: \"valid\" 或\"same\" (大小写敏感)。\n",
    "* data_format: 字符串，channels_last (默认) 或channels_first 之一，表示输入中维度的顺序。channels_last 对应输入尺寸为(batch, height, width, channels)，channels_first 对应输入尺寸为(batch, channels, height, width)。\n",
    "* dilation_rate: 一个整数或2 个整数的元组或列表，指定膨胀卷积的膨胀率。可以是一个整数，为所有空间维度指定相同的值。当前，指定任何dilation_rate 值!= 1 与指定stride值!= 1 两者不兼容。\n",
    "* activation: 要使用的激活函数(详见activations)。如果你不指定，则不使用激活函数(即线\n",
    "性激活：a(x) = x)。\n",
    "* use_bias: 布尔值，该层是否使用偏置向量。\n",
    "* kernel_initializer: kernel 权值矩阵的初始化器(详见initializers)。\n",
    "* bias_initializer: 偏置向量的初始化器(详见initializers)。\n",
    "* kernel_regularizer: 运用到kernel 权值矩阵的正则化函数(详见regularizer)。\n",
    "* bias_regularizer: 运用到偏置向量的正则化函数(详见regularizer)。\n",
    "* activity_regularizer: 运用到层输出（它的激活值）的正则化函数(详见regularizer)。\n",
    "* kernel_constraint: 运用到kernel 权值矩阵的约束函数(详见constraints)。\n",
    "* bias_constraint: 运用到偏置向量的约束函数(详见constraints)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 池化层\n",
    "* **MaxPooling1D(2D/3D)**:对时域1D(空域/3D)信号进行最大值池化.\n",
    "* **AveragePoling1D(2D/3D)**:对时域1D(空域/3D)信号进行平均值池化.\n",
    "* GlobalMaxPooling1D/2D:对于时间/空域信号的全局最大池化.\n",
    "* GlobalAveragePooling1D/2D:为时域/空域信号施加全局平均值池化.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxPooling1D\n",
    "```py\n",
    "keras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid')\n",
    "```\n",
    "对于时序数据的最大池化。\n",
    "\n",
    "参数\n",
    "* pool_size: 整数，最大池化的窗口大小。\n",
    "* strides: 整数，或者是None。作为缩小比例的因数。例如，2 会使得输入张量缩小一半。如果是None，那么默认值是pool_size。\n",
    "* padding: \"valid\" 或者\"same\" （区分大小写）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxPooling2D \n",
    "```py\n",
    "keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid',data_format=None)\n",
    "```\n",
    "对于空域数据的最大池化。\n",
    "\n",
    "参数\n",
    "* pool_size: 整数，或者2 个整数元组，（垂直方向，水平方向）缩小比例的因数。（2，2）会把输入张量的两个维度都缩小一半。如果只使用一个整数，那么两个维度都会使用同样的窗口长度。\n",
    "* strides: 整数，整数元组或者是None。步长值。如果是None，那么默认值是pool_size。\n",
    "* padding: \"valid\" 或者\"same\" （区分大小写）。\n",
    "* data_format: 一个字符串，channels_last （默认值）或者channels_first。输入张量中的维度顺序。channels_last 代表尺寸是(batch, height, width, channels) 的输入张量，而channels_first 代表尺寸是(batch, channels, height, width) 的输入张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 融合层\n",
    "提供了一系列用于融合两个层或张量的层对象和方法.\n",
    "* Add:接收一个列表张量并返回它们的和\n",
    "* SubStract:将两个输入的层相减\n",
    "* Multiply/Average/Maximum/Concatenate:接收一个列表的同shape张量并返回它们的逐元素积/逐元素均值/逐元素最大值/按照给定轴相接构成的张量,shape不变\n",
    "* Dot:计算两个张量中样本的张量成绩.\n",
    "* add/substract/multiply/average/maximum/concatenate/dot:上述层的函数式包装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 规范层\n",
    "* BatchNormalization:在每个batch上将前一层的激活值重新规范化,使得其输出数据的均值接近0,标准差接近1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Keras函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 预处理函数\n",
    "\n",
    "#### 图片预处理:\n",
    "* ImageDataGenerator:图片生成器,用以生成一个batch的图像数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 损失函数\n",
    "#### 损失函数的使用\n",
    "损失函数Loss（或称目标函数、优化评分函数）是编译模型时所需的两个参数之一：\n",
    "目标函数是编译模型必要的两个参数之一,可通过传递预定义目标函数名字指定目标函数,也可以传递一个符号函数作为目标函数.\n",
    "#### 可用的目标函数\n",
    "* **mean_squared_error(mse)**:均方误差\n",
    "* mean_absolute_error(mae):平均绝对误差\n",
    "* mean_absolute_percentage_error(mape):平均绝对百分误差\n",
    "* mean_squared_logarithmic_error(msle):平均对数平方误差\n",
    "* squared_hinge:平方铰链误差,主要用于SVM.\n",
    "* hinge:铰链误差,主要用于SVM.\n",
    "* categorical_hinge\n",
    "* **binary_crossentropy（ogloss）**:对数损失函数\n",
    "* logcosh:预测误差的双曲余弦函数的对数\n",
    "* **categorical_crossentropy**：多类的对数损失\n",
    "* sparse_categorical_crossentrop：接受稀疏标签的多类对数损失\n",
    "* kullback_leibler_divergence:相对熵,KL散度\n",
    "* poisson：(predictions - targets * log(predictions))的均值\n",
    "* cosine_proximity：预测误差的余弦距离平均值的相反数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 评估标准\n",
    "\n",
    "评价函数用于评估当前训练模型的性能。当模型编译后（compile），评价函数应该作为metrics 的参数来输入。\n",
    "\n",
    "性能评估类似于目标函数,只不过该性能的评估结果不会用于训练.在模型编译时由metrics设置.可以用预定义的函数,也可以自己定制.\n",
    "预定义张量(参数均为y_ture,y_pred):\n",
    "* **binary_accuracy**:二分类问题的平均正确率\n",
    "* **categorical_accuracy**:多分类问题的平均正确率\n",
    "* sparese_categorical_accuracy:同上,适用于稀疏情况\n",
    "* top_k_categorical_accracy:top-k正确率\n",
    "* sparese_top_k_categorical_accuracy:同上,适用于稀疏情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**自定义评价函数**\n",
    "\n",
    "定制的评估函数可以在模型编译时传入，该函数应该以(y_true,y_pred)为参数，返回单个张量，或从metric_name映射到metric_value的字典，下面是一个示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def mean_pred(y_true,y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy',mean_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 优化器\n",
    "优化器Optimizer是编译模型必要的两个参数之一,可以初始化一个优化器对象传入,也可以传递一个预定义优化器名(参数将使用默认值).\n",
    "可用的优化器:\n",
    "* **SGD**:随机梯度下降法\n",
    "* **RMSprop**:root mean square prop(均方根传播)，适用于递归神经网络.\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* **Adam**\n",
    "* Adamax\n",
    "* Nadam\n",
    "* TFOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD  \n",
    "随机梯度下降法，支持动量参数，支持学习衰减率，支持Nesteror动量。\n",
    "\n",
    "#### 参数  \n",
    "* lr:大于或等于0的浮点数，学习率\n",
    "* momentum:大于或等于0的浮点数，动量参数\n",
    "* decay:大或等于0的浮点数，每次更新后的学习率衰减值\n",
    "* nesteror: 布尔值，确定时候使用Nesterov动量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd=SGD(lr=.01,decay=1e-6,momentum=.9,nesterov=True)\n",
    "model.compile(loss='mean_squared_error',optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSprop\n",
    "除学习率可调整外，建议其他参数默认  \n",
    "该优化器通常是面对递归神经网络的一个良好选择\n",
    "\n",
    "#### 参数\n",
    "* lr:学习率\n",
    "* rho:大或等于0的浮点数\n",
    "* epsilom: 大或等于0的小浮点数，防止除0错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RMSprop(lr=.001,rho=.9,epsilon=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5 激活函数\n",
    "激活函数可以通过设置单独的激活层实现,也可以在构造层对象时通过传递activation参数实现.\n",
    "预定义激活函数:\n",
    "* **softmax**\n",
    "* elu\n",
    "* selu\n",
    "* softplus\n",
    "* softsign\n",
    "* **relu**\n",
    "* **tanh**\n",
    "* **sigmoid**\n",
    "* hard_sigmoid\n",
    "* linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#等价于\n",
    "model.add(Dense(64, activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.6 回调函数\n",
    "\n",
    "回调函数是一个函数的合集，会在训练的阶段中所使用。你可以使用回调函数来查看训练模型的内在状态和统计。\n",
    "\n",
    "你可以传递一个列表的回调函数（作为callbacks 关键字参数）到Sequential 或Model 类型的.fit() 方法。\n",
    "\n",
    "在训练时，相应的回调函数的方法就会被在各自的阶段被调用。\n",
    "\n",
    "* BaseLogger:对每个epoch累加metrics指定的监视指标的epoch平均值\n",
    "* ProgbarLogger:将metrics指定的监视指标输出到标准输出上\n",
    "* History:会自动调用,即fit方法的返回值\n",
    "* ModelCheckpoing:将在每个epoch后保存到filepath\n",
    "* **EarlyStopping**:用于早停\n",
    "* RemoteMonitor:用于向服务器发送事件流\n",
    "* LearningRateScheduler:学习率调度器\n",
    "* TensorBoard:可视化展示器\n",
    "* ReduceLROnPlateau:当评价指标不再提升时减少学习率\n",
    "* CSVLogger:将epoch的训练结果保存在csv中\n",
    "* LambdaCallback:用于创建简单的callback类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.7 初始化方法\n",
    "初始化方法定义了对keras层设置初始化权重的方法,一般由kernel_initializer和bias_initializer来指定.可以用预定义初始化器也可以自定义函数.\n",
    "\n",
    "预定义初始化方法:\n",
    "* Zeros:全零初始化\n",
    "* Ones:全1初始化\n",
    "* Constant:初始化为一个固定值\n",
    "* RandomNormal:正态分布初始化\n",
    "* RandomUniform:均匀分布初始化\n",
    "* TruncatedNormal:截尾高斯分布初始化,是神经网络权重和滤波器的推荐初始化方法\n",
    "* VarianceScaling:可以根据参数distribution决定不同的初始化方式\n",
    "* Orthogonal:随机正交矩阵初始化\n",
    "* Identiy:单位矩阵初始化\n",
    "* lecun_uniform:LeCun均匀分布初始化\n",
    "* lecun_normal:LeCun正态分布初始化\n",
    "* glorot_normal:Glorot正态分布初始化\n",
    "* glorot_uniform:Glorot均匀分布初始化\n",
    "* he_normal:He正态分布初始化\n",
    "* he_uniform:He均匀分布初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(64,kernel_initializer='random_uniform',bias_initializer='zeros'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Keras高级用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 数据并行\n",
    "\n",
    "数据并行包括在每个设备上复制一次目标模型，并使用每个模型副本处理不同部分的输入数据。\n",
    "\n",
    "Keras 有一个内置的实用函数keras.utils.multi_gpu_model，它可以生成任何模型的数据并行版本，在多达8 个GPU 上实现准线性加速。\n",
    "\n",
    "```py\n",
    "from keras.utils import multi_gpu_model\n",
    "# 将`model` 复制到8 个GPU 上。\n",
    "# 假定你的机器有8 个可用的GPU。\n",
    "parallel_model = multi_gpu_model(model, gpus=8)\n",
    "parallel_model.compile(loss='categorical_crossentropy',optimizer='rmsprop')\n",
    "# 这个`fit` 调用将分布在8 个GPU 上。\n",
    "# 由于batch size 为256，每个GPU 将处理32 个样本。\n",
    "parallel_model.fit(x, y, epochs=20, batch_size=256)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 设备并行\n",
    "\n",
    "设备并行性包括在不同设备上运行同一模型的不同部分。对于具有并行体系结构的模型，例如有两个分支的模型，这种方式很合适。\n",
    "\n",
    "这种并行可以通过使用TensorFlow device scopes 来实现。这里是一个简单的例子：\n",
    "\n",
    "```py\n",
    "# 模型中共享的LSTM 用于并行编码两个不同的序列\n",
    "input_a = keras.Input(shape=(140, 256))\n",
    "input_b = keras.Input(shape=(140, 256))\n",
    "shared_lstm = keras.layers.LSTM(64)\n",
    "# 在一个GPU 上处理第一个序列\n",
    "with tf.device_scope('/gpu:0'):\n",
    "    encoded_a = shared_lstm(tweet_a)\n",
    "# 在另一个GPU 上处理下一个序列\n",
    "with tf.device_scope('/gpu:1'):\n",
    "    encoded_b = shared_lstm(tweet_b)\n",
    "# 在CPU 上连接结果\n",
    "with tf.device_scope('/cpu:0'):\n",
    "    merged_vector = keras.layers.concatenate([encoded_a, encoded_b],axis=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Keras使用小结\n",
    "![](./images/keras.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
