{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 从网络和硬盘访问文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、从网络上下载文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"https://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode(\"utf8\")\n",
    "print(type(raw),\"\\n\", len(raw),\"\\n\")\n",
    "raw[:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "print (type(tokens),\"\\n\",len(tokens))\n",
    "tokens[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、从这个列表创建一个NLTK 文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text),\"\\n\")\n",
    "print(text[1024:1062],\"\\n\")\n",
    "text.collocations()                # 常用搭配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、手工检查文件以发现标记内容开始和结尾的独特的字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.find(\"PART I\"),\"\\n\")\n",
    "print(raw.rfind(\"End of Project Gutenberg’s Crime\")) # 注意，这里的 ’  是中文符号下的 ‘    \n",
    "         # 这里的raw.rfind() 是反向find的意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1 = raw[5336:1157812]\n",
    "raw1.find(\"PART I\")\n",
    "raw1[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、处理HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode(\"utf8\")\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html).get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、处理搜索引擎的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "llog = feedparser.parse(\"http://feed.cnblogs.com/blog/sitehome/rss\")\n",
    "llog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llog[\"feed\"][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llog.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = llog.entries[2]\n",
    "post.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = post.content[0].value\n",
    "content[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = BeautifulSoup(content).get_text()\n",
    "word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、读取本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"3.document.txt\",'r') # 'r'意味着以只读方式打开文件（默认），'U'表示“通用”，它让我们忽略不同的换行约定。\n",
    "raw = f.read()\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"3.document.txt\",\"r\")\n",
    "for line in f:\n",
    "    print(line.strip()) # strip()方法删除输入行结尾的换行符。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8、从PDF、MS Word 及其他二进制格式中提取文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文字常常以二进制格式出现，如PDF 和MSWord，只能使用专门的软件打开。第三方函数库如pypdf和pywin32提供了对这些格式的访问。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9、NLP 的流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3.1.png](./picture/3.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10、str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
    ">>> b = [' ' * 2 * (7 - i) + 'very' * i for i in a]\n",
    ">>> for line in b:\n",
    "...     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表**中的元素可以很大也可以很小，只要我们喜欢：例如，它们可能是段落、句子、短语、单词、字符。\n",
    "\n",
    "因此，我们在一段NLP 代码中可能做的第一件事情就是将一个字符串分词放入一个**字符串列表**中。\n",
    "\n",
    "相反，当我们要将结果写入到一个文件或终端，我们通常会将它们格式化为一个**字符串**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 使用正则表达式检测词组搭配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words(\"en\") if w.islower()]\n",
    "wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、使用基本的元字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordlist if re.search(\"ed$\", w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordlist if re.search(\"^..j..t..$\", w)] # 匹配第三个是j第六个是t的8个字母组成的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordlist if re.search(\"..j..t..\", w)] # 如果不限制 ^ 匹配字符的开始 $ 匹配字符的结尾，那么会有很多超过8字符的被匹配到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for w in wordlist if re.search(\"^e-?mail$\", w)) # ? 匹配前边的字符0次或1次         # 这行代码的意思是统计总共由多少email或e-mail "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、范围与闭包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![3.2](./picture/3.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过序列4653输入。有哪些其它词汇由相同的序列产生？\n",
    "[w for w in wordlist if re.search(\"^[ghi][mno][jkl][def]$\", w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匹配只使用中间行的4、5、6 键的词汇\n",
    "[w for w in wordlist if re.search(\"^[g-o]+$\", w)]   # - 表示范围 + 表示匹配1次或多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "[w for w in chat_words if re.search(\"^m+i+n+e+$\", w)]  # + 表示匹配1次或多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in chat_words if re.search(\"^m*i*n*e*$\", w)]   # * 表示匹配0次或多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in chat_words if re.search(\"^[ha]+$\", w)]  # [ ] 匹配集合里边的没有顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "[w for w in wsj if re.search(\"^[0-9]+\\.[0-9]+$\", w)] # \\. 表示后边的字符.不在具有转义含义而是字面的表示 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wsj if re.search(\"^[A-Z]+\\$$\", w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wsj if re.search(\"^[0-9]{4}$\", w)] # {4} 表示匹配前边的 字符或集合 四次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wsj if re.search(\"^[0-9]+-[a-z]{3,5}$\", w)] # 中间的 - 表示字符本身， {3,5} 表示匹配前边的字符或组合3次或5次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wsj if re.search(\"^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$\", w)]  # {5,} 表示匹配前边的字符或组合5次或5次以上 {,6} 表示匹配前边的字符或组合6次或6次以下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wsj if re.search(\"(ed|ing)$\", w)] # (ed|ing) 表示匹配已组合ed或者ing结尾的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [w for w in wsj if re.search(\"ed|ing$\", w)]:                # 不加() 只要遇到ed就匹配截止\n",
    "    if i not in [w for w in wsj if re.search(\"(ed|ing)$\", w)]:\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wsj if re.search(\"w(i|e|ai|oo)t\", w)] # 匹配含有wit，wet，wait，woot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"supercalifragilisticexpialidocious\"\n",
    "re.findall(r\"[aeiou]\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看一些文本中的两个或两个以上的元音序列，并确定它们的相对频率：\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for vs in re.findall(r\"[aeiou]{2,}\", word)  for word in wsj)\n",
    "fd.most_common(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    ">>> fd = nltk.FreqDist(vs for word in wsj for vs in re.findall(r'[aeiou]{2,}', word))\n",
    ">>> fd.most_common(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[vs for word in wsj for vs in re.findall(r'[aeiou]{2,}', word)]   # 疑问：for word in wsj 放在前边和放在后边为啥不一样？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[vs for vs in re.findall(r'[aeiou]{2,}', word)  for word in wsj]   # 疑问：for word in wsj 放在前边和放在后边为啥不一样？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "[int(n) for n in re.findall(\"[0-9]{2,}\", '2009-12-31')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、忽略掉词内部的元音\n",
    "英文文本是高度冗余的，忽略掉词内部的元音仍然可以很容易的阅读，有些时候这很明显。例如，declaration变成dclrtn，inalienable变成inlnble，保留所有词首或词尾的元音序列。在我们的下一个例子中，正则表达式匹配词首元音序列，词尾元音序列和所有的辅音；其它的被忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = r\"^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]\"\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return \"\".join(pieces)\n",
    "english_udhr = nltk.corpus.udhr.words(\"English-Latin1\")\n",
    "print(english_udhr[:75],\"\\n\")\n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、将正则表达式与条件频率分布结合起来\n",
    "在这里，我们将从罗托卡特语词汇中提取所有辅音-元音序列，如ka和si。因为每部分都是成对的，它可以被用来初始化一个条件频率分布。然后我们为每对的频率画出表格："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words(\"rotokas.dic\")\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r\"[ptksvr][aeiou]\", w)]\n",
    "print (cvs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、辅音-元音对的单词的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_word_pairs = [(cv, w) for w in rotokas_words for cv in re.findall(r\"[ptksvr][aeiou]\", w)]\n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "print(cv_index[\"su\"],\"\\n\\n\",cv_index[\"po\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码依次处理每个词w，对每一个词找出匹配正则表达式«[ptksvr][aeiou]»的所有子字符串。对于词kasuari，它找到ka, su和ri。因此，cv_word_pairs将包含('ka', 'kasuari'), ('su', 'kasuari')和('ri', 'kasuari')。更进一步使用nltk.Index()转换成有用的索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、查找词干"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$\", \"processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$\", \"processing\")  # (?:) 表示返回匹配到的字符串，而不是匹配到的部分片段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$\", \"processing\") # (.*) 表示两个部分分别提取出来 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')   # (.*) 表示贪婪提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$\", \"processing\") # (.*?) 添加一个 *? 号表示非贪婪提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language') # 后边添加？表示可选提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem2(word):\n",
    "    regexp  = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    " is no basis for a system of government.  Supreme executive power derives from\n",
    " a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "print([stem(t) for t in tokens],\"\\n\\n\",[stem2(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、使用nltk.findall搜索已分词文本\n",
    "你可以使用一种特殊的正则表达式搜索一个文本中多个词（这里的文本是一个词符列表）。例如，\"<a > <man>\" 找出文本中所有a man的实例。\n",
    "    \n",
    " 尖括号用于标记词符的边界，尖括号之间的所有空白都被忽略（这只对NLTK中的findall()方法处理文本有效）。\n",
    " \n",
    " 在下面的例子中，我们使用<.*>[1]，它将匹配所有单个词符，将它括在括号里，于是只匹配词（例如monied）而不匹配短语（例如，a monied man）会生成。\n",
    " \n",
    " 第二个例子找出以词bro结尾的三个词组成的短语[2]。\n",
    " \n",
    " 最后一个例子找出以字母l开始的三个或更多词组成的序列[3]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words(\"melville-moby_dick.txt\"))\n",
    "print(moby[:10],\"\\n\")\n",
    "print(moby.findall(r\"<a><.*><man>\"),\"\\n\")\n",
    "print(moby.findall(r\"<a>(<.*>)<man>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = nltk.Text(nps_chat.words())\n",
    "print(chat[:10],\"\\n\")\n",
    "chat.findall(r\"<.*><.*><bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=r'[a-zA-Z]+'\n",
    "nltk.re_show(p,'123asd456')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8、在大型文本语料库中搜索x and other ys形式的表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories = [\"hobbies\", \"learned\"]))\n",
    "hobbies_learned.findall(r\"<\\w*><and><other><\\w*s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobbies_learned.findall(r\"<as><\\w*><as><\\w*>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 规范化文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "... is no basis for a system of government.  Supreme executive power derives from\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、词干提取器\n",
    "看Porter词干提取器正确处理了词lying（将它映射为lie），而Lancaster词干提取器并没有处理好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、词形归并\n",
    "WordNet词形归并器只在产生的词在它的词典中时才删除词缀。这个额外的检查过程使词形归并器比刚才提到的词干提取器要慢。请注意，它并没有处理lying，但它将women转换为woman。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 用正则表达式为文本分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、分词的简单方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
    "print(re.split(r\" \", raw)) # 在 空格字符 处分割原始文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.split(r\"[ \\t\\n]+\",raw)) # 在 空格 或 制表符（\\t） 或 换行符（\\n） 处分割原始文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.split(r\"\\s+\", raw)) # 在 所有空白字符 处分割原始文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.split(r\"\\W+\", raw)) # 在 \\w 的补集处分割原始文本 ； \\w 表示匹配所有字符，相当于[a-zA-Z0-9_] ; \\W 表示 \\w 的补集，即所有字母数字下划线以外的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(r\"\\w+|\\S\\w*\", raw))  # 首先匹配字母数字下划线，如果没有则匹配非空白字符（\\S 是\\s 的补集）加上字母数字下划线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))  # \\w+(?:[-']\\w+)* 会匹配 hot-tempered和it's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、nltk自带的正则匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8 分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、断句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents()) # 计算布朗语料库中每个句子的平均词数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用nltk自带的Punkt句子分割器为一篇小说文本断句\n",
    "text = nltk.corpus.gutenberg.raw(\"chesterton-thursday.txt\")\n",
    "sents = nltk.sent_tokenize(text)\n",
    "pprint.pprint(sents[79:89])          # pprint()模块打印出来的数据结构更加完整，每行为一个数据结构，更加方便阅读打印输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、分词\n",
    "类似的问题在口语语言处理中也会出现，听者必须将连续的语音流分割成单个的词汇。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == \"1\":\n",
    "            words.append(text[last:i+1])\n",
    "            last = i + 1\n",
    "    words.append(text[last:])\n",
    "    return words\n",
    "print(segment(text,seg1))\n",
    "print(segment(text,seg2))\n",
    "print(segment(text,seg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算目标函数：给定一个假设的源文本的分词（左），推导出一个词典和推导表，它能让源文本重构，然后合计每个词项（包括边界标志）与推导表的字符数，作为分词质量的得分；得分值越小表明分词越好。\n",
    "\n",
    "![3.3](./picture/3.3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
    "    return text_size + lexicon_size\n",
    "print(evaluate(text, seg1))\n",
    "print(evaluate(text, seg2))\n",
    "print(evaluate(text, seg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例 3-4. 使用模拟退火算法的非确定性搜索:一开始仅搜索短语分词;随机扰动 0 和 1,\n",
    "它们与“温度”成比例;每次迭代温度都会降低,扰动边界会减少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]  # 将segs中pos位置的数字翻转：1变0 ; 0变1\n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0,len(segs)-1))                        # 随机翻转segs中的0或者1 n次\n",
    "    return segs\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, int(round(temperature)))  # round 返回浮点数的四舍五入   \n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print (evaluate(text, segs), segment(text, segs))\n",
    "    print\n",
    "    return segs\n",
    "\n",
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "\n",
    "anneal(text, seg1, 5000, 1.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了足够的数据,就可能以一个合理的准确度自动将文本分割成词汇。这种方法可用于\n",
    "为那些词的边界没有任何视觉表示的书写系统分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.9 格式化：从列表到字符串"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、从列表到字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
    "print(\" \".join(silly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、字符串与格式\n",
    "我们已经看到了有两种方式显示一个对象的内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"cat\"\n",
    "sentence = \"\"\"\n",
    "hello\n",
    "word\n",
    "\"\"\"\n",
    "print(word)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print命令让Python努力以人最可读的形式输出的一个对象的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二种方法——叫做变量提示——向我们显示可用于重新创建该对象的字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、格式化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.变量和常量交替出现\n",
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "for word in fdist:\n",
    "    print(word, \"->\", fdist[word], end = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.使用str.format（）方法\n",
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "for word in fdist:\n",
    "    print(\"{}->{};\".format(word, fdist[word]), end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、使用str.format（）方法对齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:6}\".format(41)   # 字符宽度为6，数字默认右对齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:<6}\".format(41)   # 字符宽度为6，数字 < 表示左对齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:6}\".format(\"dog\") # 字符宽度为6，字符默认左对齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:>6}\".format(\"dog\") # 字符宽度为6，字符 > 表示左对齐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定浮点数的符号和精度\n",
    "import math\n",
    "\"{:.4f}\".format(math.pi)  # 表示小数点后边显示4位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表示百分数\n",
    "\"accuracy for {} words: {:.4%}\".format(9375, 3205 / 9375)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、格式化字符串用于数据制表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate(cfdist, words, categories):\n",
    "    print(\"{:20}\".format(\"Category\"), end = \" \")\n",
    "    for word in words:\n",
    "        print(\"{:>6}\".format(word), end = \" \")\n",
    "    print ()\n",
    "    for category in categories:\n",
    "        print(\"{:20}\".format(category), end = \" \")\n",
    "        for word in words:\n",
    "            print(\"{:6}\".format(cfdist[category][word]), end = \" \")\n",
    "        print()\n",
    "        \n",
    "from nltk.corpus import brown\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word) \n",
    "    for genre in brown.categories() \n",
    "    for word in brown.words(categories = genre))\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "\n",
    "tabulate(cfd, modals, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动定制列的宽度\n",
    "#width = max(len(w) for w in words)\n",
    "\"{:{width}}\".format(\"Monty Python\", width = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、将结果写入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = open(\"3.output.txt\", \"w\")\n",
    "words = set(nltk.corpus.genesis.words(\"english-kjv.txt\"))\n",
    "for word in sorted(words):\n",
    "    print(word, file = output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 当我们将非文本数据写入文件时，我们必须先将它转换为字符串。\n",
    "print(str(len(words)), file = output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、文本换行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
    "         'more', 'is', 'said', 'than', 'done', '.']\n",
    "for word in saying:\n",
    "    print(word, \"(\" + str(len(word)) + \") , \", end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在Python 的textwrap模块的帮助下采取换行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import fill\n",
    "format = \"%s (%d) , \"\n",
    "pieces = [format % (word, len(word)) for word in saying]\n",
    "output = \" \".join(pieces)\n",
    "print(output,\"\\n\")\n",
    "print(fill(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
