{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. pLSA、共轭先验分布；LDA主题模型原理\n",
    "# 1.1 PLSA（Probabilistic Latent Semantic Analysis）\n",
    "**概率隐语义分析**（PLSA）是一个著名的针对文本建模的模型，是一个生成模型。因为加入了主题模型，所以可以很大程度上改善多词一义和一词多义的问题。Hoffmm在1999年提出了概率隐语义分析。他认为每个主题下都有一个词汇的概率分布，而一篇文章通常由多个主题构成，并且文章中的每个单词都是由某个主题生成的。\n",
    "\n",
    "关于PLSA的原理及公式推导可以参考博客 http://www.cnblogs.com/bentuwuying/p/6219970.html\n",
    "\n",
    "1.1.1 PLSA的优势\n",
    "\n",
    "定义了概率模型，而且每个变量以及相应的概率分布和条件概率分布都有明确的物理解释。\n",
    "相比于LSA隐含了高斯分布假设，pLSA隐含的Multi-nomial分布假设更符合文本特性。\n",
    "pLSA的优化目标是是KL-divergence最小，而不是依赖于最小均方误差等准则。\n",
    "可以利用各种model selection和complexity control准则来确定topic的维数。\n",
    "\n",
    "1.1.2pLSA的不足\n",
    "\n",
    "概率模型不够完备：在document层面上没有提供合适的概率模型，使得pLSA并不是完备的生成式模型，而必须在确定document i的情况下才能对模型进行随机抽样。\n",
    "随着document和term 个数的增加，pLSA模型也线性增加，变得越来越庞大。\n",
    "EM算法需要反复的迭代，需要很大计算量。\n",
    "\n",
    "## 1.2 共轭先验分布\n",
    "设θ是总体分布中的参数(或参数向量)，π(θ)是θ的先验密度函数，假如由抽样信息算得的后验密度函数与π(θ)有相同的函数形式，则称π(θ)是θ的(自然)共轭先验分布。\n",
    "\n",
    "1.2.1 共轭先验分布的参数确定\n",
    "如对于总体为二项分布，其成功概率的共轭先验分布为Beta(α,β)Beta(α,β)，在确定了共轭先验分布之后，我们还需要确定先验分布中的参数，像这里的(α,β)(α,β)。因此下面介绍两种常见方法来确定其参数。\n",
    "\n",
    "(1) 先验矩\n",
    "\n",
    "假如利用先验信息能得到成功概率θθ的若干个估计值，θ1、θ2、...、θk θ1、θ2、...、θkθ1、θ2、...、θk。由此可算得先验均值$\\overline{\\theta}$和先验方差$S^2_\\theta$。\n",
    "同时由先验分布贝塔分布Beta(α,β)，可以得出(α,β)(α,β)表示的期望和方差。\n",
    "由此可解得(α,β)(α,β)的值。\n",
    "\n",
    "(2) 先验分位数\n",
    "\n",
    "若由先验信息可以确定贝塔分布的两个分位数，则可由分位数的定义列出两个方程组同样接触所需参数。\n",
    "\n",
    "## 1.2.2  常见的共轭先验分布\n",
    "\n",
    "总体分布|\t参数|\t共轭先验分布\n",
    "---:|---:|---:\n",
    "二项分布|\t成功概率|\t贝塔分布$\\B(\\alpha,\\beta)$\n",
    "泊松分布|\t均值|\t伽马分布$\\Gamma(k,\\theta)$\n",
    "指数分布|\t均值的倒数|\t伽马分布$\\Gamma(k,\\theta)$\n",
    "正态分布(方差已知)|\t均值|\t正态分布$\\N(\\mu,\\sigma^2)$\n",
    "正态分布(方差未知)|\t方差|\t逆伽马分布$\\IGa(\\alpha,\\beta)$\n",
    "\n",
    "1） 先验知识\n",
    "\n",
    "LDA涉及到的先验知识有：二项分布、Gamma函数、Beta分布、多项分布、Dirichlet分布、马尔科夫链、MCMC、Gibs Sampling、EM算法等。\n",
    "\n",
    "2）词袋模型\n",
    "\n",
    "LDA 采用词袋模型。所谓词袋模型，是将一篇文档，我们仅考虑一个词汇是否出现，而不考虑其出现的顺序。在词袋模型中，“我喜欢你”和“你喜欢我”是等价的。与词袋模型相反的一个模型是n-gram，n-gram考虑了词汇出现的先后顺序。\n",
    "\n",
    "\n",
    "- 二项分布\n",
    "\n",
    "二项分布是N重伯努利分布，即为X ~ B(n, p). 概率密度公式为：\n",
    "\n",
    "$P(K=k)=\\dbinom{n}{k}p^k(1-p)^n-k$\n",
    "\n",
    "- 多项分布\n",
    "\n",
    "$P(x_1,x_2,...,x_k;n,p_1,p_2,...,p_k)=\\frac {n!}{x_1!...xk!} p_1^{x_1}...p_k^{x_k}$\n",
    "\n",
    "- Gamma 函数\n",
    "\n",
    "$\\Gamma(x)=\\int_0^{\\infty}t^{x-1}e^{-t}dt$\n",
    "\n",
    "分部积分后，可以发现Gamma函数如有这样的性质：\n",
    "\n",
    "$\\Gamma(x+1)=x\\Gamma(x)$\n",
    "\n",
    "Gamma函数可以看成是阶乘在实数集上的延拓，具有如下性质：\n",
    "\n",
    "$\\Gamma(n)=(n-1)!$\n",
    "\n",
    "- Bata 分布\n",
    "\n",
    "$f(x;\\alpha,\\beta)=\\frac {1}{B(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$\n",
    "\n",
    "其中：\n",
    "\n",
    "$\\frac {1}{B(\\alpha,\\beta)}=\\frac {\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)+\\Gamma(\\beta)}$\n",
    "\n",
    "- 共轭先验分布\n",
    "\n",
    "在贝叶斯概率理论中，如果后验概率P(θ|x)和先验概率p(θ)满足同样的分布律，那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布\n",
    "\n",
    "$P(\\theta|x)=\\frac {P(\\theta,x)}{P(x)}$\n",
    "\n",
    "- Dirichlet分布\n",
    "\n",
    "$f(x_1,x_2,...,x_k;\\alpha_1,\\alpha_2,...,\\alpha_k)=\\frac {1}{B(\\alpha)}\\prod_{i=1}^{k}x_i^{\\alpha^i-1}$\n",
    "\n",
    "其中：\n",
    "\n",
    "$B(\\alpha)=\\frac {\\prod_{i=1}^{k}\\Gamma(\\alpha^i)}{\\Gamma(\\sum_{i=1}^{k})},\\sum_{i=1}^{k}x_i=1$\n",
    "\n",
    "根据Beta分布、二项分布、Dirichlet分布、多项式分布的公式，我们可以验证上一小节中的结论 -- Beta分布是二项式分布的共轭先验分布，而狄利克雷(Dirichlet)分布是多项式分布的共轭分布。\n",
    "无穷\n",
    "\n",
    "## 1.3 LDA主题模型原理\n",
    "事实上，理解了pLSA模型，也就差不多快理解了LDA模型，因为LDA就是在pLSA的基础上加层贝叶斯框架，即LDA就是pLSA的贝叶斯版本（正因为LDA被贝叶斯化了，所以才需要考虑历史先验知识，才加的两个先验参数）。\n",
    "\n",
    "对于语料库中的每篇文档，LDA定义了如下生成过程（generative process）：\n",
    "\n",
    "(1).对每一篇文档，从主题分布中抽取一个主题\n",
    "\n",
    "(2) 从上述被抽到的主题所对应的单词分布中抽取一个单词\n",
    "\n",
    "(3) 重复上述过程直至遍历文档中的每一个单词。\n",
    "\n",
    "之前没接触过，自己也没完全搞懂，就先不写这部分了，强烈推荐[\"LDA数学八卦\"系列](http://www.52nlp.cn/lda-math-%E6%B1%87%E6%80%BB-lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6)，内容详细通俗易懂。\n",
    "# 2. LDA应用场景 \n",
    "- 通常LDA用户进主题模型挖掘，当然也可用于降维。 \n",
    "- 推荐系统：应用LDA挖掘物品主题，计算主题相似度 \n",
    "- 情感分析：学习出用户讨论、用户评论中的内容主题\n",
    "# 3. LDA优缺点 \n",
    "LDA算法既可以用来降维，又可以用来分类，但是目前来说，主要还是用于降维。\n",
    "\n",
    "LDA算法的主要**优点**有：\n",
    "\n",
    "1）在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习则无法使用类别先验知识。\n",
    "\n",
    "2）LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。\n",
    "\n",
    "LDA算法的主要**缺点**有：\n",
    "\n",
    "1）LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。\n",
    "\n",
    "2）LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。当然目前有一些LDA的进化版算法可以绕过这个问题。\n",
    "\n",
    "3）LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。\n",
    "\n",
    "4）LDA可能过度拟合数据。\n",
    "\n",
    "# 4. LDA 参数学习 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_components : int, optional (default=10)\n",
    "    #主题数\n",
    "\n",
    "doc_topic_prior : float, optional (default=None)\n",
    "    #文档主题先验Dirichlet分布θd的参数α\n",
    "\n",
    "topic_word_prior : float, optional (default=None)\n",
    "    #主题词先验Dirichlet分布βk的参数η\n",
    "\n",
    "learning_method : 'batch' | 'online', default='online'\n",
    "    #LDA的求解算法。有 ‘batch’ 和 ‘online’两种选择\n",
    "\n",
    "learning_decay : float, optional (default=0.7)\n",
    "   #控制\"online\"算法的学习率，默认是0.7\n",
    "\n",
    "learning_offset : float, optional (default=10.)\n",
    "    #仅在算法使用\"online\"时有意义，取值要大于1。用来减小前面训练样本批次对最终模型的影响\n",
    "    \n",
    "max_iter : integer, optional (default=10)\n",
    "    #EM算法的最大迭代次数\n",
    "\n",
    "batch_size : int, optional (default=128)\n",
    "   #仅在算法使用\"online\"时有意义， 即每次EM算法迭代时使用的文档样本的数量。\n",
    "\n",
    "evaluate_every : int, optional (default=0)\n",
    "    #多久评估一次perplexity。仅用于`fit`方法。将其设置为0或负数以不评估perplexity训练。\n",
    "     \n",
    "total_samples : int, optional (default=1e6)\n",
    "    #仅在算法使用\"online\"时有意义， 即分步训练时每一批文档样本的数量。在使用partial_fit函数时需要。\n",
    "\n",
    "perp_tol : float, optional (default=1e-1)\n",
    "    #batch的perplexity容忍度。\n",
    "\n",
    "mean_change_tol : float, optional (default=1e-3)\n",
    "    #即E步更新变分参数的阈值，所有变分参数更新小于阈值则E步结束，转入M步。\n",
    "\n",
    "max_doc_update_iter : int (default=100)\n",
    "    #即E步更新变分参数的最大迭代次数，如果E步迭代次数达到阈值，则转入M步。\n",
    "\n",
    "n_jobs : int, optional (default=1)\n",
    "   #在E步中使用的资源数量。 如果为-1，则使用所有CPU。\n",
    "     #``n_jobs``低于-1，（n_cpus + 1 + n_jobs）被使用。\n",
    "\n",
    "verbose : int, optional (default=0)\n",
    "    #详细程度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "print('--------------------训练完成-----------------------')\n",
    "# 利用已训练好的模型将doc转换为话题分布\n",
    "doc_topic_dist = model.transform(x_train)\n",
    "# 通过调用lda.perplexity(X)函数，可以得知当前训练的perplexity\n",
    "print(doc_topic_dist, '当前训练的perplexity', model.perplexity(x_train), sep='\\n')\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    #打印每个主题下权重较高的term\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print('打印主题-词语分布矩阵')\n",
    "    return model.components_\n",
    "\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "m = print_top_words(model, tf_feature_names, 20)\n",
    "print(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer()\n",
    "tf_train = tf_vectorizer.fit_transform(train_content)\n",
    "tf_test = tf_vectorizer.fit_transform(test_content)```\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                    max_iter=20,\n",
    "                                    learning_method='batch',\n",
    "                                    evaluate_every=200,\n",
    "                                    verbose=0)\n",
    "x_train = lda.fit(tf_train).transform(tf_train)\n",
    "x_test = lda.fit(tf_test).transform(tf_test)\n",
    "clf = nb.SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "print('--------------------训练完成-----------------------')\n",
    "pred = clf.predict(x_test)\n",
    "print(\"classification report on test set for classifier:\")\n",
    "print(classification_report(y_test, pred ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## 参考：https://blog.csdn.net/nc514819873/article/details/89374542\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
