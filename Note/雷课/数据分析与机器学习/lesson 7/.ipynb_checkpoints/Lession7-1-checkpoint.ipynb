{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>自然语言处理--文本表示实践</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课程内容\n",
    "* 1.中文分词\n",
    "* 2.文本表示\n",
    "  * 2.1 one-hot编码\n",
    "  * 2.2 词袋模型\n",
    "  * 2.3 主题模型\n",
    "  * 2.4 词向量模型\n",
    "  * 2.5 文档向量模型\n",
    "* 3.案例实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.中文分词\n",
    "\n",
    "“结巴”中文分词——最好的Python中文分词组件\n",
    "\n",
    "* 支持三种分词模式：\n",
    " - 精确模式，试图将句子最精确地切开，适合文本分析；\n",
    " - 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；\n",
    " - 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n",
    "* 支持繁体分词\n",
    "* 支持自定义词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 基本功能\n",
    "\n",
    "* jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型\n",
    "* jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"【全模式】: \" + \", \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"【精确模式】:\" + \", \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # “杭研”并没有在词典中，但是也被Viterbi算法识别出来了\n",
    "print(\"【新词识别】：\"+\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\"【搜索引擎模式】：\"+\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_list = jieba.cut(\"雷课教育-计算机前沿技术教育咨询\")\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 自定义词典\n",
    "\n",
    "* 用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径\n",
    "* 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。\n",
    "\n",
    "```\n",
    "创新办 3 i\n",
    "云计算 5\n",
    "凱特琳 nz\n",
    "台中\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sent = (\n",
    "\"李小福是创新办主任也是云计算方面的专家; 雷课教育-计算机前沿技术教育咨询\\n\"\n",
    "\"例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\\n\"\n",
    "\"「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = jieba.cut(test_sent)\n",
    "print('/'.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.load_userdict(\"data/userdict.txt\") #加载用户词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = jieba.cut(test_sent)\n",
    "print('/'.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用 add_word(word, freq=None, tag=None) 和 del_word(word) 可在程序中动态修改词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.add_word('石墨烯')\n",
    "jieba.add_word('雷课教育')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = jieba.cut(test_sent)\n",
    "print('/'.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('/'.join(jieba.cut('如果放到post中将出错。')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.suggest_freq(('中', '将'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('/'.join(jieba.cut('如果放到post中将出错。')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.文本表示\n",
    "\n",
    "文本是最常用的序列之一，可以理解为字符序列或单词序列，但最常见的是单词级处理。与其他神经网络一样，深度学习模型不会接收原始文本作为输入，只能处理数值张量。文本表示是指将文本转换为数值张量的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = ['Python是目前最流行的数据分析和机器学习编程语言',\n",
    "        'Python语言编程将很快成为各个高校的必修课',\n",
    "        'Python是科研工作者开展科学研究的高效工具']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for text in texts:\n",
    "    seg_list = jieba.cut(text)\n",
    "    sentences.append(' '.join(seg_list))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建单词索引\n",
    "tk.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把单词转换为序列\n",
    "seqs = tk.texts_to_sequences(sentences)\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for seq in seqs:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#one hot编码\n",
    "one_hot_results = tk.texts_to_matrix(sentences, mode='binary')\n",
    "for one_hot_result in one_hot_results:\n",
    "    print(one_hot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用pairwise_distances计算的Cosine distance是1-（cosine similarity）结果\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "user_similarity = cosine_similarity(one_hot_results)\n",
    "user_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：数据要在一个空间内，也就是共用词典**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim\n",
    "作为自然语言处理爱好者，大家都应该听说过或使用过大名鼎鼎的Gensim吧，这是一款具备多种功能的神器。\n",
    "\n",
    "Gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。\n",
    "\n",
    "它支持包括TF-IDF，LDA，和word2vec在内的多种主题模型算法，支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口。\n",
    "* 语料（Corpus）：一组原始文本的集合，用于无监督地训练文本主题的隐层结构。语料中不需要人工标注的附加信息。在Gensim中，Corpus通常是一个可迭代的对象（比如列表）。每一次迭代返回一个可用于表达文本对象的稀疏向量。\n",
    "\n",
    "* 向量（Vector）：由一组文本特征构成的列表。是一段文本在Gensim中的内部表达。\n",
    "\n",
    "* 稀疏向量（SparseVector）：通常，我们可以略去向量中多余的0元素。此时，向量中的每一个元素是一个(key, value)的元组\n",
    "\n",
    "* 模型（Model）：是一个抽象的术语。定义了两个向量空间的变换（即从文本的一种向量表达变换为另一种向量表达）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 词袋模型\n",
    "\n",
    "![](images/wordbag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 语料预处理\n",
    "这一次，让我们从表示为字符串的文档开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "documents = sentences[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('是 的 和'.split()) #停用词列表\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] #删除常用单词（使用停止词列表）\n",
    "         for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 词袋模型\n",
    "要将文档转换为向量，我们将使用名为bag-of-words的文档表示 。在此表示中，每个文档由一个向量表示，其中每个向量元素表示问题 - 答案对，仅通过它们的（整数）id来表示问题是有利的。问题和ID之间的映射称为字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts) #创建一个映射字典\n",
    "dictionary.save('tmp/deerwester.dict') \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们为语料库中出现的所有单词分配了一个唯一的整数id gensim.corpora.dictionary.Dictionary。这会扫描文本，收集字数和相关统计数据。最后，我们看到在处理过的语料库中有22个不同的单词，这意味着每个文档将由22个数字表示（即，通过22-D向量）。要查看单词及其ID之间的映射："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要将标记化文档实际转换为向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc = \"Python 语言 编程 是 数据分析 的 编程语言\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())  #使用我们刚构造的字典来进行编码\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "该函数doc2bow()只计算每个不同单词的出现次数，将单词转换为整数单词id，并将结果作为稀疏向量返回。 因此，稀疏向量 [(0, 1), (4, 1), (7, 1), (8, 1), (9, 1)]读取：在文档“Python语言编程是数据分析的编程语言”中，词Python（id 0）、数据分析（id 4）、编程语言（id 7）、语言（id 8）和编程（id 9）出现一次; 其他十七个字典词（隐含地）出现零次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('tmp/deerwester.mm', corpus) \n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 主题模型\n",
    "\n",
    "对文本向量的变换是Gensim的核心。通过挖掘语料中隐藏的语义结构特征，我们最终可以变换出一个简洁高效的文本向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 TF-IDF模型\n",
    "\n",
    "在Gensim中，每一个向量变换的操作都对应着一个主题模型，例如上一小节提到的对应着词袋模型的doc2bow变换。每一个模型又都是一个标准的Python对象。下面以TF-IDF模型为例，介绍Gensim模型的一般使用方法。\n",
    "\n",
    "首先是模型对象的初始化。通常，Gensim模型都接受一段训练语料（注意在Gensim中，语料对应着一个稀疏向量的迭代器）作为初始化的参数。显然，越复杂的模型需要配置的参数越多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(texts)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建转换\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，corpus是一个返回bow向量的迭代器。这两行代码将完成对corpus中出现的每一个特征的IDF值的统计工作。\n",
    "\n",
    "接下来，我们可以调用这个模型将任意一段语料（依然是bow向量的迭代器）转化成TFIDF向量（的迭代器）。需要注意的是，这里的bow向量必须与训练语料的bow向量共享同一个特征字典（即共享同一个向量空间）。\n",
    "\n",
    "\n",
    "TF-IDF用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n",
    "\n",
    "TF:词频，指的是某一个给定的词语在该文件中出现的频率\n",
    "\n",
    "IDF:倒文档词频，是一个词语普遍重要性的度量，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。\n",
    "挑选文档的特征词：某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。\n",
    "\n",
    "TF-IDE=TF*IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc = \"Python 语言 编程 是 数据分析 的 编程语言\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())  #使用我们刚构造的字典来进行编码\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tfidf[new_vec])\n",
    "#输出每个词 在这个文档中重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 或者将转换应用于整个语料库：\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 可以将训练好的模型持久化到磁盘上，以便下一次使用：\n",
    "tfidf.save(\"tmp/model.tfidf\") #保存\n",
    "new_tfidf = models.TfidfModel.load(\"tmp/model.tfidf\")#加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tfidf[new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(new_tfidf[new_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 LDA模型\n",
    "人类是怎么生成文档的呢？\n",
    "\n",
    "比如假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练，获取每个主题Topic对应的词语。如下图所示：\n",
    "![](images/lda2.jpg)\n",
    "\n",
    "然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断的重复这两步，最终生成如下图所示的一篇文章（其中不同颜色的词语分别对应上图中不同主题下的词）\n",
    "![](images/lda1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10) # 需要指定主题的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc = \"Python 语言 编程 是 数据分析 的 编程语言\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())  #使用我们刚构造的字典来进行编码\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lda[new_vec])\n",
    "#输出每个词 在这个文档中重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tfidf[new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将单个主题作为格式化字符串\n",
    "lda.print_topic(2, topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把所有的主题打印出来看看\n",
    "lda.print_topics(num_topics=10, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 或者将转换应用于整个语料库：\n",
    "corpus_lda = lda[corpus]\n",
    "for doc in corpus_lda:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 可以将训练好的模型持久化到磁盘上，以便下一次使用：\n",
    "lda.save(\"tmp/model.lda\")\n",
    "new_lda = models.LdaModel.load(\"tmp/model.lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc = \"Python 语言 编程 是 数据分析 的 编程语言\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())  #使用我们刚构造的字典来进行编码\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_lda[new_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA应用场景 \n",
    "- 通常LDA用户进主题模型挖掘，当然也可用于降维。 \n",
    "- 推荐系统：应用LDA挖掘物品主题，计算主题相似度 \n",
    "- 情感分析：学习出用户讨论、用户评论中的内容主题\n",
    "\n",
    "#### LDA优缺点 \n",
    "\n",
    "LDA算法的主要**优点**有：\n",
    "\n",
    "- 1）在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习则无法使用类别先验知识。\n",
    "\n",
    "- 2）LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。\n",
    "\n",
    "LDA算法的主要**缺点**有：\n",
    "\n",
    "- 1）LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。\n",
    "\n",
    "- 2）LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。当然目前有一些LDA的进化版算法可以绕过这个问题。\n",
    "\n",
    "- 3）LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。\n",
    "\n",
    "- 4）LDA可能过度拟合数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 文档相似度的计算\n",
    "在得到每一篇文档对应的主题向量后，我们就可以计算文档之间的相似度，进而完成如文本聚类、信息检索之类的任务。在Gensim中，也提供了这一类任务的API接口。\n",
    "\n",
    "以信息检索为例。对于一篇待检索的query，我们的目标是从文本集合中检索出主题相似度最高的文档。\n",
    "\n",
    "首先，我们需要将待检索的query和文本放在同一个向量空间里进行表达（以LDA向量空间为例）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary.load('tmp/deerwester.dict') #加载字典\n",
    "corpus = corpora.MmCorpus('tmp/deerwester.mm') #加载语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个二维LDA空间：\n",
    "new_lda = models.LdaModel.load(\"tmp/model.lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"Python 语言 编程 是 数据分析 的 编程语言\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split()) #词袋模型编码\n",
    "vec_lda = new_lda[vec_bow] #求主题模型向量\n",
    "print(vec_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化查询\n",
    "index = similarities.MatrixSimilarity(new_lda[corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* similarities.MatrixSimilarity只有当整个向量集适合内存时，该类才适用。例如，当与此类一起使用时，一百万个文档的语料库在256维LSI空间中将需要2GB的RAM。\n",
    "* 如果没有2GB的可用RAM，则需要使用similarities.Similarity该类。此类通过在磁盘上的多个文件（称为分片）之间拆分索引，在固定内存中运行。它使用similarities.MatrixSimilarity和similarities.SparseMatrixSimilarity内部，所以它仍然很快，虽然稍微复杂一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 要获得我们的查询文档与三个索引文档的相似性：\n",
    "sims = index[vec_lda] \n",
    "print(list(enumerate(sims)))\n",
    "#余弦测量返回范围中的相似度（越大，越相似）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将这些相似性按降序排序，并获得查询 Python语言编程是数据分析的编程语言 的最终答案：\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 词向量简介\n",
    "\n",
    "词向量指的是一个词的向量表示。如果你希望计算机能够进行一些复杂点的文本语义学习，你必须得将文本数据编码成计算机能够处理的数值向量吧，所以词向量是一个自然语言处理任务中非常重要的一环。\n",
    "\n",
    "如何得到上述具有语义Distributional  representations的词向量呢，2013年提出的word2vec的方法就是一种非常方便得到高质量词向量的方式。其主要思想是：一个词的上下文可以很好的表达出词的语义，它是一种通过无监督的学习文本来用产生词向量的方式。\n",
    "\n",
    "100+ Chinese Word Vectors 上百种预训练中文词向量\n",
    "https://github.com/Embedding/Chinese-Word-Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "new_model = models.KeyedVectors.load_word2vec_format('tmp/sgns.wiki.bigram-char', binary=False) #词向量模型是文本格式binary=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model['北京']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(new_model['北京'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 得到与一个词最相关的若干词及相似程度\n",
    "new_model.similar_by_word('北京')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model.similar_by_word('苹果', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 得到两组词的相似度\n",
    "list1 = [u'核能']\n",
    "list2 = [u'电能']\n",
    "list3 = [u'电力']\n",
    "list_sim1 =  new_model.n_similarity(list1, list2) #比较两句话的相似度\n",
    "print(list_sim1)\n",
    "list_sim2 = new_model.n_similarity(list2, list3)\n",
    "print(list_sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = new_model.wv.similarity(u\"苹果\", u\"香蕉\")\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in new_model.wv.most_similar(u\"笔记本电脑\"):\n",
    "    print (i[0],i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 得到一组词中最无关的词\n",
    "list4 = [u'汽车', u'火车', u'飞机', u'北京']\n",
    "print(new_model.doesnt_match(list4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词语的加减游戏\n",
    "英文词类比中最有名的一个例子大概就是: king - man + woman = queen\n",
    "\n",
    "当我把这个例子换成中文映射到腾讯的中文词向量中并且用gensim来计算，竟然能完美复现：国王 - 男人 + 女人 = 王后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model.most_similar(positive=['国王', '女人'], negative=['男人'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后把国王换成皇帝，将输出什么？\n",
    "```\n",
    "机场-飞机+火车\n",
    "老婆-老公+丈夫\n",
    "北京-中国+法国\n",
    "上海-中国+美国\n",
    "小龙女-杨过+郭靖\n",
    "汽车-轮胎+翅膀\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 训练词向量\n",
    "\n",
    "word2vec中有两个非常经典的模型：skip-gram和cbow。\n",
    "\n",
    "* skip-gram：已知中心词预测周围词。\n",
    "* cbow：已知周围词预测中心词。\n",
    "\n",
    "比如 \"the quick brown fox jumps over the lazy dog\" 如果定义window-size为2的话，就会产生如下图所示的数据集，window-size决定了目标词会与多远距离的上下文产生关系：\n",
    "\n",
    "```\n",
    "Skip-Gram:(the,quick) ,其中the 是模型的输入，quick是模型的输出。\n",
    "Cbow： ((quick,brown),the) ,其中 （quick，brown）是模型的输入，the是模型的输出。\n",
    "```\n",
    "![](images/word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用word2vec.Word2Vec这个API的来训练词向量，参数说明如下\n",
    "```\n",
    "size: 表示词向量的维度，默认值是100。\n",
    "window：决定了目标词会与多远距离的上下文产生关系，默认值是5。\n",
    "sg: 如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。\n",
    "```\n",
    "采用CBOW模型——通过周围词预测中心词的方式训练词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "fpath = 'data/IT'\n",
    "flist = os.listdir(fpath) # 列出文件目录\n",
    "flist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas = list()\n",
    "for i in range(0, len(flist)):\n",
    "    path = os.path.join(fpath, flist[i])\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        datas.append(f.read().strip())\n",
    "datas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "for data in datas:\n",
    "    sentences.append(data.split())\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, size=50, window=5, min_count=5, workers=4) #size词向量维度\n",
    "model.save('tmp/my.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('tmp/my.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv['电脑']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(model.wv['电脑'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.similarity(u'笔记本', u'电脑')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词向量增量训练\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec.load(fname)\n",
    "model.train(sentences,total_examples=new_model.corpus_count, epochs=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 文档向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Doc2vec原理\n",
    "\n",
    "前文总结了Word2vec训练词向量的细节，讲解了一个词是如何通过word2vec模型训练出唯一的向量来表示的。那接着可能就会想到，有没有什么办法能够将一个句子甚至一篇短文也用一个向量来表示呢？答案是肯定有的，构建一个句子向量有很多种方法，今天我们接着word2vec来介绍下Doc2vec，看下Doc2vec是怎么训练一个句子向量的。\n",
    "\n",
    "Doc2vec又叫Paragraph Vector是Tomas Mikolov基于word2vec模型提出的，其具有一些优点，比如不用固定句子长度，接受不同长度的句子做训练样本，Doc2vec是一个无监督学习算法，该算法用于预测一个向量来表示不同的文档，该模型的结构潜在的克服了词袋模型的缺点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和word2vec一样，Doc2vec也有两种训练方式，一种是PV-DM（Distributed Memory Model of paragraphvectors）类似于word2vec中的CBOW模型，如图一：\n",
    "\n",
    "![](images/doc1.jpeg)\n",
    "\n",
    "另一种是PV-DBOW（Distributed Bag of Words of paragraph vector)类似于word2vec中的skip-gram模型，如图二：\n",
    "\n",
    "![](images/doc2.jpeg)\n",
    "\n",
    "在Doc2vec中，每一句话用唯一的向量来表示，用矩阵D的某一列来代表。每一个词也用唯一的向量来表示，用矩阵W的某一列来表示。每次从一句话中滑动采样固定长度的词，取其中一个词作预测词，其他的作输入词。输入词对应的词向量word vector和本句话对应的句子向量Paragraph vector作为输入层的输入，将本句话的向量和本次采样的词向量相加求平均或者累加构成一个新的向量X，进而使用这个向量X预测此次窗口内的预测词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences2 = sentences[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "# Create the tagged document needed for Doc2Vec\n",
    "for i in range(len(sentences2)):\n",
    "    sentences2[i] = gensim.models.doc2vec.TaggedDocument(words = sentences2[i], tags = [i]) #给每篇文档打tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 模型训练\n",
    "```py\n",
    "model = Doc2Vec(documents = sentences, dm = 1, size = 100, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)\n",
    "```\n",
    "参数说明\n",
    "* documents: training data (has to be iterable TaggedDocument instances)\n",
    "* size: 向量的维度\n",
    "* dm: 1 PV-DM, 0 PV-DBOW\n",
    "* window: 上下文词语离当前词语的最大距离\n",
    "* min_count: 词频小于min_count的词会被忽略\n",
    "* iter: 在整个语料上的迭代次数(epochs)，推荐10到20\n",
    "* workers: number of worker threads to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Init the Doc2Vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(dm=1, size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Build the Volabulary\n",
    "model.build_vocab(sentences2)\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "model.train(sentences2, total_examples=model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 与标签‘0’最相似的\n",
    "model.docvecs.most_similar(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 进行相关性比较\n",
    "model.docvecs.similarity(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输出标签为‘0’句子的向量\n",
    "model.docvecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 也可以推断一个句向量(未出现在语料中)\n",
    "# 新句子：硅谷动力消息国外媒体报道\n",
    "model.infer_vector(['硅谷动力', '消息', '国外', '媒体', '报道']) #新文档，分词后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model['电脑'] #unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习：文本匹配与相似度计算\n",
    "# 利用文本表示方法将文本转换为向量进行相似度计算\n",
    "\n",
    "# 训练数据\n",
    "train_documents = [\n",
    "    '南京江心洲污泥偷排或处置不当而造成的污染问题，不断被媒体曝光',\n",
    "    '面对美国金融危机冲击与国内经济增速下滑形势，中国政府在2008年11月初快速推出“4万亿”投资十项措施',\n",
    "    '全国大面积出现的雾霾，使解决我国环境质量恶化问题的紧迫性得到全社会的广泛关注',\n",
    "    '大约是1962年的夏天吧，潘文突然出现在我们居住的安宁巷中，她旁边走着40号王孃孃家的大儿子，一看就知道，他们是一对恋人。那时候，潘文梳着一条长长的独辫',\n",
    "    '坐落在美国科罗拉多州的小镇蒙特苏马有一座4200平方英尺(约合390平方米)的房子，该建筑外表上与普通民居毫无区别，但其内在构造却别有洞天',\n",
    "    '据英国《每日邮报》报道，美国威斯康辛州的非营利组织“占领麦迪逊建筑公司”(OMBuild)在华盛顿和俄勒冈州打造了99平方英尺(约9平方米)的迷你房屋',\n",
    "    '长沙市公安局官方微博@长沙警事发布消息称，3月14日上午10时15分许，长沙市开福区伍家岭沙湖桥菜市场内，两名摊贩因纠纷引发互殴，其中一人被对方砍死',\n",
    "    '乌克兰克里米亚就留在乌克兰还是加入俄罗斯举行全民公投，全部选票的统计结果表明，96.6%的选民赞成克里米亚加入俄罗斯，但未获得乌克兰和国际社会的普遍承认',\n",
    "    '京津冀的大气污染，造成了巨大的综合负面效应，显性的是空气污染、水质变差、交通拥堵、食品不安全等，隐性的是各种恶性疾病的患者增加，生存环境越来越差',\n",
    "    '1954年2月19日，苏联最高苏维埃主席团，在“兄弟的乌克兰与俄罗斯结盟300周年之际”通过决议，将俄罗斯联邦的克里米亚州，划归乌克兰加盟共和国',\n",
    "    '北京市昌平区一航空训练基地，演练人员身穿训练服，从机舱逃生门滑降到地面',\n",
    "    '腾讯入股京东的公告如期而至，与三周前的传闻吻合。毫无疑问，仅仅是传闻阶段的“联姻”，已经改变了京东赴美上市的舆论氛围',\n",
    "    '国防部网站消息，3月8日凌晨，马来西亚航空公司MH370航班起飞后与地面失去联系，西安卫星测控中心在第一时间启动应急机制，配合地面搜救人员开展对失联航班的搜索救援行动',\n",
    "    '新华社昆明3月2日电，记者从昆明市政府新闻办获悉，昆明“3·01”事件事发现场证据表明，这是一起由新疆分裂势力一手策划组织的严重暴力恐怖事件',\n",
    "    '在即将召开的全国“两会”上，中国政府将提出2014年GDP增长7.5%左右、CPI通胀率控制在3.5%的目标',\n",
    "    '中共中央总书记、国家主席、中央军委主席习近平看望出席全国政协十二届二次会议的委员并参加分组讨论时强调，团结稳定是福，分裂动乱是祸。全国各族人民都要珍惜民族大团结的政治局面，都要坚决反对一切危害各民族大团结的言行'\n",
    "]\n",
    "\n",
    "# 测试数据\n",
    "test_document = '媒体曝光南京江心洲污泥偷排或处置不当而造成的污染问题'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 3.案例实践\n",
    "#### 电商产品评论数据情感分析\n",
    "\n",
    "* 随着网上购物越来越流行，人们对于网上购物的需求变得越来越高，这让京东、淘宝等电商平台得到了很大的发展机遇。在这种电商平台激烈竞争的大背景下，除了提高商品质量、压低商品价格外，了解更多消费者的心声对于电商平台来说也变得越来越有必要，其中非常重要的方式就是对消费者的文本评论数据进行内在信息的数据挖掘分析，有利于对应商品的生产厂家自身竞争力的提升。\n",
    "*  针对京东商城上“美的”品牌的热水器的消费者的文本评论数据，在对文本进行基本的机器预处理、中文分词、停用词过滤后，通过建立多种数据挖掘模型，实现对文本评论数据的倾向性判断以及所隐藏的信息挖掘并分析，以期望得到有价值的内在内容。\n",
    "\n",
    "#### 数据文件\n",
    "* 京东用户评论数据\n",
    "\n",
    "#### 本节任务\n",
    "\n",
    "对京东平台上的热水器评论进行文本挖掘分析\n",
    "* 分析某一品牌的用户情感倾向\n",
    "\n",
    "![](images/JD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 3.1 提取评论数据\n",
    "\n",
    "将品牌为“美的”的评论一列抽取，另存为meidi_jd.txt，编码为UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputfile = \"data/huizong.csv\"\n",
    "data = pd.read_csv(inputfile, encoding=\"utf-8\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[[u\"评论\"]][data[u\"品牌\"] == u\"美的\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputfile = \"data/meidi_jd.txt\"\n",
    "data.to_csv(outputfile, index=False, header=False, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 评论预处理\n",
    "* 取到文本后，首先要进行文本评论数据的预处理。文本评论数据里存在大量价值含量很低甚至没有价值含量的条目，如果将这些评论数据也引入进行分词、词频统计乃至情感分析等，必然会对分析造成很大的影响，得到的结果的质量也必然是存在问题的。那么，在利用这些文本评论数据之前就必须先进行文本预处理，把大量的此类无价值含量的评论去除。\n",
    "* 文本评论数据的预处理主要由3个部分组成：文本去重、机械压缩去词以及短句删除。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 删除重复评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputfile = \"data/meidi_jd.txt\"\n",
    "data = pd.read_csv(inputfile, encoding=\"utf8\", header=None)\n",
    "l1 = len(data)\n",
    "\n",
    "data = pd.DataFrame(data[0].unique())\n",
    "l2 = len(data)\n",
    "print(u\"删除了%s条评论\" % (l1 - l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 过滤短句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1 = len(data)\n",
    "\n",
    "data = data[data[0].str.len()>4]\n",
    "l2 = len(data)\n",
    "\n",
    "print(u\"过滤了%s条评论\" % (l1 - l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "# 情绪判断，返回值为正面情绪的概率，越接近1表示正面情绪，越接近0表示负面情绪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 = '这部电影真心棒'\n",
    "s1 = SnowNLP(text1)\n",
    "print(text1, s1.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2 = '这部电影简直烂到爆'\n",
    "s2 = SnowNLP(text2)\n",
    "print(text2, s2.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sub = data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sub.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coms = []\n",
    "coms = data_sub[0].apply(lambda x:SnowNLP(x).sentiments)\n",
    "coms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造正面情绪\n",
    "data_pos = data_sub[coms>=0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_pos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造负面情绪\n",
    "data_neg = data_sub[coms<0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_neg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_neg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词\n",
    "使用jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mycut(s): return \" \".join(jieba.cut(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 正面情绪\n",
    "data1 = data_pos[0].apply(mycut)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 负面情绪\n",
    "data2 = data_neg[0].apply(mycut)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 正面情绪\n",
    "data1.to_csv('data/meidi_jd_pos.txt',index=False,encoding='utf-8',header=None)\n",
    "# 负面情绪\n",
    "data2.to_csv('data/meidi_jd_neg.txt',index=False,encoding='utf-8',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 评论数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据\n",
    "数据处理，剔除空格与标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 以正面评论为例分析\n",
    "posfile = \"data/meidi_jd_pos.txt\"\n",
    "stoplist = \"data/stoplist.txt\"\n",
    "\n",
    "pos = pd.read_csv(posfile, encoding=\"utf8\", header=None)\n",
    "\"\"\"\n",
    "sep设置分割词，由于csv默认半角逗号为分割词，而且该词恰好位于停用词表中\n",
    "所以会导致读取错误\n",
    "解决办法是手动设置一个不存在的分割词，这里使用的是tipdm\n",
    "参数engine加上，指定引擎，避免警告\n",
    "\"\"\"\n",
    "stop = pd.read_csv(stoplist, encoding=\"utf8\", header=None, sep=\"tipdm\", engine=\"python\")\n",
    "\n",
    "# pandas自动过滤了空格，这里手动添加\n",
    "stop = [\" \", \"\"] + list(stop[0])\n",
    "\n",
    "# 定义分割函数，然后用apply进行广播， 去掉停用词\n",
    "pos[1] = pos[0].apply(lambda s: s.split(\" \"))\n",
    "pos[2] = pos[1].apply(lambda x: [i for i in x if i not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词频统计\n",
    "统计词频，用标签云展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contents = pos[2].values.tolist()\n",
    "contents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for line in contents:\n",
    "    for word in line:\n",
    "        all_words.append(word)\n",
    "all_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_words = pd.DataFrame({'all_words':all_words})\n",
    "df_all_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_all_words.loc[:, 'count'] = 1\n",
    "words_count = df_all_words.groupby('all_words').agg({\"count\": 'count'})\n",
    "# words_count = df_all_words.groupby(by=['all_words'])['all_words'].agg({'count':np.size})\n",
    "words_count = words_count.reset_index().sort_values(by=['count'],ascending=False)\n",
    "words_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_frequency = {x[0]:x[1] for x in words_count.values}\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0,10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path='simhei.ttf',background_color='white',max_font_size=100)\n",
    "word_frequency = {x[0]:x[1] for x in words_count.head(200).values}\n",
    "wordcloud = wordcloud.fit_words(word_frequency)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义背景图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "aimask=np.array(Image.open(\"data/ai-mask.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path='simhei.ttf',background_color='white',max_font_size=100,mask=aimask)\n",
    "word_frequency = {x[0]:x[1] for x in words_count.head(200).values}\n",
    "wordcloud = wordcloud.fit_words(word_frequency)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA主题分析\n",
    "分词之后的语义分析，LDA模型分析正面负面情感"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 正面主题分析\n",
    "# 建立词典\n",
    "pos_dict = corpora.Dictionary(pos[2]) #词袋模型，将每个单词编码为整数\n",
    "\n",
    "# 建立语料库\n",
    "pos_corpus = [pos_dict.doc2bow(i) for i in pos[2]]  # bag of word 词袋模型编码\n",
    "\n",
    "# LDA模型训练\n",
    "pos_lda = models.LdaModel(pos_corpus, num_topics=3, id2word=pos_dict)\n",
    "\n",
    "for topic in pos_lda.print_topics(num_words=8):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习：负面评论数据分析\n",
    "# 标签云\n",
    "# 主题分析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
