{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>自然语言处理--提高篇</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.理解长短期记忆网络\n",
    "\n",
    "循环神经网络(RNN)的变种是长短期模型网络(LSTM)。它使用广泛，因为它的架构克服了困扰着所有周期性的神经网络梯度消失和梯度爆炸的问题，允许创建非常大的、非常深的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习字母表\n",
    "\n",
    "学习字母表是一个简单的序列预测问题。也就是说，根据字母表的字母，可以预测字母表的下一个字母。\n",
    "\n",
    "这是一个简单的序列预测问题，一旦被理解，就可以被推广到其他的序列预测问题，如时间序列预测和序列分类。\n",
    "\n",
    "![](images/LSTM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单字符——单字符的映射的简单LSTM\n",
    "学习如何根据一个字符的上下文来预测字母表中的下一个字符。\n",
    "\n",
    "定义一个LSTM网络，它有32个单元，一个输出层，其中有一个softmax的激活函数来进行预测。由于这是一个多类分类问题，所以我们可以使用在Keras中使用对数损失函数(称为“分类交叉熵”(categorical_crossentropy))，并使用ADAM优化函数对网络进行优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对随机数生成器选定随机数种子，以确保每次执行代码时结果都是相同的。\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "# 为了便于阅读，我们用大写字母来定义字母表。\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "# 神经网络是对数字建模，因此我们需要将字母表中的字母映射到整数值（把字母映射为数字）。我们可以很容易地通过创建字母索引的字典(map)到字符。\n",
    "# 我们还可以创建一个反向查找，以便将预测转换回字符，以便稍后使用。\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "# 造这样一个数据集，用一个字母，来预测下一个字母是什么\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "# 需要将NumPy数组重新构造为LSTM网络所期望的格式，即[samples示例, time steps时间步数, features特征]。\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "# 需要把我们的整数值归一化到0～1的区间上，这是LSTM网络使用的s形激活函数（sigmoid）的范围\n",
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "# 把这个问题看作是一个序列分类任务，其中26个字母代表一个不同的类。\n",
    "# 用keras的内置的 to_categorical()函数把输出output(y)进行 one－hot编码(one-hot指n维单位向量a=(0,…,0,1,0,…,0))作为输出层的结果。\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and fit the model\n",
    "# 定义一个LSTM网络，它有32个单元，一个输出层，其中有一个softmax的激活函数来进行预测。\n",
    "# 由于这是一个多类分类问题，所以我们可以使用在Keras中使用对数损失函数(称为“分类交叉熵”(categorical_crossentropy))，\n",
    "# 并使用ADAM优化函数对网络进行优化。\n",
    "# 该模型以500批次(epochs)，每批次数据输入大小(batch)为1的形式训练\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize performance of the model\n",
    "# 对整个训练集的性能进行评估和总结\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demonstrate some model predictions\n",
    "# 通过网络重新运行训练数据，并生成预测，将输入和输出对转换回原来的字符格式，以获得关于网络如何了解问题的视觉效果。\n",
    "for pattern in dataX:\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原因是可怜的lstm单元根本没有可以利用的上下文章信息。 \n",
    "每个输入输出模式都以随机的顺序显示在网络中，并且网络的状态在每个模式之后被重置(每个批处理的每个批次包含一个模式)。\n",
    "\n",
    "这是对LSTM网络架构的滥用，因为我们把它当作了一个标准的多层感知器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三字符特征——单字符的映射的简单LSTM\n",
    "\n",
    "在多层感知器中添加更多上下文最流行的方法是特征窗口方法(Feature Window method)。\n",
    "\n",
    "即序列中的前面步骤的输出被作为附加的输入特性提供给网络。我们可以用相同的技巧，为LSTM网络提供更多的上下文。\n",
    "\n",
    "**将序列长度从1增加到3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive LSTM to learn three-char window to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "# 在这里，我们将序列长度从1增加到3，例如: 我们把输入从一个字符升到三个字符。\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "# 将序列中的每个元素作为网络的一个新输入特性提供给它。这需要修改输入序列在数据准备步骤中的reshape:\n",
    "X = numpy.reshape(dataX, (len(dataX), 1, seq_length))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现有了一点点的提升，但是这一点点的提升未必是真的，梯度下降算法本来就是具有随机性的。\n",
    "\n",
    "也就是说我们再一次的错误使用了lstm循环神经网络。 \n",
    "我们确实给了上下文，但是并不是合适的方式， \n",
    "实际上，字母序列ABC才是一个特征的timesteps，而不是单独的一个特征的timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM的正确打开方式**\n",
    "\n",
    "利用lstm的关键是以时间序列(time steps)的方法来提供上下文，而不是像其他网络结构(CNN)一样，通过windowed features的方式。\n",
    "\n",
    "timesteps这个参数，我们设置了3，而不是前面的1。\n",
    "\n",
    "不同之处是，对输入数据的reshape是将输入序列作为一个特性的time step序列，而不是多个特性的单一time step。 \n",
    "也就是说我们把ABC 看成独立的一个特征组成的多个时间序列，而不是把ABC看成一个多个特征组成一个时间序列。 \n",
    "\n",
    "在训练 ABC——D的时候，BCD，CDE，都可以发挥作用。而最开始那种使用方法，只是利用了ABC——D这样一个训练样本。\n",
    "```py\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "```\n",
    "timesteps这个参数，我们设置了3，而不是前面的1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive LSTM to learn three-char time steps to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经学会了用字母表中的三个字母来预测下一个字母的顺序。它可以显示字母表中的任意三个字母的随机序列，并预测下一个字母。\n",
    "\n",
    "LSTM网络是有状态的。它们应该能够学习整个字母表序列，但是在默认情况下，keras在每次训练之后重新设置网络状态。\n",
    "\n",
    "我们还没有展示出循环神经网络的强大之处，因为上面这个问题我们用多层感知器，足够多的神经元，足够多的迭代次数也可以很好的解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras实现的LSTM在每一个batch以后，都重置了LSTM的状态。\n",
    "\n",
    "这表明，如果我们的批处理大小足够容纳所有输入模式，如果所有输入模式都按顺序排序，LSTM就可以使用序列中的序列上下文来更好地学习序列。\n",
    "\n",
    "通过修改第一个示例来学习一对一映射，并将批处理大小从1增加到训练数据集的大小，我们可以很容易地演示这一点。\n",
    "\n",
    "此外，在每个epoch前，keras都重置了训练数据集。为了确保训练数据模式保持顺序，我们可以禁用这种洗牌。\n",
    "```py\n",
    "model.fit(X, y, epochs=500, batch_size=len(dataX), verbose=2, shuffle=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stateful LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "for i in range(300):\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "model.reset_states()\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demonstrate some model predictions\n",
    "seed = [char_to_int[alphabet[0]]]\n",
    "for i in range(0, len(alphabet)-1):\n",
    "\tx = numpy.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demonstrate a random starting point\n",
    "letter = \"K\"\n",
    "seed = [char_to_int[letter]]\n",
    "print(\"New start: \", letter)\n",
    "for i in range(0, 5):\n",
    "\tx = numpy.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，网络已经完美地记住了整个字母表。它使用了样本的上下文，并学习了预测序列中下一个字符所需要的依赖关系。\n",
    "\n",
    "我们还可以看到，如果我们用第一个字母输入网络，它就能正确地对字母表的其他部分进行正确的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结\n",
    "* 如何开发一个简单的LSTM网络，一个字符到一个字符的预测。\n",
    "* 如何配置一个简单的LSTM，以在一个示例中跨时间步骤学习一个序列。\n",
    "* 如何配置LSTM来通过手动管理状态来学习跨示例的序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
