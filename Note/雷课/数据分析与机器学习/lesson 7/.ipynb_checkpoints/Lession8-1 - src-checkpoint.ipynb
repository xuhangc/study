{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>深度学习（二）——NLP处理</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 课程内容\n",
    "\n",
    "* 1.循环神经网络RNN\n",
    "* 2.长短期记忆网络LSTM\n",
    "* 3.NLP与Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.循环神经网络RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 循环神经网络简介\n",
    "\n",
    "CNN等传统神经网络的局限在于：将固定大小的向量作为输入（比如一张图片），然后输出一个固定大小的向量（比如不同分类的概率）。不仅如此，CNN还按照固定的计算步骤（比如模型中层的数量）来实现这样的输入输出。这样的神经网络没有持久性：假设你希望对电影中每一帧的事件类型进行分类，传统的神经网络就没有办法使用电影中先前的事件推断后续的事件。\n",
    "\n",
    "RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。在自然语言处理(NLP)领域，RNN已经可以做语音识别、机器翻译、生成手写字符，以及构建强大的语言模型 (Sutskever et al.)，(Graves)，(Mikolov et al.)（字符级别和单词级别的都有。在机器视觉领域，RNN也非常流行。包括帧级别的视频分类，图像描述，视频描述以及基于图像的Q&A等等。\n",
    "\n",
    "#### RNN结构\n",
    "\n",
    "神经网络的模块A正在读取某个输入xt，并输出一个值ht，循环可以使得信息从当前步传递到下一步，将这个循环展开，如下所示。链式的特征揭示了 RNN 本质上是与序列和列表相关的，它们是对于这类数据的最自然的神经网络架构。\n",
    "\n",
    "![image](images/2.png)\n",
    "\n",
    "#### 长期依赖问题\n",
    "\n",
    "RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。\n",
    "\n",
    "![image](images/3.png)\n",
    "\n",
    "但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France... I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。\n",
    "\n",
    "![image](images/4.png)\n",
    "\n",
    "在理论上，RNN 绝对可以处理这样的长期依赖问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。然而，幸运的是，LSTM 并没有这个问题！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 理解循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#只返回最后一个时间步的输出\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#返回完整的状态序列\n",
    "model =Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将多个循环层逐个堆叠---需要所有中间层都返回完整的输出序列\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在IMDB数据上应用SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 1000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(input_train), 'train_sequences')\n",
    "print(len(input_test), 'test_sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train.shape: ',input_train.shape )\n",
    "print('input_test.shape: ', input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在IMDB数据上应用LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.长短期记忆网络LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 长短期记忆网络简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Long Short Term 网络—— 一般就叫做 LSTM，是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由Hochreiter & Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！\n",
    "\n",
    "所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。\n",
    "\n",
    "![image](images/5.png)\n",
    "\n",
    "LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。\n",
    "\n",
    "![image](images/6.png)\n",
    "\n",
    "#### LSTM网络详解\n",
    "\n",
    "下面对LSTM网络进行详细说明，首先说明一下图中使用的图标，如下：\n",
    "\n",
    "![image](images/7.png)\n",
    "\n",
    "在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表按位 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。\n",
    "\n",
    "LSTM 的关键就是细胞状态cell state，水平线在图上方贯穿运行，也就是贯穿每个重复结构的上面这条flow。细胞状态类似于传送带，直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。这条flow其实就承载着之前所有状态的信息，每当flow流经一个重复结构A的时候，都会有相应的操作来决定舍弃什么旧的信息以及添加什么新的信息。\n",
    "\n",
    "![image](images/8.png)\n",
    "\n",
    "LSTM 有通过精心设计对信息增减进行控制的结构，称作为“门”。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个按位的乘法操作。Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！\n",
    "\n",
    "![image](images/9.png)\n",
    "\n",
    "LSTM 拥有三个门，来保护和控制细胞状态，分别是遗忘门 (forget gate)、输入门 (input gate)、输出门 (output gate)。下面对这三个门进行详细讲解\n",
    "\n",
    "#### 遗忘门 (forget gate)\n",
    "\n",
    "遗忘门决定了要从cell state中舍弃什么信息。其通过输入上一状态的输出ht-1、当前状态输入信息Xt到一个Sigmoid函数中，产生一个介于0到1之间的数值，与cell state相乘之后来确定舍弃（保留）多少信息。0 表示“完全舍弃”，1 表示“完全保留”。\n",
    "\n",
    "![image](images/10.png)\n",
    "\n",
    "上式中，<img src=\"https://latex.codecogs.com/svg.latex?\\dpi{300}&space;W_{f}\" title=\"W_{f}\" />可以写为：<img src=\"https://latex.codecogs.com/svg.latex?\\dpi{300}&space;[W_{f}]\\begin{bmatrix}&space;h_{t-1}\\\\&space;x_{t}&space;\\end{bmatrix}=\\begin{bmatrix}&space;W_{fh}&space;&&space;W_{fx}&space;\\end{bmatrix}\\begin{bmatrix}&space;h_{t-1}\\\\&space;x_{t}&space;\\end{bmatrix}=W_{fh}h_{t-1}&plus;W_{fx}x_{t}\" title=\"[W_{f}]\\begin{bmatrix} h_{t-1}\\\\ x_{t} \\end{bmatrix}=\\begin{bmatrix} W_{fh} & W_{fx} \\end{bmatrix}\\begin{bmatrix} h_{t-1}\\\\ x_{t} \\end{bmatrix}=W_{fh}h_{t-1}+W_{fx}x_{t}\" />\n",
    "\n",
    "\n",
    "#### 输入门 (input gate)\n",
    "\n",
    "输入门决定了要往cell state中保存什么新的信息。其通过输入上一状态的输出、当前状态输入信息到一个Sigmoid函数中，产生一个介于0到1之间的数值来确定我们需要保留多少的新信息。同时，一个tanh层会通过上一状态的输出、当前状态输入信息来得到一个将要加入到cell state中的“候选新信息”。\n",
    "\n",
    "![image](images/11.png)\n",
    "\n",
    "现在计算当前时刻的单元状态。它是由上一次的单元状态按元素乘以遗忘门，丢弃掉我们确定需要丢弃的信息；然后把当前输入的单元状态按元素乘以输入门，将两个积加和，这就是新的候选值：\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\dpi{300}&space;C_{t}=f_{t}*C_{t-1}&plus;i_{t}*\\widetilde{C_{t}}\" title=\"C_{t}=f_{t}*C_{t-1}+i_{t}*\\widetilde{C_{t}}\" />\n",
    "\n",
    "![image](images/12.png)\n",
    "\n",
    "#### 输出门 (output gate)\n",
    "\n",
    "输出门决定了要从cell state中输出什么信息。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本，会先有一个Sigmoid函数产生一个介于0到1之间的数值来确定我们需要输出多少cell state中的信息。cell state的信息再与相乘时首先会经过一个tanh层进行“激活”（非线性变换）。得到的就是这个LSTM block的输出信息。\n",
    "\n",
    "![image](images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 理解长短期记忆网络\n",
    "\n",
    "循环神经网络(RNN)的变种是长短期模型网络(LSTM)。它使用广泛，因为它的架构克服了困扰着所有周期性的神经网络梯度消失和梯度爆炸的问题，允许创建非常大的、非常深的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习字母表\n",
    "\n",
    "学习字母表是一个简单的序列预测问题。也就是说，根据字母表的字母，可以预测字母表的下一个字母。\n",
    "\n",
    "这是一个简单的序列预测问题，一旦被理解，就可以被推广到其他的序列预测问题，如时间序列预测和序列分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单字符——单字符的映射的简单LSTM\n",
    "学习如何根据一个字符的上下文来预测字母表中的下一个字符。\n",
    "\n",
    "定义一个LSTM网络，它有32个单元，一个输出层，其中有一个softmax的激活函数来进行预测。由于这是一个多类分类问题，所以我们可以使用在Keras中使用对数损失函数(称为“分类交叉熵”(categorical_crossentropy))，并使用ADAM优化函数对网络进行优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "# 对随机数生成器选定随机数种子，以确保每次执行代码时结果都是相同的。\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "# 为了便于阅读，我们用大写字母来定义字母表。\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "# 神经网络是对数字建模，因此我们需要将字母表中的字母映射到整数值（把字母映射为数字）。我们可以很容易地通过创建字母索引的字典(map)到字符。\n",
    "# 我们还可以创建一个反向查找，以便将预测转换回字符，以便稍后使用。\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "# 造这样一个数据集，用一个字母，来预测下一个字母是什么\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "# 需要将NumPy数组重新构造为LSTM网络所期望的格式，即[samples示例, time steps时间步数, features特征]。\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "# normalize\n",
    "# 需要把我们的整数值归一化到0～1的区间上，这是LSTM网络使用的s形激活函数（sigmoid）的范围\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "# 把这个问题看作是一个序列分类任务，其中26个字母代表一个不同的类。\n",
    "# 用keras的内置的 to_categorical()函数把输出output(y)进行 one－hot编码(one-hot指n维单位向量a=(0,…,0,1,0,…,0))作为输出层的结果。\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "# 定义一个LSTM网络，它有32个单元，一个输出层，其中有一个softmax的激活函数来进行预测。\n",
    "# 由于这是一个多类分类问题，所以我们可以使用在Keras中使用对数损失函数(称为“分类交叉熵”(categorical_crossentropy))，\n",
    "# 并使用ADAM优化函数对网络进行优化。\n",
    "# 该模型以500批次(epochs)，每批次数据输入大小(batch)为1的形式训练\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "# summarize performance of the model\n",
    "# 对整个训练集的性能进行评估和总结\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "# 通过网络重新运行训练数据，并生成预测，将输入和输出对转换回原来的字符格式，以获得关于网络如何了解问题的视觉效果。\n",
    "for pattern in dataX:\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "原因是可怜的lstm单元根本没有可以利用的上下文章信息。 \n",
    "每个输入输出模式都以随机的顺序显示在网络中，并且网络的状态在每个模式之后被重置(每个批处理的每个批次包含一个模式)。\n",
    "\n",
    "这是对LSTM网络架构的滥用，因为我们把它当作了一个标准的多层感知器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三字符特征——单字符的映射的简单LSTM\n",
    "\n",
    "在多层感知器中添加更多上下文最流行的方法是特征窗口方法(Feature Window method)。\n",
    "\n",
    "即序列中的前面步骤的输出被作为附加的输入特性提供给网络。我们可以用相同的技巧，为LSTM网络提供更多的上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive LSTM to learn three-char window to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "# 在这里，我们将序列长度从1增加到3，例如: 我们把输入从一个字符升到三个字符。\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "# 将序列中的每个元素作为网络的一个新输入特性提供给它。这需要修改输入序列在数据准备步骤中的reshape:\n",
    "X = numpy.reshape(dataX, (len(dataX), 1, seq_length))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "发现有了一点点的提升，但是这一点点的提升未必是真的，梯度下降算法本来就是具有随机性的。\n",
    "\n",
    "也就是说我们再一次的错误使用了lstm循环神经网络。 \n",
    "我们确实给了上下文，但是并不是合适的方式， \n",
    "实际上，字母序列ABC才是一个特征的timesteps，而不是单独的一个特征的timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用lstm的关键是以时间序列(time steps)的方法来提供上下文，而不是像其他网络结构(CNN)一样，通过windowed features的方式。\n",
    "\n",
    "timesteps这个参数，我们设置了3，而不是前面的1。\n",
    "\n",
    "不同之处是，对输入数据的reshape是将输入序列作为一个特性的time step序列，而不是多个特性的单一time step。 \n",
    "也就是说我们把ABC 看成独立的一个特征组成的多个时间序列，而不是把ABC看成一个多个特征组成一个时间序列。 \n",
    "\n",
    "在训练 ABC——D的时候，BCD，CDE，都可以发挥作用。而最开始那种使用方法，只是利用了ABC——D这样一个训练样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive LSTM to learn three-char time steps to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "已经学会了用字母表中的三个字母来预测下一个字母的顺序。它可以显示字母表中的任意三个字母的随机序列，并预测下一个字母。\n",
    "\n",
    "LSTM网络是有状态的。它们应该能够学习整个字母表序列，但是在默认情况下，keras在每次训练之后重新设置网络状态。\n",
    "\n",
    "我们还没有展示出循环神经网络的强大之处，因为上面这个问题我们用多层感知器，足够多的神经元，足够多的迭代次数也可以很好的解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras实现的LSTM在每一个batch以后，都重置了LSTM的状态。\n",
    "\n",
    "这表明，如果我们的批处理大小足够容纳所有输入模式，如果所有输入模式都按顺序排序，LSTM就可以使用序列中的序列上下文来更好地学习序列。\n",
    "\n",
    "通过修改第一个示例来学习一对一映射，并将批处理大小从1增加到训练数据集的大小，我们可以很容易地演示这一点。\n",
    "\n",
    "此外，在每个epoch前，keras都重置了训练数据集。为了确保训练数据模式保持顺序，我们可以禁用这种洗牌。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stateful LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "for i in range(300):\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "model.reset_states()\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "seed = [char_to_int[alphabet[0]]]\n",
    "for i in range(0, len(alphabet)-1):\n",
    "\tx = numpy.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()\n",
    "# demonstrate a random starting point\n",
    "letter = \"K\"\n",
    "seed = [char_to_int[letter]]\n",
    "print(\"New start: \", letter)\n",
    "for i in range(0, 5):\n",
    "\tx = numpy.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "可以看到，网络已经完美地记住了整个字母表。它使用了样本的上下文，并学习了预测序列中下一个字符所需要的依赖关系。\n",
    "\n",
    "我们还可以看到，如果我们用第一个字母输入网络，它就能正确地对字母表的其他部分进行正确的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可变长度输入——单字符输出的LSTM\n",
    "\n",
    "在上一节中，我们发现keras的“有状态的”LSTM实际上只是重新播放第一个n序列的一个快捷方式，并没有真正学习一个通用的字母表模型。\n",
    "\n",
    "在这一节中，我们将探索一个“无状态”LSTM的变体，它学习了字母表中的随机子序列，并可以根据任意字母或字母序列去预测字母表中的下一个字母。\n",
    "\n",
    "首先，我们改变问题的框架。为了简化，我们定义一个最大的输入序列长度(maximum input sequence length)，并将其设置为5这样的小值来加速训练。这就定义了(用于训练的字母表的)子序列的最大长度。在扩展中，如果我们允许循环回到序列的开始，这就可以设置为完整的字母表(26)或更长。\n",
    "\n",
    "我们还需要定义要创建的随机序列的数量，在本例中为1000。这也可能是更多或更少。我希望实际需要的模式更少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM with Variable Length Input Sequences to One Character Output\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 1000\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "\tstart = numpy.random.randint(len(alphabet)-2)\n",
    "\tend = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "\tsequence_in = alphabet[start:end+1]\n",
    "\tsequence_out = alphabet[end + 1]\n",
    "\tdataX.append([char_to_int[char] for char in sequence_in])\n",
    "\tdataY.append(char_to_int[sequence_out])\n",
    "\tprint(sequence_in, '->', sequence_out)\n",
    "# convert list of lists to array and pad sequences if needed\n",
    "# 输入序列的长度在1和maxlen之间变化，因此需要zero padding(零填充)。\n",
    "# 使用了left-hand-side (prefix) padding和 keras自带的pad_sequences()函数。\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(X, (X.shape[0], max_len, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], 1)))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for i in range(20):\n",
    "\tpattern_index = numpy.random.randint(len(dataX))\n",
    "\tpattern = dataX[pattern_index]\n",
    "\tx = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "\tx = numpy.reshape(x, (1, max_len, 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "可以看到，尽管这个模型没有从随机生成的子序列中完美地学习字母表，但它做得很好。该模型没有进行调整，可能需要更多的训练或更大的网络，或者两者都需要(为读者提供一个练习)。\n",
    "\n",
    "这是一个很好的自然扩展，对于“每个批处理中的所有顺序输入示例”，都可以在上面学到，它可以处理特殊的查询，但是这一次是任意的序列长度(最多的是最大长度)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 小结\n",
    "* 如何开发一个简单的LSTM网络，一个字符到一个字符的预测。\n",
    "* 如何配置一个简单的LSTM，以在一个示例中跨时间步骤学习一个序列。\n",
    "* 如何配置LSTM来通过手动管理状态来学习跨示例的序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.NLP与Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然语言处理（NLP）如今越来越流行，在深度学习开发的背景下变得尤为引人注目。在人工智能领域中，自然语言处理（NLP）从文本中理解和提取重要信息，并基于文本数据进行进一步的数据训练，其主要任务包括语音识别和生成、文本分析、情感分析、机器翻译等。\n",
    "\n",
    "* NLTK（Python自然语言工具包）用于诸如标记化、词形还原、词干化、解析、POS标注等任务。该库具有几乎所有NLP任务的工具。\n",
    "\n",
    "* Spacy是NLTK的主要竞争对手。这两个库可用于相同的任务。\n",
    "\n",
    "* Scikit-learn为机器学习提供了一个大型库。此外还提供了用于文本预处理的工具。\n",
    "\n",
    "* Gensim是一个主题和向量空间建模、文档集合相似性的工具包。\n",
    "\n",
    "* Pattern库的一般任务是充当Web挖掘模块。因此，它仅支持自然语言处理（NLP）作为辅助任务。\n",
    "\n",
    "* Polyglot是自然语言处理（NLP）的另一个Python工具包。它不是很受欢迎，但也可以用于各种NLP任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1Gensim介绍\n",
    "作为自然语言处理爱好者，大家都应该听说过或使用过大名鼎鼎的Gensim吧，这是一款具备多种功能的神器。\n",
    "Gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。\n",
    "它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，\n",
    "支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本概念\n",
    "\n",
    "* 语料（Corpus）：一组原始文本的集合，用于无监督地训练文本主题的隐层结构。语料中不需要人工标注的附加信息。在Gensim中，Corpus通常是一个可迭代的对象（比如列表）。每一次迭代返回一个可用于表达文本对象的稀疏向量。\n",
    "\n",
    "* 向量（Vector）：由一组文本特征构成的列表。是一段文本在Gensim中的内部表达。\n",
    "\n",
    "* 稀疏向量（SparseVector）：通常，我们可以略去向量中多余的0元素。此时，向量中的每一个元素是一个(key, value)的元组\n",
    "\n",
    "* 模型（Model）：是一个抽象的术语。定义了两个向量空间的变换（即从文本的一种向量表达变换为另一种向量表达）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2训练语料的预处理\n",
    "这一次，让我们从表示为字符串的文档开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tools\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个由九个文档组成的小型语料库，每个文档只包含一个句子。\n",
    "\n",
    "首先，让我们对文档进行标记，删除常用单词（使用玩具停止列表）以及仅在语料库中出现一次的单词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要将文档转换为向量，我们将使用名为bag-of-words的文档表示 。在此表示中，每个文档由一个向量表示，其中每个向量元素表示问题 - 答案对，仅通过它们的（整数）id来表示问题是有利的。问题和ID之间的映射称为字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['human', 'interface', 'computer', 'survey', 'user']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('tmp/deerwester.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们为语料库中出现的所有单词分配了一个唯一的整数id gensim.corpora.dictionary.Dictionary。这会扫描文本，收集字数和相关统计数据。最后，我们看到在处理过的语料库中有12个不同的单词，这意味着每个文档将由12个数字表示（即，通过12-D向量）。要查看单词及其ID之间的映射："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human': 0, 'interface': 1, 'computer': 2, 'survey': 3, 'user': 4, 'system': 5, 'response': 6, 'time': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要将标记化文档实际转换为向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "该函数doc2bow()只计算每个不同单词的出现次数，将单词转换为整数单词id，并将结果作为稀疏向量返回。 因此，稀疏向量 [(0, 1), (1, 1)] 读取：在文档“人机交互”中，单词computer （id 0）和human（id 1）出现一次; 其他十个字典单词（隐含地）出现零次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(1, 1), (4, 1), (5, 1), (8, 1)], [(0, 1), (5, 2), (8, 1)], [(4, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(3, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('tmp/deerwester.mm', corpus)  # store to disk, for later use\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3主题向量的变换\n",
    "对文本向量的变换是Gensim的核心。通过挖掘语料中隐藏的语义结构特征，我们最终可以变换出一个简洁高效的文本向量。\n",
    "\n",
    "在Gensim中，每一个向量变换的操作都对应着一个主题模型，例如上一小节提到的对应着词袋模型的doc2bow变换。每一个模型又都是一个标准的Python对象。下面以TF-IDF模型为例，介绍Gensim模型的一般使用方法。\n",
    "\n",
    "首先是模型对象的初始化。通常，Gensim模型都接受一段训练语料（注意在Gensim中，语料对应着一个稀疏向量的迭代器）作为初始化的参数。显然，越复杂的模型需要配置的参数越多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建转换\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，corpus是一个返回bow向量的迭代器。这两行代码将完成对corpus中出现的每一个特征的IDF值的统计工作。\n",
    "\n",
    "接下来，我们可以调用这个模型将任意一段语料（依然是bow向量的迭代器）转化成TFIDF向量（的迭代器）。需要注意的是，这里的bow向量必须与训练语料的bow向量共享同一个特征字典（即共享同一个向量空间）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "# 变换向量\n",
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(2, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.3244870206138555), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.44424552527467476)]\n",
      "[(1, 0.5710059809418182), (4, 0.4170757362022777), (5, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(0, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(4, 0.45889394536615247), (6, 0.6282580468670046), (7, 0.6282580468670046)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(3, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "# 或者将转换应用于整个语料库：\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 可以将训练好的模型持久化到磁盘上，以便下一次使用：\n",
    "tfidf.save(\"tmp/model.tfidf\")\n",
    "new_tfidf = models.TfidfModel.load(\"tmp/model.tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4文档相似度的计算\n",
    "在得到每一篇文档对应的主题向量后，我们就可以计算文档之间的相似度，进而完成如文本聚类、信息检索之类的任务。在Gensim中，也提供了这一类任务的API接口。\n",
    "\n",
    "以信息检索为例。对于一篇待检索的query，我们的目标是从文本集合中检索出主题相似度最高的文档。\n",
    "\n",
    "首先，我们需要将待检索的query和文本放在同一个向量空间里进行表达（以LSI向量空间为例）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 12 features, 28 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary.load('tmp/deerwester.dict')\n",
    "corpus = corpora.MmCorpus('tmp/deerwester.mm') # comes from the first tutorial, \"From strings to vectors\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个二维LSI空间：\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.46182100453271513), (1, 0.0700276652790003)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化查询\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* similarities.MatrixSimilarity只有当整个向量集适合内存时，该类才适用。例如，当与此类一起使用时，一百万个文档的语料库在256维LSI空间中将需要2GB的RAM。\n",
    "* 如果没有2GB的可用RAM，则需要使用similarities.Similarity该类。此类通过在磁盘上的多个文件（称为分片）之间拆分索引，在固定内存中运行。它使用similarities.MatrixSimilarity和similarities.SparseMatrixSimilarity内部，所以它仍然很快，虽然稍微复杂一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.998093), (1, 0.93748635), (2, 0.9984453), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.09879464), (8, 0.050041765)]\n"
     ]
    }
   ],
   "source": [
    "# 要获得我们的查询文档与九个索引文档的相似性：\n",
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "余弦测量返回范围中的相似度（越大，越相似），因此第一个文档的得分为0.99809301等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.9984453), (0, 0.998093), (3, 0.9865886), (1, 0.93748635), (4, 0.90755945), (8, 0.050041765), (7, -0.09879464), (6, -0.10639259), (5, -0.12416792)]\n"
     ]
    }
   ],
   "source": [
    "# 将这些相似性按降序排序，并获得查询 人机交互 的最终答案：\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5加载预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model = models.KeyedVectors.load_word2vec_format('tmp/zhwiki_2017_03.sg_50d.word2vec',binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.836317 -0.561314 -0.43831   0.064629 -0.188466  0.379224 -0.057194\n",
      "  0.819906 -0.14666  -0.153498  0.409435 -0.0617    0.765084  0.512464\n",
      "  0.509696  0.617913 -0.260147  0.732777 -0.371359 -0.459202 -0.430338\n",
      " -0.008408 -0.427761  0.897292 -0.00312  -0.245841  0.290636 -0.191271\n",
      " -0.101611 -0.332142 -0.362474  0.278584 -0.972548 -0.1305    0.358514\n",
      "  0.484478 -0.140359 -0.605624 -1.032877 -0.586738  0.130927 -0.160141\n",
      "  0.381306 -0.231827  0.812195  0.214016  0.185493  0.504    -0.368843\n",
      " -0.746841]\n"
     ]
    }
   ],
   "source": [
    "print(new_model['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('上海', 0.8524885773658752), ('广州', 0.8133988976478577), ('南京', 0.8087798953056335), ('沈阳', 0.8043987154960632), ('长春', 0.7917946577072144), ('哈尔滨', 0.7850498557090759), ('杭州', 0.7794261574745178), ('济南', 0.7794216275215149), ('福寿园', 0.7779054045677185), ('潞河', 0.7772673964500427)] 10\n"
     ]
    }
   ],
   "source": [
    "# 得到与一个词最相关的若干词及相似程度\n",
    "print(new_model.similar_by_word('北京'), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5909499429590144\n",
      "0.8381448239950575\n"
     ]
    }
   ],
   "source": [
    "# 得到两组词的相似度\n",
    "list1 = [u'核能']\n",
    "list2 = [u'电能']\n",
    "list3 = [u'电力']\n",
    "list_sim1 =  new_model.n_similarity(list1, list2)\n",
    "print(list_sim1)\n",
    "list_sim2 = new_model.n_similarity(list2, list3)\n",
    "print(list_sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tools\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\models\\keyedvectors.py:568: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "# 得到一组词中最无关的词\n",
    "list4 = [u'汽车', u'火车', u'飞机', u'北京']\n",
    "print(new_model.doesnt_match(list4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
