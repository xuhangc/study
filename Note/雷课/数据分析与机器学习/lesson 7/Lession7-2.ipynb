{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>自然语言处理--文本分类实践</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课程内容\n",
    "\n",
    "* 1.IMDB数据集介绍\n",
    "* 2.传统分类方法实践\n",
    "* 3.深度学习方法实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.IMDB数据集介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB电影评论分类\n",
    "\n",
    "二分类可能是机器学习最常解决的问题。我们将基于评论的内容将电影评论分类：正类和父类。\n",
    "\n",
    "数据集来自 IMDB 的 50,000 条电影评论，以情绪（正面/负面）标记，其中25,000条用来训练，25,000条用来测试，每个部分正负评论各占50%。评论已经过预处理，并编码为词索引（整数）的序列表示。影评就是类似\"I like this movie\"这样的一段话, IMDB将影评中全部出现的字进行统计, 并给它们标号，比如\"I like this movie\"就可以用[1 4 23 6]这样的数组来表示。为了方便起见，将词按数据集中出现的频率进行索引，例如整数3编码数据中第三个最频繁的词。\n",
    "\n",
    "IMDB数据集下载速度慢，可以在QQ群文件中找到**imdb.npz**下载，下载后放到C:/Users/用户名/**.keras/datasets/**目录下，即可正常运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载IMDB数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### load packages ####################\n",
    "%matplotlib inline\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是加载IMDB数据集：\n",
    "```py\n",
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=3000)\n",
    "```\n",
    "* num_words=3000表示保留训练数据中前3000个最常出现的单词。\n",
    "\n",
    "* train_data和test_data这两个变量都是评论组成的列表，每条评论又是单词索引组成的列表。\n",
    "\n",
    "* train_labels和test_labels都是0和1组成的列表，0为负面评论，1为正面评论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### load data ####################\n",
    "######### 只考虑最常见的3000个词 ########\n",
    "num_words = 3000\n",
    "\n",
    "######### 导入数据 #########\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看第一篇文本内容\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看第一篇情感类别\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练数据类型以及形状\n",
    "print(type(train_data))\n",
    "print(type(train_data[0]))\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练数据类别类型及形状\n",
    "print(type(train_labels))\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对数据集切片，取一部分来实验，为了演示节约时间\n",
    "train_data = train_data[:5000]\n",
    "train_labels = train_labels[:5000]\n",
    "test_data = test_data[:5000]\n",
    "test_labels = test_labels[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计文本长度\n",
    "import numpy as np\n",
    "text_len_li = list(map(len, train_data))\n",
    "print(\"最短文本长度=\", min(text_len_li))\n",
    "print(\"最长文本长度=\", max(text_len_li))\n",
    "print(\"平均文本长度=\", np.mean(text_len_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制文本长度分布图\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(text_len_li, bins=range(min(text_len_li), max(text_len_li)+50, 50))\n",
    "plt.title(\"IMDb text len distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.传统分类方法实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 数据预处理——one-hot编码\n",
    "\n",
    "不能直接将list类型的数据送到算法模型中训练，必须将list类型转换为等长向量。\n",
    "\n",
    "* 填充列表使每个列表长度都相同。\n",
    "* 将列表进行one-hot编码，转换成0、 1向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义一个函数\n",
    "def vectorize_onehot(sequences, dimension=3000):\n",
    "    results = np.zeros((len(sequences), dimension)) #数据集长度，每个评论维度3000\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        sequence = [x -1 for x in sequence] \n",
    "        results[i, sequence] = 1 # one-hot\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = vectorize_onehot(train_data)\n",
    "x_test = vectorize_onehot(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32') # 向量化标签数据\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看输入数据内容及形状\n",
    "print(x_train.shape)\n",
    "print(len(x_train[0]), x_train[0])\n",
    "print(len(x_train[1]), x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据分布情况\n",
    "plt.plot(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看输出数据内容及形状\n",
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 多层神经网络\n",
    "\n",
    "采用三层神经网络对文本数据进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = Sequential()\n",
    "baseline_model.add(Dense(16, activation='relu', input_shape=(num_words,)))\n",
    "baseline_model.add(Dense(16, activation='relu'))\n",
    "baseline_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "baseline_model.compile(optimizer='adam',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_history = baseline_model.fit(x_train,\n",
    "                                      y_train,\n",
    "                                      epochs=20,\n",
    "                                      batch_size=512,\n",
    "                                      validation_data=(x_test, y_test),\n",
    "                                      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证模型\n",
    "scores = baseline_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 欠拟合与过拟合\n",
    "* 过拟合：过分依赖训练数据\n",
    "* 欠拟合：未能学习训练数据中的关系\n",
    "\n",
    "将仅使用`Dense`层作为基准来创建一个简单的模型，然后创建较小和较大的版本并进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 欠拟合模型\n",
    "创建一个隐藏单元更少的模型，以与刚刚创建的基线模型进行比较："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_model = Sequential([\n",
    "    Dense(4, activation='relu', input_shape=(num_words,)),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "smaller_model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "smaller_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并使用相同的数据训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_history = smaller_model.fit(x_train,\n",
    "                                    y_train,\n",
    "                                    epochs=20,\n",
    "                                    batch_size=512,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 过拟合模型\n",
    "创建一个更大的模型，并查看它开始过拟合的速度。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(num_words,)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bigger_model.compile(optimizer='adam',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "bigger_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次，使用相同的数据训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_history = bigger_model.fit(x_train, y_train,\n",
    "                                  epochs=20,\n",
    "                                  batch_size=512,\n",
    "                                  validation_data=(x_test, y_test),\n",
    "                                  verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 模型对比\n",
    "绘制训练和验证损失，实线表示训练损失，而虚线表示验证损失（请记住：验证损失越小表示模型越好）。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(histories, key='loss'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.xlim([0,max(history.epoch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制loss\n",
    "plot_history([('baseline', baseline_history),\n",
    "              ('smaller', smaller_history),\n",
    "              ('bigger', bigger_history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制acc\n",
    "plot_history([('baseline', baseline_history),\n",
    "              ('smaller', smaller_history),\n",
    "              ('bigger', bigger_history)], key='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "较小的网络存在欠拟合，测试数据仍有改进空间时，一是模型过于简单，一是训练时间不够。\n",
    "\n",
    "较小的网络存在过拟合，导致训练和验证损失之间存在较大差异，由于网络的参数过多，将能够更快地对训练数据进行建模（导致较低的训练损失）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 防止过拟合的策略\n",
    "\n",
    "#### 添加 dropout\n",
    "\n",
    "dropout 是 Hinton 和他在多伦多大学的学生开发的最有效，最常用的神经网络正则化技术之一。应用于图层的辍学包括在训练过程中随机“dropping out”（即设置为零）该图层的许多输出特征。假设在训练过程中，给定的图层通常会为给定的输入样本返回向量`[0.2, 0.5, 1.3, 0.8, 1.1]`；应用删除后，此向量将有一些零个条目随机分布，例如`[0, 0.5, 1.3, 0, 1.1]`。 “dropout 率”是被清零的特征的一部分。通常设置在0.2到0.5之间。在测试时，不会丢失任何单元，而是将图层的输出值按等于丢失率的比例缩小，以平衡一个活跃的单元（而不是训练时）的事实。\n",
    "\n",
    "在keras中，您可以通过Dropout层在网络中引入Dropout，该层将立即应用于该层的输出。\n",
    "\n",
    "让我们在IMDB网络中添加两个Dropout层，看看它们在减少过拟合方面的表现如何："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpt_model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(num_words,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "dpt_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "dpt_model_history = dpt_model.fit(x_train, y_train,\n",
    "                                  epochs=20,\n",
    "                                  batch_size=512,\n",
    "                                  validation_data=(x_test, y_test),\n",
    "                                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('baseline', baseline_history),\n",
    "              ('dropout', dpt_model_history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history([('baseline', baseline_history),\n",
    "              ('dropout', dpt_model_history)], key='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加 dropout 是对基线模型的明显改进，以下是防止神经网络过拟合的最常用方法：\n",
    "\n",
    "* 获取更多数据\n",
    "* 减少网络容量\n",
    "* 添加权重调整\n",
    "* 添加 dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 数据预处理——词袋模型\n",
    "\n",
    "```py\n",
    "from collections import Counter\n",
    "def vectorize_wordbag(sequences, dimension=3000):\n",
    "    results = np.zeros((len(sequences), dimension)) #数据集长度，每个评论维度3000\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        ct = Counter(sequence)\n",
    "        results[i] = [ct[i+1] for i in range(dimension)] # 词袋模型\n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习\n",
    "# 基于词袋模型表示的分类方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.深度学习方法实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 序列预处理\n",
    "* pad_sequences：填充序列,将标量序列转化为2D的numpy array。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先要明白整数序列是不能直接输入神经网络的，所以要先将列表转换为张量，分别为填充列表和对列表进行one_hot编码。\n",
    "```py\n",
    "keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, padding='pre')\n",
    "```\n",
    "\n",
    "解释：\n",
    "\n",
    "* sequences：浮点数或整数构成的两层嵌套列表\n",
    "\n",
    "* maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
    "\n",
    "* padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(len(train_data[0]), train_data[0])\n",
    "print(len(train_data[1]), train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### preprocess data ####################\n",
    "######## 句子长度最长设置为50 ########\n",
    "max_len = 50 # 便于演示\n",
    "\n",
    "######## 对文本进行填充，将文本转成相同长度 ########\n",
    "x_train = preprocessing.sequence.pad_sequences(train_data, maxlen=max_len)\n",
    "x_test = preprocessing.sequence.pad_sequences(test_data, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(len(x_train[0]), x_train[0])\n",
    "print(len(x_train[1]), x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## 对label做one-hot处理 ########\n",
    "num_class = 2\n",
    "y_train = to_categorical(train_labels, num_class)\n",
    "y_test = to_categorical(test_labels, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 嵌入层Embedding\n",
    "* Embedding：将正整数转换为具有固定大小的向量，只能作为第一个隐藏层，与词向量有关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "```py\n",
    "keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform',\n",
    "embeddings_regularizer=None, activity_regularizer=None,\n",
    "embeddings_constraint=None, mask_zero=False, input_length=None)\n",
    "```\n",
    "将正整数（索引值）转换为固定尺寸的稠密向量。例如：[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "该层只能用作模型中的第一层。\n",
    "\n",
    "参数\n",
    "* input_dim: int > 0。词汇表大小，即，最大整数index + 1。\n",
    "* output_dim: int >= 0。词向量的维度。\n",
    "* embeddings_initializer: embeddings 矩阵的初始化方法(详见initializers)。\n",
    "* embeddings_regularizer: embeddings matrix 的正则化方法(详见regularizer)。\n",
    "* embeddings_constraint: embeddings matrix 的约束函数(详见constraints)。\n",
    "* mask_zero: 是否把0 看作为一个应该被遮蔽的特殊的”padding” 值。这对于可变长的循环神经网络层十分有用。如果设定为True，那么接下来的所有层都必须支持masking，否则就会抛出异常。如果mask_zero 为True，作为结果，索引0 就不能被用于词汇表中（input_dim应该与vocabulary + 1 大小相同）。\n",
    "* input_length: 输入序列的长度，当它是固定的时。如果你需要连接Flatten 和Dense 层，则这个参数是必须的（没有它，dense 层的输出尺寸就无法计算）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "import numpy as np\n",
    "model = Sequential()\n",
    "model.add(Embedding(3000, 64, input_length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 卷积神经网络CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.2.1 卷积神经网络简介\n",
    "\n",
    "CNN除了处理图像数据之外，还适用于文本分类。CNN模型首次使用在文本分类，是Yoon Kim在论文(2014 EMNLP) Convolutional Neural Networks for Sentence Classification提出TextCNN。将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息（类似于多窗口大小的ngram），从而能够更好地捕捉局部相关性。\n",
    "\n",
    "![image](images/CNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图很好地诠释了模型的框架。假设我们有一些句子需要对其进行分类。句子中每个词是由n维词向量组成的，也就是说输入矩阵大小为m*n，其中m为句子长度。CNN需要对输入样本进行卷积操作，对于文本数据，filter不再横向滑动，仅仅是向下移动，有点类似于N-gram在提取词与词间的局部相关性。然后对每一个向量进行最大化池化操作并拼接各个池化值，最终得到这个句子的特征表示，将这个句子向量丢给分类器进行分类，至此完成整个流程。\n",
    "\n",
    "（1）嵌入层（Embedding Layer）\n",
    "\n",
    "通过一个隐藏层, 将 one-hot 编码的词投影到一个低维空间中，本质上是特征提取器，在指定维度中编码语义特征。 这样语义相近的词, 它们的欧氏距离或余弦距离也比较近。（作者使用的单词向量是预训练的，方法为fasttext得到的单词向量，当然也可以使用word2vec和GloVe方法训练得到的单词向量）。\n",
    "\n",
    "（2）卷积层（Convolution Laye）\n",
    "\n",
    "在处理图像数据时，CNN使用的卷积核的宽度和高度的一样的，但是在text-CNN中，卷积核的宽度是与词向量的维度一致！这是因为我们输入的每一行向量代表一个词，在抽取特征的过程中，词做为文本的最小粒度。而高度和CNN一样，可以自行设置，高度就类似于n-gram了。由于我们的输入是一个句子，句子中相邻的词之间关联性很高，因此，当我们用卷积核进行卷积时，不仅考虑了词义而且考虑了词序及其上下文（类似于skip-gram和CBOW模型的思想）。\n",
    "\n",
    "（3）池化层（Pooling Layer）\n",
    "\n",
    "因为在卷积层过程中我们使用了不同高度的卷积核，使得我们通过卷积层后得到的向量维度会不一致，所以在池化层中，我们使用1-Max-pooling对每个特征向量池化成一个值，即抽取每个特征向量的最大值表示该特征，而且认为这个最大值表示的是最重要的特征。当我们对所有特征向量进行1-Max-Pooling之后，还需要将每个值给拼接起来。得到池化层最终的特征向量。在池化层到全连接层之前可以加上dropout防止过拟合。\n",
    "\n",
    "（4）全连接层（Fully connected layer）\n",
    "\n",
    "全连接层跟其他模型一样，假设有两层全连接层，第一层可以加上’relu’作为激活函数，第二层则使用softmax激活函数得到属于每个类的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 在IMDB数据上应用CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# 设置参数\n",
    "max_features = 3000\n",
    "embedding_dims = 50\n",
    "filters = 16\n",
    "kernel_size = 3\n",
    "hidden_dims = 64\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=max_len))#和图像处理不同的地方\n",
    "model.add(Conv1D(filters, kernel_size, padding = 'same', activation = 'relu')) #图像Conv2D 文本Conv1D\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_dims, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history_cnn = model.fit(x_train, y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=2,\n",
    "                        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#绘制训练的学习曲线\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_cnn.history['acc']\n",
    "val_acc = history_cnn.history['val_acc']\n",
    "loss = history_cnn.history['loss']\n",
    "val_loss = history_cnn.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#验证模型\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习 \n",
    "# 基于LeNet-5模型的IMDB评论分类\n",
    "# 1. 数据处理\n",
    "\n",
    "# 2. 构建模型\n",
    "\n",
    "# 3. 模型训练\n",
    "\n",
    "# 4. 模型评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 循环神经网络RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 循环神经网络简介\n",
    "\n",
    "CNN等传统神经网络的局限在于：将固定大小的向量作为输入（比如一张图片），然后输出一个固定大小的向量（比如不同分类的概率）。不仅如此，CNN还按照固定的计算步骤（比如模型中层的数量）来实现这样的输入输出。这样的神经网络没有持久性：假设你希望对电影中每一帧的事件类型进行分类，传统的神经网络就没有办法使用电影中先前的事件推断后续的事件。\n",
    "\n",
    "RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。在自然语言处理(NLP)领域，RNN已经可以做语音识别、机器翻译、生成手写字符，以及构建强大的语言模型 (Sutskever et al.)，(Graves)，(Mikolov et al.)（字符级别和单词级别的都有。在机器视觉领域，RNN也非常流行。包括帧级别的视频分类，图像描述，视频描述以及基于图像的Q&A等等。\n",
    "\n",
    "#### RNN结构\n",
    "\n",
    "神经网络的模块A正在读取某个输入xt，并输出一个值ht，循环可以使得信息从当前步传递到下一步，将这个循环展开，如下所示。链式的特征揭示了 RNN 本质上是与序列和列表相关的，它们是对于这类数据的最自然的神经网络架构。\n",
    "\n",
    "![image](images/2.png)\n",
    "\n",
    "#### 长期依赖问题\n",
    "\n",
    "RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。\n",
    "\n",
    "![image](images/3.png)\n",
    "\n",
    "但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France... I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。\n",
    "\n",
    "![image](images/4.png)\n",
    "\n",
    "在理论上，RNN 绝对可以处理这样的长期依赖问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。然而，幸运的是，LSTM 并没有这个问题！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 循环层Recurrent\n",
    "* Recurrent:循环层的抽象类,所有的循环层(SimpleRNN,LSTM,GRU)都继承本层.\n",
    "* **SimpleRNN**:全连接RNN,输出会被回馈到输入.\n",
    "* **LSTM**:长短期记忆模型.\n",
    "* **GRU**:门控循环单元.\n",
    "* ConvLSTM2D:输入变换和循环变换通过卷积实现的LSTM网络.\n",
    "* SimpleRNNCell:SimpleRNN的Cell类.\n",
    "* GRUCell:GRU的Cell类.\n",
    "* LSTMCell:LSTM的Cell类\n",
    "* StackedRNNCells:用于将多个recurrent cell包装起来,实现高效的stacked RNN.\n",
    "* CuDNNGRU:基于CuDNN的快速GRU实现,只能在GPU上运行.\n",
    "* CuDNNLSTM:基于CuDNN的快速LSTM实现,只能在GPU上运行."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleRNN\n",
    "```py\n",
    "keras.layers.SimpleRNN(units, activation='tanh', use_bias=True,\n",
    "kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "bias_initializer='zeros', kernel_regularizer=None,\n",
    "recurrent_regularizer=None, bias_regularizer=None,\n",
    "activity_regularizer=None, kernel_constraint=None,\n",
    "recurrent_constraint=None, bias_constraint=None,\n",
    "dropout=0.0, recurrent_dropout=0.0, return_sequences=False,\n",
    "return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "```\n",
    "完全连接的RNN，其输出将被反馈到输入。\n",
    "\n",
    "参数\n",
    "* units: 正整数，输出空间的维度。\n",
    "* activation: 要使用的激活函数(详见activations)。如果传入None，则不使用激活函数(即线性激活：a(x) = x)。\n",
    "* use_bias: 布尔值，该层是否使用偏置向量。\n",
    "* kernel_initializer: kernel 权值矩阵的初始化器，用于输入的线性转换(详见initializers)。\n",
    "* recurrent_initializer: recurrent_kernel 权值矩阵的初始化器，用于循环层状态的线性转换(详见initializers)。\n",
    "* bias_initializer: 偏置向量的初始化器(详见initializers).\n",
    "* kernel_regularizer: 运用到kernel 权值矩阵的正则化函数(详见regularizer)。\n",
    "* recurrent_regularizer: 运用到recurrent_kernel 权值矩阵的正则化函数(详见regularizer)。\n",
    "* bias_regularizer: 运用到偏置向量的正则化函数(详见regularizer)。\n",
    "* activity_regularizer: 运用到层输出（它的激活值）的正则化函数(详见regularizer)。\n",
    "* kernel_constraint: 运用到kernel 权值矩阵的约束函数(详见constraints)。\n",
    "* recurrent_constraint: 运用到recurrent_kernel 权值矩阵的约束函数(详见constraints)。\n",
    "* bias_constraint: 运用到偏置向量的约束函数(详见constraints)。\n",
    "* dropout: 在0 和1 之间的浮点数。单元的丢弃比例，用于输入的线性转换。\n",
    "* recurrent_dropout: 在0 和1 之间的浮点数。单元的丢弃比例，用于循环层状态的线性转换。\n",
    "* return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。\n",
    "* return_state: 布尔值。除了输出之外是否返回最后一个状态。\n",
    "* go_backwards: 布尔值(默认False)。如果为True，则向后处理输入序列并返回相反的序列。\n",
    "* stateful: 布尔值(默认False)。如果为True，则批次中索引i 处的每个样品的最后状态将用作下一批次中索引i 样品的初始状态。\n",
    "* unroll: 布尔值(默认False)。如果为True，则网络将展开，否则将使用符号循环。展开可以加速RNN，但它往往会占用更多的内存。展开只适用于短序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#只返回最后一个时间步的输出\n",
    "\n",
    "#构建一个序列的网络结构\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32)) #添加一层字嵌入式\n",
    "model.add(SimpleRNN(32)) #增加RNN\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#返回完整的状态序列\n",
    "model =Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将多个循环层逐个堆叠---需要所有中间层都返回完整的输出序列\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 在IMDB数据上应用RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dense   #Dense就是常用的全连接层\n",
    "\n",
    "#构造序列的网络结构\n",
    "model = Sequential()\n",
    "\n",
    "#Embedding层只能作为模型的第一层\n",
    "#input_dim：大或等于0的整数，字典长度，即输入数据最大下标+1\n",
    "#output_dim：大于0的整数，代表全连接嵌入的维度\n",
    "model.add(Embedding(max_features, 32))\n",
    "\n",
    "model.add(SimpleRNN(32)) #必须也是32位\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#编译模型\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "#训练模型\n",
    "history_rnn = model.fit(x_train, y_train,\n",
    "                   epochs=2,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#绘制训练的学习曲线\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_rnn.history['acc']\n",
    "val_acc = history_rnn.history['val_acc']\n",
    "loss = history_rnn.history['loss']\n",
    "val_loss = history_rnn.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#验证模型\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN 对比 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = \"Arial\"\n",
    "# Then, \"ALWAYS use sans-serif fonts\"\n",
    "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
    "\n",
    "matplotlib.rcParams['axes.linewidth'] = 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoches = np.arange(1,3)\n",
    "history_RNN_train = np.array(history_rnn.history['acc'])*100\n",
    "history_RNN_test = np.array(history_rnn.history['val_acc'])*100\n",
    "history_CNN_train = np.array(history_cnn.history['acc'])*100\n",
    "history_CNN_test = np.array(history_cnn.history['val_acc'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width1 = 3\n",
    "color1, color2 = 'blue','red'\n",
    "font1, font2, font3 = 18, 16, 18\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epoches,history_RNN_train,'--',color = color1, linewidth= width1 )\n",
    "plt.plot(epoches,history_RNN_test,color = color1, linewidth= width1)\n",
    "plt.plot(epoches,history_CNN_train,'--',color = color2, linewidth= width1)\n",
    "plt.plot(epoches,history_CNN_test,color = color2, linewidth= width1)\n",
    "plt.title('Model accuracy', fontsize = font1)\n",
    "plt.ylabel('Accuracy (%)', fontsize = font1)\n",
    "plt.yticks(fontsize = font2)\n",
    "plt.xlabel('Epoches', fontsize = font1)\n",
    "plt.xticks(fontsize = font2)\n",
    "plt.legend(['RNN Train', 'RNN Test','CNN Train', 'CNN Test'], fontsize = font3)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 长短期记忆网络简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Long Short Term 网络—— 一般就叫做 LSTM，是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由Hochreiter & Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！\n",
    "\n",
    "所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。\n",
    "\n",
    "![image](images/5.png)\n",
    "\n",
    "LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。\n",
    "\n",
    "![image](images/6.png)\n",
    "\n",
    "#### LSTM网络详解\n",
    "\n",
    "下面对LSTM网络进行详细说明，首先说明一下图中使用的图标，如下：\n",
    "\n",
    "![image](images/7.png)\n",
    "\n",
    "在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表按位 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。\n",
    "\n",
    "LSTM 的关键就是细胞状态cell state，水平线在图上方贯穿运行，也就是贯穿每个重复结构的上面这条flow。细胞状态类似于传送带，直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。这条flow其实就承载着之前所有状态的信息，每当flow流经一个重复结构A的时候，都会有相应的操作来决定舍弃什么旧的信息以及添加什么新的信息。\n",
    "\n",
    "![image](images/8.png)\n",
    "\n",
    "LSTM 有通过精心设计对信息增减进行控制的结构，称作为“门”。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个按位的乘法操作。Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！\n",
    "\n",
    "![image](images/9.png)\n",
    "\n",
    "LSTM 拥有三个门，来保护和控制细胞状态，分别是遗忘门 (forget gate)、输入门 (input gate)、输出门 (output gate)。下面对这三个门进行详细讲解\n",
    "\n",
    "#### 遗忘门 (forget gate)\n",
    "\n",
    "遗忘门决定了要从cell state中舍弃什么信息。其通过输入上一状态的输出ht-1、当前状态输入信息Xt到一个Sigmoid函数中，产生一个介于0到1之间的数值，与cell state相乘之后来确定舍弃（保留）多少信息。0 表示“完全舍弃”，1 表示“完全保留”。\n",
    "\n",
    "![image](images/10.png)\n",
    "\n",
    "上式中，<img src=\"https://latex.codecogs.com/svg.latex?\\dpi{300}&space;W_{f}\" title=\"W_{f}\" />可以写为：<img src=\"https://latex.codecogs.com/svg.latex?\\dpi{300}&space;[W_{f}]\\begin{bmatrix}&space;h_{t-1}\\\\&space;x_{t}&space;\\end{bmatrix}=\\begin{bmatrix}&space;W_{fh}&space;&&space;W_{fx}&space;\\end{bmatrix}\\begin{bmatrix}&space;h_{t-1}\\\\&space;x_{t}&space;\\end{bmatrix}=W_{fh}h_{t-1}&plus;W_{fx}x_{t}\" title=\"[W_{f}]\\begin{bmatrix} h_{t-1}\\\\ x_{t} \\end{bmatrix}=\\begin{bmatrix} W_{fh} & W_{fx} \\end{bmatrix}\\begin{bmatrix} h_{t-1}\\\\ x_{t} \\end{bmatrix}=W_{fh}h_{t-1}+W_{fx}x_{t}\" />\n",
    "\n",
    "\n",
    "#### 输入门 (input gate)\n",
    "\n",
    "输入门决定了要往cell state中保存什么新的信息。其通过输入上一状态的输出、当前状态输入信息到一个Sigmoid函数中，产生一个介于0到1之间的数值来确定我们需要保留多少的新信息。同时，一个tanh层会通过上一状态的输出、当前状态输入信息来得到一个将要加入到cell state中的“候选新信息”。\n",
    "\n",
    "![image](images/11.png)\n",
    "\n",
    "现在计算当前时刻的单元状态。它是由上一次的单元状态按元素乘以遗忘门，丢弃掉我们确定需要丢弃的信息；然后把当前输入的单元状态按元素乘以输入门，将两个积加和，这就是新的候选值：\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\dpi{300}&space;C_{t}=f_{t}*C_{t-1}&plus;i_{t}*\\widetilde{C_{t}}\" title=\"C_{t}=f_{t}*C_{t-1}+i_{t}*\\widetilde{C_{t}}\" />\n",
    "\n",
    "![image](images/12.png)\n",
    "\n",
    "#### 输出门 (output gate)\n",
    "\n",
    "输出门决定了要从cell state中输出什么信息。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本，会先有一个Sigmoid函数产生一个介于0到1之间的数值来确定我们需要输出多少cell state中的信息。cell state的信息再与相乘时首先会经过一个tanh层进行“激活”（非线性变换）。得到的就是这个LSTM block的输出信息。\n",
    "\n",
    "![image](images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "```py\n",
    "keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "use_bias=True, kernel_initializer='glorot_uniform',\n",
    "recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "unit_forget_bias=True, kernel_regularizer=None,\n",
    "recurrent_regularizer=None, bias_regularizer=None,\n",
    "activity_regularizer=None, kernel_constraint=None,\n",
    "recurrent_constraint=None, bias_constraint=None,\n",
    "dropout=0.0, recurrent_dropout=0.0,\n",
    "implementation=1, return_sequences=False,\n",
    "return_state=False, go_backwards=False, stateful=False,\n",
    "unroll=False)\n",
    "```\n",
    "长短期记忆网络层- Hochreiter 1997.\n",
    "\n",
    "参数\n",
    "* units: 正整数，输出空间的维度。\n",
    "* activation: 要使用的激活函数(详见activations)。如果传入None，则不使用激活函数(即线性激活：a(x) = x)。\n",
    "* recurrent_activation: 用于循环时间步的激活函数(详见activations)。\n",
    "* use_bias: 布尔值，该层是否使用偏置向量。\n",
    "* kernel_initializer: kernel 权值矩阵的初始化器，用于输入的线性转换(详见initializers)。\n",
    "* recurrent_initializer: recurrent_kernel 权值矩阵的初始化器，用于循环层状态的线性转换(详见initializers)。\n",
    "* bias_initializer: 偏置向量的初始化器(详见initializers).\n",
    "* unit_forget_bias: 布尔值。如果为True，初始化时，将忘记门的偏置加1。将其设置为True同时还会强制bias_initializer=\"zeros\"。\n",
    "* kernel_regularizer: 运用到kernel 权值矩阵的正则化函数(详见regularizer)。\n",
    "* recurrent_regularizer: 运用到recurrent_kernel 权值矩阵的正则化函数(详见regularizer)。\n",
    "* bias_regularizer: 运用到偏置向量的正则化函数(详见regularizer)。\n",
    "* activity_regularizer: 运用到层输出（它的激活值）的正则化函数(详见regularizer)。\n",
    "* kernel_constraint: 运用到kernel 权值矩阵的约束函数(详见constraints)。\n",
    "* recurrent_constraint: 运用到recurrent_kernel 权值矩阵的约束函数(详见constraints)。\n",
    "* bias_constraint: 运用到偏置向量的约束函数(详见constraints)。\n",
    "* dropout: 在0 和1 之间的浮点数。单元的丢弃比例，用于输入的线性转换。\n",
    "* recurrent_dropout: 在0 和1 之间的浮点数。单元的丢弃比例，用于循环层状态的线性转换。\n",
    "* implementation: 实现模式，1 或2。模式1 将把它的操作结构化为更多的小的点积和加法操作，而模式2 将把它们分批到更少，更大的操作中。这些模式在不同的硬件和不同的应用中具有不同的性能配置文件。\n",
    "* return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。\n",
    "* return_state: 布尔值。除了输出之外是否返回最后一个状态。\n",
    "* go_backwards: 布尔值(默认False)。如果为True，则向后处理输入序列并返回相反的序列。\n",
    "* stateful: 布尔值(默认False)。如果为True，则批次中索引i 处的每个样品的最后状态将用作下一批次中索引i 样品的初始状态。\n",
    "* unroll: 布尔值(默认False)。如果为True，则网络将展开，否则将使用符号循环。展开可以加速RNN，但它往往会占用更多的内存。展开只适用于短序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 在IMDB数据上应用LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))  #将每个词原来是整数，转换为 32位的向量\n",
    "model.add(LSTM(32))                       #LSTM\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "history_lstm = model.fit(x_train, y_train,\n",
    "                   epochs=2,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = history_lstm.history['acc']\n",
    "val_acc = history_lstm.history['val_acc']\n",
    "loss = history_lstm.history['loss']\n",
    "val_loss = history_lstm.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#验证模型\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.5 在IMDB数据上应用GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU\n",
    "```py\n",
    "keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "use_bias=True, kernel_initializer='glorot_uniform',\n",
    "recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "kernel_regularizer=None, recurrent_regularizer=None,\n",
    "bias_regularizer=None, activity_regularizer=None,\n",
    "kernel_constraint=None, recurrent_constraint=None,\n",
    "bias_constraint=None, dropout=0.0,\n",
    "recurrent_dropout=0.0, implementation=1,\n",
    "return_sequences=False, return_state=False,\n",
    "go_backwards=False, stateful=False, unroll=False, reset_after=False)\n",
    "```\n",
    "门限循环单元网络- Cho et al. 2014.\n",
    "![image](images/GRU.png)\n",
    "\n",
    "参数\n",
    "* units: 正整数，输出空间的维度。\n",
    "* activation: 要使用的激活函数(详见activations)。默认：双曲正切(tanh)。如果传入None，则不使用激活函数(即线性激活：a(x) = x)。\n",
    "* recurrent_activation: 用于循环时间步的激活函数(详见activations)。默认：分段线性近似sigmoid (hard_sigmoid)。\n",
    "* use_bias: 布尔值，该层是否使用偏置向量。\n",
    "* kernel_initializer: kernel 权值矩阵的初始化器，用于输入的线性转换(详见initializers)。\n",
    "* recurrent_initializer: recurrent_kernel 权值矩阵的初始化器，用于循环层状态的线性转换(详见initializers)。\n",
    "* bias_initializer: 偏置向量的初始化器(详见initializers).\n",
    "* kernel_regularizer: 运用到kernel 权值矩阵的正则化函数(详见regularizer)。\n",
    "* recurrent_regularizer: 运用到recurrent_kernel 权值矩阵的正则化函数(详见regularizer)。\n",
    "* bias_regularizer: 运用到偏置向量的正则化函数(详见regularizer)。\n",
    "* activity_regularizer: 运用到层输出（它的激活值）的正则化函数(详见regularizer)。\n",
    "* kernel_constraint: 运用到kernel 权值矩阵的约束函数(详见constraints)。\n",
    "* recurrent_constraint: 运用到recurrent_kernel 权值矩阵的约束函数(详见constraints)。\n",
    "* bias_constraint: 运用到偏置向量的约束函数(详见constraints)。\n",
    "* dropout: 在0 和1 之间的浮点数。单元的丢弃比例，用于输入的线性转换。\n",
    "* recurrent_dropout: 在0 和1 之间的浮点数。单元的丢弃比例，用于循环层状态的线性转换。\n",
    "* implementation: 实现模式，1 或2。模式1 将把它的操作结构化为更多的小的点积和加法操作，而模式2 将把它们分批到更少，更大的操作中。这些模式在不同的硬件和不同的应用中具有不同的性能配置文件。\n",
    "* return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。\n",
    "* return_state: 布尔值。除了输出之外是否返回最后一个状态。\n",
    "* go_backwards: 布尔值(默认False)。如果为True，则向后处理输入序列并返回相反的序列。\n",
    "* stateful: 布尔值(默认False)。如果为True，则批次中索引i 处的每个样品的最后状态将用作下一批次中索引i 样品的初始状态。\n",
    "* unroll: 布尔值(默认False)。如果为True，则网络将展开，否则将使用符号循环。展开可以加速RNN，但它往往会占用更多的内存。展开只适用于短序列。\n",
    "* reset_after: GRU 公约(是否在矩阵乘法之前或者之后使用重置门)。False =「之前」(默认)，Ture =「之后」( CuDNN 兼容)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习 \n",
    "# 在IMDB数据上应用GRU\n",
    "\n",
    "# 1. 数据处理\n",
    "\n",
    "# 2. 构建模型\n",
    "\n",
    "# 3. 模型训练\n",
    "\n",
    "# 4. 模型评价\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 复杂深度神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 双层LSTM\n",
    "LSTM由于其设计的特点，非常适合用于对时序数据的建模，如文本数据。将词的表示组合成句子的表示，可以采用相加的方法，即将所有词的表示进行加和，或者取平均等方法，但是这些方法没有考虑到词语在句子中前后顺序。如句子“我不觉得他好”。“不”字是对后面“好”的否定，即该句子的情感极性是贬义。使用LSTM模型可以更好的捕捉到较长距离的依赖关系。因为LSTM通过训练过程可以学到记忆哪些信息和遗忘哪些信息。\n",
    "\n",
    "词向量没有采用预训练好的向量，训练中生成，采用的网络结构如图所示：\n",
    "\n",
    "![image](images/14.png)\n",
    "\n",
    "具体代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM\n",
    "\n",
    "max_features = 3000\n",
    "embedding_dims = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "#### Embedding层 ####\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=max_len))\n",
    "\n",
    "#### 两层LSTM，第一层，设置return_sequences参数为True ####\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "\n",
    "#### dropout ####\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#### 两层LSTM，第二层，设置return_sequences参数为False ####\n",
    "model.add(LSTM(256, return_sequences=False))\n",
    "\n",
    "#### dropout ####\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#### 输出层 ####\n",
    "model.add(Dense(num_class, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(x_train, y_train,\n",
    "                   epochs=2,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#验证模型\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 双向LSTM\n",
    "![image](images/BiLSTM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM, Bidirectional\n",
    "\n",
    "max_features = 3000\n",
    "embedding_dims = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "#### Embedding层 ####\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=max_len))\n",
    "\n",
    "#### 双向LSTM####\n",
    "model.add(Bidirectional(LSTM(256))) #Bidirectional\n",
    "\n",
    "#### dropout ####\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#### 输出层 ####\n",
    "model.add(Dense(num_class, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(x_train, y_train,\n",
    "                   epochs=2,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#验证模型\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.4.3 CLSTM\n",
    "![image](images/CLSTM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, Dropout, LSTM\n",
    "\n",
    "max_features = 3000\n",
    "embedding_dims = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=max_len))\n",
    "# CNN层\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# RNN层\n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax')) # sigmoid\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(x_train, y_train,\n",
    "                   epochs=2,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#验证模型\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 问题思考\n",
    "\n",
    "分类函数用的是softmax，用sigmoid是否可行？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "#### 基本网络\n",
    "* Conv1D:一维卷积层\n",
    "* SimpleRNN:全连接RNN\n",
    "* LSTM:长短期记忆模型\n",
    "* GRU:门控循环单元\n",
    "\n",
    "#### 复杂网络\n",
    "* 双层LSTM\n",
    "* 双向LSTM\n",
    "* CNN+LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
