{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>自然语言处理——小练习</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习1：基于词袋模型表示的分类方法\n",
    "\n",
    "```py\n",
    "from collections import Counter\n",
    "def vectorize_wordbag(sequences, dimension=3000):\n",
    "    results = np.zeros((len(sequences), dimension)) #数据集长度，每个评论维度3000\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        ct = Counter(sequence)\n",
    "        results[i] = [ct[i+1] for i in range(dimension)] # 词袋模型\n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小练习\n",
    "# 基于词袋模型表示的分类方法\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "num_words = 3000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "def vectorize_wordbag(sequences, dimension=3000):\n",
    "    results = np.zeros((len(sequences), dimension)) #数据集长度，每个评论维度3000\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        ct = Counter(sequence)\n",
    "        results[i] = [ct[i+1] for i in range(dimension)] # 词袋模型\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_wordbag(train_data)\n",
    "x_test = vectorize_wordbag(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32') # 向量化标签数据\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(num_words,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2：基于LeNet-5模型的IMDB评论分类\n",
    "\n",
    "![image](images/LeNet.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小练习 \n",
    "# 基于LeNet-5模型的IMDB评论分类\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "######### 只考虑最常见的3000个词 ########\n",
    "num_words = 3000\n",
    "\n",
    "######### 导入数据 #########\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "# 对数据集切片，取一部分来实验\n",
    "train_data = train_data[:5000]\n",
    "train_labels = train_labels[:5000]\n",
    "test_data = test_data[:5000]\n",
    "test_labels = test_labels[:5000]\n",
    "\n",
    "# 1. 数据处理\n",
    "max_len = 50\n",
    "x_train = preprocessing.sequence.pad_sequences(train_data, maxlen=max_len)\n",
    "x_test = preprocessing.sequence.pad_sequences(test_data, maxlen=max_len)\n",
    "\n",
    "num_class = 2\n",
    "y_train = to_categorical(train_labels, num_class)\n",
    "y_test = to_categorical(test_labels, num_class)\n",
    "# 2. 构建模型\n",
    "max_features = 3000\n",
    "embedding_dims = 50\n",
    "filters = 16\n",
    "kernel_size = 3\n",
    "hidden_dims = 64\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=max_len))\n",
    "model.add(Conv1D(filters, kernel_size, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(filters, kernel_size, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(120, activation = 'relu'))\n",
    "model.add(Dense(84, activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "# 3. 模型训练\n",
    "# sigmoid sotftmax\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history_cnn = model.fit(x_train, y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=2,\n",
    "                        validation_split=0.2)\n",
    "# 4. 模型评价\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习3：基于GRU网络的IMDB评论分类\n",
    "\n",
    "```py\n",
    "from keras.layers import GRU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小练习 \n",
    "# 基于GRU网络的IMDB评论分类\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding, GRU\n",
    "\n",
    "######### 只考虑最常见的3000个词 ########\n",
    "num_words = 3000\n",
    "\n",
    "######### 导入数据 #########\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "# 对数据集切片，取一部分来实验\n",
    "train_data = train_data[:5000]\n",
    "train_labels = train_labels[:5000]\n",
    "test_data = test_data[:5000]\n",
    "test_labels = test_labels[:5000]\n",
    "\n",
    "# 1. 数据处理\n",
    "max_len = 50\n",
    "x_train = preprocessing.sequence.pad_sequences(train_data, maxlen=max_len)\n",
    "x_test = preprocessing.sequence.pad_sequences(test_data, maxlen=max_len)\n",
    "\n",
    "num_class = 2\n",
    "y_train = to_categorical(train_labels, num_class)\n",
    "y_test = to_categorical(test_labels, num_class)\n",
    "# 2. 构建模型\n",
    "max_features = 3000\n",
    "embedding_dims = 50\n",
    "filters = 16\n",
    "kernel_size = 3\n",
    "hidden_dims = 64\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))  #将每个词原来是整数，转换为 32位的向量\n",
    "model.add(GRU(32))                       #LSTM\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "# 3. 模型训练\n",
    "# sigmoid sotftmax\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history_cnn = model.fit(x_train, y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=2,\n",
    "                        validation_split=0.2)\n",
    "# 4. 模型评价\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习4：文本匹配与相似度计算\n",
    "\n",
    "* 训练数据\n",
    "```py\n",
    "train_documents = [\n",
    "    '南京江心洲污泥偷排或处置不当而造成的污染问题，不断被媒体曝光',\n",
    "    '面对美国金融危机冲击与国内经济增速下滑形势，中国政府在2008年11月初快速推出“4万亿”投资十项措施',\n",
    "    '全国大面积出现的雾霾，使解决我国环境质量恶化问题的紧迫性得到全社会的广泛关注',\n",
    "    '大约是1962年的夏天吧，潘文突然出现在我们居住的安宁巷中，她旁边走着40号王孃孃家的大儿子，一看就知道，他们是一对恋人。那时候，潘文梳着一条长长的独辫',\n",
    "    '坐落在美国科罗拉多州的小镇蒙特苏马有一座4200平方英尺(约合390平方米)的房子，该建筑外表上与普通民居毫无区别，但其内在构造却别有洞天',\n",
    "    '据英国《每日邮报》报道，美国威斯康辛州的非营利组织“占领麦迪逊建筑公司”(OMBuild)在华盛顿和俄勒冈州打造了99平方英尺(约9平方米)的迷你房屋',\n",
    "    '长沙市公安局官方微博@长沙警事发布消息称，3月14日上午10时15分许，长沙市开福区伍家岭沙湖桥菜市场内，两名摊贩因纠纷引发互殴，其中一人被对方砍死',\n",
    "    '乌克兰克里米亚就留在乌克兰还是加入俄罗斯举行全民公投，全部选票的统计结果表明，96.6%的选民赞成克里米亚加入俄罗斯，但未获得乌克兰和国际社会的普遍承认',\n",
    "    '京津冀的大气污染，造成了巨大的综合负面效应，显性的是空气污染、水质变差、交通拥堵、食品不安全等，隐性的是各种恶性疾病的患者增加，生存环境越来越差',\n",
    "    '1954年2月19日，苏联最高苏维埃主席团，在“兄弟的乌克兰与俄罗斯结盟300周年之际”通过决议，将俄罗斯联邦的克里米亚州，划归乌克兰加盟共和国',\n",
    "    '北京市昌平区一航空训练基地，演练人员身穿训练服，从机舱逃生门滑降到地面',\n",
    "    '腾讯入股京东的公告如期而至，与三周前的传闻吻合。毫无疑问，仅仅是传闻阶段的“联姻”，已经改变了京东赴美上市的舆论氛围',\n",
    "    '国防部网站消息，3月8日凌晨，马来西亚航空公司MH370航班起飞后与地面失去联系，西安卫星测控中心在第一时间启动应急机制，配合地面搜救人员开展对失联航班的搜索救援行动',\n",
    "    '新华社昆明3月2日电，记者从昆明市政府新闻办获悉，昆明“3·01”事件事发现场证据表明，这是一起由新疆分裂势力一手策划组织的严重暴力恐怖事件',\n",
    "    '在即将召开的全国“两会”上，中国政府将提出2014年GDP增长7.5%左右、CPI通胀率控制在3.5%的目标',\n",
    "    '中共中央总书记、国家主席、中央军委主席习近平看望出席全国政协十二届二次会议的委员并参加分组讨论时强调，团结稳定是福，分裂动乱是祸。全国各族人民都要珍惜民族大团结的政治局面，都要坚决反对一切危害各民族大团结的言行'\n",
    "]\n",
    "```\n",
    "* 测试数据\n",
    "```py\n",
    "test_document = '媒体曝光南京江心洲污泥偷排或处置不当而造成的污染问题'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim import corpora, similarities, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练样本\n",
    "train_documents = [\n",
    "    '南京江心洲污泥偷排或处置不当而造成的污染问题，不断被媒体曝光',\n",
    "    '面对美国金融危机冲击与国内经济增速下滑形势，中国政府在2008年11月初快速推出“4万亿”投资十项措施',\n",
    "    '全国大面积出现的雾霾，使解决我国环境质量恶化问题的紧迫性得到全社会的广泛关注',\n",
    "    '大约是1962年的夏天吧，潘文突然出现在我们居住的安宁巷中，她旁边走着40号王孃孃家的大儿子，一看就知道，他们是一对恋人。那时候，潘文梳着一条长长的独辫',\n",
    "    '坐落在美国科罗拉多州的小镇蒙特苏马有一座4200平方英尺(约合390平方米)的房子，该建筑外表上与普通民居毫无区别，但其内在构造却别有洞天',\n",
    "    '据英国《每日邮报》报道，美国威斯康辛州的非营利组织“占领麦迪逊建筑公司”(OMBuild)在华盛顿和俄勒冈州打造了99平方英尺(约9平方米)的迷你房屋',\n",
    "    '长沙市公安局官方微博@长沙警事发布消息称，3月14日上午10时15分许，长沙市开福区伍家岭沙湖桥菜市场内，两名摊贩因纠纷引发互殴，其中一人被对方砍死',\n",
    "    '乌克兰克里米亚就留在乌克兰还是加入俄罗斯举行全民公投，全部选票的统计结果表明，96.6%的选民赞成克里米亚加入俄罗斯，但未获得乌克兰和国际社会的普遍承认',\n",
    "    '京津冀的大气污染，造成了巨大的综合负面效应，显性的是空气污染、水质变差、交通拥堵、食品不安全等，隐性的是各种恶性疾病的患者增加，生存环境越来越差',\n",
    "    '1954年2月19日，苏联最高苏维埃主席团，在“兄弟的乌克兰与俄罗斯结盟300周年之际”通过决议，将俄罗斯联邦的克里米亚州，划归乌克兰加盟共和国',\n",
    "    '北京市昌平区一航空训练基地，演练人员身穿训练服，从机舱逃生门滑降到地面',\n",
    "    '腾讯入股京东的公告如期而至，与三周前的传闻吻合。毫无疑问，仅仅是传闻阶段的“联姻”，已经改变了京东赴美上市的舆论氛围',\n",
    "    '国防部网站消息，3月8日凌晨，马来西亚航空公司MH370航班起飞后与地面失去联系，西安卫星测控中心在第一时间启动应急机制，配合地面搜救人员开展对失联航班的搜索救援行动',\n",
    "    '新华社昆明3月2日电，记者从昆明市政府新闻办获悉，昆明“3·01”事件事发现场证据表明，这是一起由新疆分裂势力一手策划组织的严重暴力恐怖事件',\n",
    "    '在即将召开的全国“两会”上，中国政府将提出2014年GDP增长7.5%左右、CPI通胀率控制在3.5%的目标',\n",
    "    '中共中央总书记、国家主席、中央军委主席习近平看望出席全国政协十二届二次会议的委员并参加分组讨论时强调，团结稳定是福，分裂动乱是祸。全国各族人民都要珍惜民族大团结的政治局面，都要坚决反对一切危害各民族大团结的言行'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词处理，文档序列\n",
    "corpora_documents = []\n",
    "for item_text in train_documents:\n",
    "    item_seg = list(jieba.cut(item_text))\n",
    "    corpora_documents.append(item_seg)\n",
    "corpora_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成字典\n",
    "dictionary = corpora.Dictionary(corpora_documents)\n",
    "# dictionary.save('tmp/exercise.dict') #保存\n",
    "# dictionary=Dictionary.load('tmp/exercise.dict')#加载\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成向量语料\n",
    "# 通过下面一句得到语料中每一篇文档对应的稀疏向量（这里是bow向量）\n",
    "corpus = [dictionary.doc2bow(text) for text in corpora_documents]\n",
    "# 向量的每一个元素代表了一个word在这篇文档中出现的次数\n",
    "# corpora.MmCorpus.serialize('tmp/exercise.mm',corpus)#保存\n",
    "# corpus=corpora.MmCorpus('tmp/exercise.mm')#加载\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = '媒体曝光南京江心洲污泥偷排或处置不当而造成的污染问题'\n",
    "test_seq = list(jieba.cut(test_document))\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_bow = dictionary.doc2bow(test_seq)\n",
    "vec_lda = lda_model[vec_bow] \n",
    "vec_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lda_model[corpus])\n",
    "sims = index[vec_lda] \n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_model = models.TfidfModel(corpus=corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_bow = dictionary.doc2bow(test_seq)\n",
    "vec_tfidf = tfidf_model[vec_bow] \n",
    "vec_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(tfidf_model[corpus])\n",
    "sims = index[vec_tfidf] \n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = models.KeyedVectors.load_word2vec_format('tmp/sgns.wiki.bigram-char',binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vector = np.zeros(300)\n",
    "for word in test_seq:\n",
    "    if word in word2vec_model:\n",
    "        test_vector = np.add(test_vector, word2vec_model[word])\n",
    "test_vector = np.divide(test_vector, len(test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vectors = []\n",
    "for sentence in corpora_documents:\n",
    "    sentenceVector = np.zeros(300)\n",
    "    for word in sentence:\n",
    "        if word in word2vec_model:\n",
    "            sentenceVector = np.add(sentenceVector, word2vec_model[word])\n",
    "    sentenceVector = np.divide(sentenceVector, len(sentence))\n",
    "    train_vectors.append(sentenceVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vectors = train_vectors[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vectors.insert(0, test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity=cosine_similarity(all_vectors)\n",
    "similarity[0]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
