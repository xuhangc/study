{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>Sklearn特征选择</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：\n",
    "\n",
    "* 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。\n",
    "* 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除移除低方差法外，本文介绍的其他方法均从相关性考虑。\n",
    "\n",
    "\n",
    "根据特征选择的形式又可以将特征选择方法分为3种：\n",
    "\n",
    "* **Filter**：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\n",
    "\n",
    "- 1. 移除低方差的特征 (Removing features with low variance)\n",
    "- 2. 单变量特征选择 (Univariate feature selection)\n",
    "    - 2.1 卡方(Chi2)检验\n",
    "    - 2.2 Pearson相关系数 (Pearson Correlation)\n",
    "    - 2.3 互信息和最大信息系数 (Mutual information and maximal information coefficient (MIC)\n",
    "    - 2.4 距离相关系数 (Distance Correlation)\n",
    "    - 2.5 基于模型的特征排序 (Model based ranking)\n",
    "\n",
    "\n",
    "* **Wrapper**：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。\n",
    "\n",
    "* 3. 递归特征消除 (Recursive Feature Elimination)\n",
    "\n",
    "* **Embedded**：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。\n",
    "\n",
    "* 4. 使用SelectFromModel选择特征 (Feature selection using SelectFromModel)\n",
    "    * 4.1 基于L1的特征选择 (L1-based feature selection)\n",
    "    * 4.2 随机稀疏模型 (Randomized sparse models)\n",
    "    * 4.3 基于树的特征选择 (Tree-based feature selection)\n",
    "* 5. 将特征选择过程融入pipeline (Feature selection as part of a pipeline)\n",
    "\n",
    "特征选择主要有两个目的：\n",
    "\n",
    "* 减少特征数量、降维，使模型泛化能力更强，减少过拟合；\n",
    "* 增强对特征和特征值之间的理解。\n",
    "　　\n",
    "  \n",
    "  拿到数据集，一个特征选择方法，往往很难同时完成这两个目的。通常情况下，选择一种自己最熟悉或者最方便的特征选择方法（往往目的是降维，而忽略了对特征和数据理解的目的）。本文将结合 Scikit-learn提供的例子 介绍几种常用的特征选择方法，它们各自的优缺点和问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 sklearn.feature_selection 模块中的类可以用来对样本集进行 feature selection（特征选择）和 dimensionality reduction（降维），这将会提高估计器的准确度或者增强它们在高维数据集上的性能。\n",
    "\n",
    "**PCA可以用于降维，得到的新特征不是原始特征；本节的方法则是选择一些原始特征**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 移除低方差特征\n",
    "\n",
    "VarianceThreshold 是特征选择的一个简单基本方法，它会移除所有那些方差不满足一些阈值的特征。默认情况下，它将会移除所有的零方差特征，即那些在所有的样本上的取值均不变的特征。\n",
    "\n",
    "当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 2, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "print(X)\n",
    "sel = VarianceThreshold(threshold=0.2)  #保留方差大于threshold的特征    \n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一列数的平均数  0+0+1+0+0+0/6= 1/6\n",
    "# 平方差   \n",
    "((0-1/6)**2 *5+ (1-1/6)**2 )/ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VarianceThreshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如预期一样， VarianceThreshold 移除了第一列 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 单变量特征选择\n",
    "单变量的特征选择是通过基于单变量的统计测试来选择最好的特征。它可以当做是评估器的预处理步骤。Scikit-learn 将特征选择的内容作为实现了 transform 方法的对象：\n",
    "\n",
    "* SelectKBest 移除那些除了评分最高的 K 个特征之外的所有特征\n",
    "* SelectPercentile 移除除了用户指定的最高得分百分比之外的所有特征\n",
    "* 对每个特征应用常见的单变量统计测试: 假阳性率（false positive rate） SelectFpr, 伪发现率（false discovery rate） SelectFdr , 或者族系误差（family wise error） SelectFwe 。\n",
    "* GenericUnivariateSelect 允许使用可配置方法来进行单变量特征选择。它允许超参数搜索评估器来选择最好的单变量特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.feature_selection.SelectPercentile(score_func=<function f_classif>, percentile=10)\n",
    "    \n",
    "sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, k=10)\n",
    "\n",
    "其中的参数 score_func 有以下选项：\n",
    "\n",
    "回归：\n",
    "\n",
    "f_regression：相关系数，计算每个变量与目标变量的相关系数，然后计算出F值和P值\n",
    "\n",
    "mutual_info_regression：互信息，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。\n",
    "\n",
    "分类 :\n",
    "\n",
    "chi2：卡方检验；\n",
    "\n",
    "f_classif：方差分析，计算方差分析（ANOVA）的F值 (组间均方 / 组内均方)；\n",
    "\n",
    "mutual_info_classif：互信息，互信息方法可以捕捉任何一种统计依赖，但是作为非参数方法，需要更多的样本进行准确的估计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)#利用了标签监督信息\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用SelectKBest筛选特征后怎么知道选出来的是哪些特征 \n",
    "\n",
    "# 查看,影响最大的项,为True\n",
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用 SelectFromModel 选取特征\n",
    "\n",
    "SelectFromModel 是一个 meta-transformer（元转换器） ，它可以用来处理任何带有 coef_ 或者 feature_importances_ 属性的训练之后的评估器。 如果相关的coef_ 或者 featureimportances 属性值低于预先设置的阈值，这些特征将会被认为不重要并且移除掉。除了指定数值上的阈值之外，还可以通过给定字符串参数来使用内置的启发式方法找到一个合适的阈值。可以使用的启发式方法有 mean 、 median 以及使用浮点数乘以这些（例如，0.1*mean ）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. 基于 L1 的特征选取\n",
    "Linear models 使用 L1 正则化的线性模型会得到稀疏解：他们的许多系数为 0。 当目标是降低使用另一个分类器的数据集的维度， 它们可以与 feature_selection.SelectFromModel 一起使用来选择非零系数。特别的，可以用于此目的的稀疏评估器有用于回归的 linear_model.Lasso , 以及用于分类的 linear_model.LogisticRegression 和 svm.LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 SVM 和逻辑回归中，参数 C 是用来控制稀疏性的：小的 C 会导致少的特征被选择。使用 Lasso，alpha 的值越大，越少的特征会被选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. 基于 Tree 的特征选取\n",
    "基于树的 estimators （查阅 sklearn.tree 模块和树的森林 在 sklearn.ensemble 模块） 可以用来计算特征的重要性，然后可以消除不相关的特征（当与 sklearn.feature_selection.SelectFromModel 等元转换器一同使用时）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "clf.feature_importances_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离职率预测\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_dir = r'C:/Users/justin/pythoncode11/lesson 2/'  \n",
    "#这个请替换为自己lesson2-pandas & Matplotlib正确的路径\n",
    "\n",
    "# 设置读取文件的路径\n",
    "train_filename = os.path.join(current_dir, 'data/pfm_train.csv')\n",
    "print(train_filename)\n",
    "test_filename = os.path.join(current_dir, 'data/pfm_test.csv')\n",
    "print(test_filename)\n",
    "# 探索数据\n",
    "# 加载训练集和测试集\n",
    "train_data = pd.read_csv(train_filename)\n",
    "test_data = pd.read_csv(test_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将Attrition（该字段为标签）移至1列，方便索引\n",
    "Attrition = train_data['Attrition']\n",
    "train_data.drop(['Attrition'], axis = 1, inplace = True)\n",
    "train_data.insert(0, 'Attrition', Attrition)\n",
    "\n",
    "# 为了在train和test中的MonthlyIncome进行分组后的区间一致，需要保持两个数据集中MonthlyIncome的最大值和最小值一致，这里使用等宽分组\n",
    "# 使用pandas的cut进行分组，分为10组\n",
    "train_data['MonthlyIncome'] = pd.cut(train_data['MonthlyIncome'], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对train数据集进行one-hot编码\n",
    "train_encode = pd.get_dummies(train_data)\n",
    "train_encode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_encode.iloc[:, 1:]\n",
    "y = train_encode.iloc[:, 0]\n",
    "# 后面就是模型的选择、评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "#特征选择\n",
    "selector = SelectKBest(chi2, k=30)    \n",
    "X_new = selector.fit_transform(X, y) \n",
    "print(selector.get_support())\n",
    "\n",
    "X.head() \n",
    "\n",
    "X.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建模预测\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 划分训练集以及测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "train_score = lr.score(X_train, y_train)  # 0.8886363636363637\n",
    "print(train_score)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "print(np.mean(pred == y_test))  # 0.8863636363636364"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择补充阅读材料：https://www.cnblogs.com/stevenlk/p/6543628.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
