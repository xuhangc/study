{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证和模型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一节中，我们会探索 *模型评估* 和对 *超参数* 的调整，所谓超参数就是定义模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use seaborn for plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证模型\n",
    "\n",
    "机器学习最重要的一部分之一就是**模型验证**：也就是，看看你训练出来的模型与给定的数据集是否相配。但是这里有几点是需要注意的。\n",
    "\n",
    "我们回想一下我们之前看过的数字的例子，我们如何去评判一个模型的好坏呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们初始化一个K-neighbors分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们用这个分类器去预测数字对应的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以看一看训练模型的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0} / {1} correct\".format(np.sum(y == y_pred), len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果似乎表明我们有了一个完美的分类器！\n",
    "\n",
    "**问题：错误究竟在哪里？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证集\n",
    "\n",
    "上面我们的错误之处在于，我们在训练集上去测试我们的数据。**通常来说这不是一个好方法**。如果我们按照这种方式去优化estimator的时候，往往我们会得到**过拟合**的结果，也就是说，我们在训练时把数据的噪声也学进去了。\n",
    "\n",
    "测试模型的一个好方法就是用未曾训练过的数据作为测试集。我们在scikit-learn的测试/训练集合划分中已经掌握了这个思想了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们在训练集上进行训练，在测试集上进行验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"{0} / {1} correct\".format(np.sum(y_test == y_pred), len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它给了我们一个对于模型的好坏更可信的估计。\n",
    "\n",
    "我们这里使用的验证方法，也就是看正确个数之于总样本个数的方法，叫做**正确率**，而且我们可以使用以下的代码得到它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以通过``model.score``方法直接计算出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这个验证方法，我们可以看一看当我们改变模型参数的时候，正确率是如何变化的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [1, 5, 10, 20, 30]:\n",
    "    knn = KNeighborsClassifier(n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(n_neighbors, knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到在这个案例中，较小的邻居个数是较好的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证\n",
    "\n",
    "使用交叉验证集的一个问题就是实际上您\"损失\"了一部分数据。在上面的过程中，我们只用了数据集的3/4作为训练集，用了1/4作为验证集。还有一个选择就是使用**2-fold 交叉验证**，在这里我们把数据样本划分成一半对一半，然后做两次验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "X1.shape, X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KNeighborsClassifier(1).fit(X2, y2).score(X1, y1))\n",
    "print(KNeighborsClassifier(1).fit(X1, y1).score(X2, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，2-fold 交叉验证给了我们对模型参数的两次估计。\n",
    "\n",
    "因为这个过程完全手动去做可能有一些痛苦，所以scikit-learn提供了相应的包，简化了流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv = cross_val_score(KNeighborsClassifier(1), X, y, cv=10)\n",
    "cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold 交叉验证\n",
    "\n",
    "在上面我们使用的是2-fold交叉验证。这个只是$k$-fold交叉验证的一个特例，在$k$-fold交叉验证中，我们将数据集划分成$k$个部分，并且做$k$次拟合。每一个部分都会轮流成为验证集。我们可以改变上面的 ``cv`` 参数去实现它。让我们接下来做10-fold的交叉验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(KNeighborsClassifier(1), X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它让更清楚的让我们了解了自己模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过拟合，欠拟合和模型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们对验证以及交叉验证的基础知识都有了一定的了解，我们可以深入的探索模型选择的问题了。\n",
    "\n",
    "验证和交叉验证是机器学习实践中的最重要的方面之一。如何对模型进行选择也是很重要的，但是这一点恰恰被一些进入机器学习领域的新手所忽略。\n",
    "\n",
    "下面一些问题十分的重要：\n",
    "\n",
    "**如果我们的estimator不能很好的工作，我们接下来应该怎么办呢？**\n",
    "\n",
    "- 使用更简单的或者更复杂的模型？\n",
    "- 给模型添加更多的特征？\n",
    "- 添加更多的训练数据？\n",
    "\n",
    "这个答案经常是违反直觉的。特别的，**有时候一个复杂的模型会给出一个 _差_ 的结果**。同时，**有时候添加训练数据的个数并不会提升你的结果**。区分成功的机器学习领域的练习者与不成功的，就是选择方法去提高模型效果的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于偏置和方差取舍的说明\n",
    "\n",
    "在这一个小节中，我们将讨论一个简单的一维的回归问题。这个例子会让我们清楚直观地看到我们的数据和模型，结果可以简单的扩展到高维数据。我们来看一看 **线性回归** 的问题。我们可以用scikit-learn的`sklearn.linear_model`模块来完成这个功能。\n",
    "\n",
    "我们创造一个简单的非线性函数供我们拟合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(x, err=0.5):\n",
    "    y = 10 - 1. / (x + 0.1)\n",
    "    if err > 0:\n",
    "        y = np.random.normal(y, err)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "让我们把数据集画出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(N=40, error=1.0, random_seed=1):\n",
    "    # randomly sample the data\n",
    "    np.random.seed(1)\n",
    "    X = np.random.random(N)[:, np.newaxis]\n",
    "    y = test_func(X.ravel(), error)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_data(40, error=1)\n",
    "plt.scatter(X.ravel(), y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们想在这个数据集中做回归预测。我们用集成的线性回归方法去处理一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用一条直线去拟合了这一组数据，但是很明显这个模型并不是一个好的选择。这个模型对数据的拟合有**偏差**，也就是这个模型是**欠拟合**的。\n",
    "\n",
    "让我去用更复杂的模型去提升效果。我们可以增加模型的自由度，用一个多项式级别的回归去预测我们的模型。Scikit-learn通过``PolynomialFeatures``处理器让预测变的特别简单。\n",
    "\n",
    "我们用一个简单的方法去做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用一个二次曲线去拟合这个数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在的这个拟合减少了均方差，而且比之前的拟合更优。如果我们选取更高维的多项式会有什么结果呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolynomialRegression(30)\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)))\n",
    "plt.ylim(-4, 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们将多项式的维度提高到这个程度的时候，很明显这个结果已经不再能良好的体现出数据的分布了，它对数据中的噪声变的很敏感。因此，我们叫它高方差模型，也就是说这个模型过拟合了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过验证曲线检测过拟合\n",
    "\n",
    "在训练集上计算出错误显然是不够的。我们可以使用**交叉验证**的方式更好的去看一看这个模型的优劣。\n",
    "\n",
    "我们在这里也使用``validation_curve``这个工具。为了弄明白，我们用一个稍微大一点的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_data(120, error=1.0)\n",
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "def rms_error(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "\n",
    "degree = np.arange(0, 18)\n",
    "val_train, val_test = validation_curve(PolynomialRegression(), X, y,\n",
    "                                       'polynomialfeatures__degree', degree, cv=7,\n",
    "                                       scoring=rms_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在画出验证曲线："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_err(x, data, **kwargs):\n",
    "    mu, std = data.mean(1), data.std(1)\n",
    "    lines = plt.plot(x, mu, '-', **kwargs)\n",
    "    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n",
    "                     facecolor=lines[0].get_color(), alpha=0.2)\n",
    "\n",
    "plot_with_err(degree, val_train, label='training scores')\n",
    "plot_with_err(degree, val_test, label='validation scores')\n",
    "plt.xlabel('degree'); plt.ylabel('rms error')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意曲线的趋势，这对于这种类型的图来说很正常。\n",
    "\n",
    "1. 对于低复杂度的模型而言，训练集上的错误率和验证集上的错误率非常的类似。这说明现在模型是**欠拟合**：它目前的复杂度不具备完好表达数据分布的能力。换个角度来说，这个模型称为**高偏置**的模型.\n",
    "\n",
    "2. 当模型的复杂度增加的时候，训练集和验证集上的错误率开始分离开来。这表明这个模型已经**过拟合**了，它过于复杂以至于它对数据中的噪声过于敏感了。换个角度来说，这个模型称为**高方差**模型。\n",
    "\n",
    "3. 我们需要注意到，训练的准确率几乎总是随着模型复杂度的提高而增加。这是因为越复杂的模型越能够学习噪声，所以训练的准确率会上升。验证数据通常有一个最佳位置，一般来说在度为5的时候最好。\n",
    "\n",
    "在这里我们根据交叉验证，给出最佳拟合数据的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolynomialRegression(5).fit(X, y)\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X_test, model.predict(X_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 总结\n",
    "\n",
    "我们针对模型验证以及介绍了以下几种有用的方法\n",
    "\n",
    "- **训练得分** 显示了模型在训练集上的训练效果，但这并不是模型适用性的评判标准。\n",
    "- **验证得分** 显示了模型对于非训练集的数据的拟合效果。最有效的方法是，用非训练集以外的几个数据集进行交叉验证。\n",
    "\n",
    "- **验证曲线** 是以模型复杂度为参数的训练集得分和验证集得分的曲线：\n",
    "     + 当两个曲线相近的时候，代表*欠拟合* \n",
    "     + 当两个曲线逐渐分开的时候，代表*过拟合* \n",
    "     + \"最佳位置\"在曲线的中间 \n",
    "\n",
    "    \n",
    "这些方法在您评估一个模型的时候会非常有用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
