{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn数据集及数据变换\n",
    "\n",
    "在本节中会介绍Scikit-learn的基本原理，它是一个集成了很多机器学习工具并被广泛使用的包，用Python实现。详情请参考http://scikit-learn.org 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "**主要目标**：介绍Scikit-learn数据表示与特征处理\n",
    "\n",
    "* Scikit-learn中的数据表示和数据集\n",
    "* Scikit-learn中特征变化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn中的数据表示\n",
    "\n",
    "机器学习是从数据中建立模型的，我们将会从怎样让用电脑理解的方式去表示数据开始。同时，我们会用matplotlib的例子讲解如何将数据用图表的形式显示出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Scikit-learn中，大多数的机器学习算法的数据在二维的数组或者矩阵中存储。这些数据可能是``numpy``数组，在某些情况下也可能是``scipy.sparse``矩阵。数组的大小应该是`[样本数，特征数]` (【译者注】sample - 样本，feature - 特征)\n",
    "\n",
    "- **样本数（n_sample）:** 样本的数目。每一个样本都是一个需要处理的独立个体（例如：需要被分类），一个样本可能是一个文档、一幅图片、一段音频、一段视频、一个天文学数据、数据库或者CSV文件中的一行，或者任意一个确定的数值的集合。\n",
    "- **特征数（n_feature）:** 特征的数目，特征是描述一个样本的数值表达。特征一般是实数，不过在某些情况下也会是布尔值或者是离散数据。\n",
    "\n",
    "特征数必须提前确定。但是对于给定的样本，特征可以是很大（百万级）的一个零占大多数的集合。这种情况下，`scipy.sparse`矩阵就派上了用场，用这个矩阵比numpy矩阵在存储上会更加高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Layout](images/data-layout.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个简单的例子：Iris 数据集\n",
    "\n",
    "作为简单数据集的例子，我们将会介绍scikit-learn中存储的iris数据集。数据由3种不同品种的鸢尾花组成。下面是数据集中的3个品种，我们可以通过下面的代码显示出它们："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "display(Image(filename='images/iris_setosa.jpg'))\n",
    "print(\"Iris Setosa\\n\")\n",
    "\n",
    "display(Image(filename='images/iris_versicolor.jpg'))\n",
    "print(\"Iris Versicolor\\n\")\n",
    "\n",
    "display(Image(filename='images/iris_virginica.jpg'))\n",
    "print(\"Iris Virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题：\n",
    "\n",
    "**如果我们想设计一个算法去分辨iris的品种，数据可能是什么？**\n",
    "\n",
    "记住：我们需要一个2D的数组，其大小为`[样本数 * 特征数]`\n",
    "\n",
    "- `样本数`指的是什么？\n",
    "\n",
    "- `特征数`指的是什么？\n",
    "\n",
    "记住每一个样本的特征数必须是**固定**的，而且对于每一个样本，特征数``i``必须是一个数值型的元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用scikit-learn 加载 Iris 数据\n",
    "\n",
    "Scikit-learn对于Iris数据有一个非常直接表示。数据表示如下：\n",
    "\n",
    "- Iris 数据集的特征：\n",
    "\n",
    "    1. 萼片长度(cm)\n",
    "    2. 萼片宽度(cm)\n",
    "    3. 花瓣长度(cm)\n",
    "    4. 花瓣宽度(cm)\n",
    "    \n",
    "    \n",
    "- 预测的目标类别\n",
    "    \n",
    "    1. Iris Setosa\n",
    "    2. Iris Versicolour\n",
    "    3. Iris Virginica\n",
    "    \n",
    "``scikit-learn`` 准备好了一个iris CSV文件的拷贝和一个加载函数去从numpy数组中加载它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = iris.data.shape\n",
    "print((n_samples, n_features))\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names)\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数据是四维的，但是我们可以使用简单的scatter-plot一次显示出两维的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_index = 1\n",
    "y_index = 2\n",
    "\n",
    "# 这段代码使用iris的名字来标注颜色条(colorbar)\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index],\n",
    "            c=iris.target, cmap=plt.cm.get_cmap('RdYlBu', 3))\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.clim(-1, 2.5)\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 快速练习：\n",
    " \n",
    "**在上面的脚本中改变**  **x_index** 和 **y_index**， 找到一种可以最大化分隔出三个类别的它们的组合。**\n",
    "\n",
    "这个练习是**降维算法**的一个预告，我们在之后会看到。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他数据\n",
    "\n",
    "它们分为如下三种：\n",
    "\n",
    "- **包内置数据：** 这些小的数据集已经被集成在scikit-learn的安装包里面了，可以用``sklearn.datasets.load_*``去下载它\n",
    "- **供下载数据：** 这些较大的数据可以供用户们下载，scikit-learn里面已经包含了下载这些数据集的流通道。这些数据可以在``sklearn.datasets.fetch_*``中找到。\n",
    "- **生成数据：** 通过随机种子，可以通过现有模型随机生成一些数据集。它们可以在``sklearn.datasets.make_*``中找到\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets.l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.fetch_20newsgroups()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据特征表示与变换 transformer\n",
    "许多无监督学习的实例，例如降维，流形学习和特征提取，在没有任何额外输入的情况下找到输入数据的新表示。 （与监督学习相反，无监督算法不需要或考虑目标变量）。\n",
    "\n",
    "一个非常基本的例子是我们的数据重缩放，这是许多机器学习算法的要求，因为它们不是规模不变的 - 重缩放属于数据预处理类别，几乎不能称为学习。 存在许多不同的重缩放技术，在下面的示例中，我们将看一个通常称为“标准化”的特定方法。 在这里，我们将重缩放数据，使每个特征以零（均值为 0）为中心，具有单位方差（标准差为 1）。\n",
    "\n",
    "例如，如果我们的一维数据集的值为`[1,2,3,4,5]`，则标准化值为：\n",
    "\n",
    "+   1 -> -1.41\n",
    "+   2 -> -0.71\n",
    "+   3 -> 0.0\n",
    "+   4 -> 0.71\n",
    "+   5 -> 1.41\n",
    "\n",
    "通过等式`z = (x - μ) / σ`计算，其中`μ`是样本均值，`σ`是标准差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary = np.array([1, 2, 3, 4, 5])\n",
    "ary_standardized = (ary - ary.mean()) / ary.std()\n",
    "ary_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "import pandas as pd \n",
    "data = [[100.5, 5.3], [80, 3], [90, 2], [78, 1]]\n",
    "df=pd.DataFrame(data)\n",
    "print(df)\n",
    "scaler11 = preprocessing.StandardScaler()  #sklearn 实现标准化 的 transformer 对象\n",
    "scaler = scaler11.fit(df) #基于mean和std的标准化\n",
    "\n",
    "newo=scaler.transform(df)\n",
    "newo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "import pandas as pd \n",
    "data = [[100.5, 5.3], [80, 3], [90, 2], [78, 1]]\n",
    "df=pd.DataFrame(data)\n",
    "print(df)\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,5)).fit(df)  #. 将每个特征值归一化到一个固定范围\n",
    "\n",
    "scaler.transform(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "data = [['red', 'good', 'level3'], \n",
    "        ['white', 'very good', 'level0'], \n",
    "         ['yellow', 'very very good', 'level1'], \n",
    "         ['black', 'good', 'level2']]\n",
    "print(np.array(data))\n",
    "encoder = preprocessing.OneHotEncoder().fit(data)  #one-hot编码\n",
    "newdata=encoder.transform(data).toarray()\n",
    "print(newdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot编码\n",
    "\n",
    "优点：独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有0和1，不同的类型存储在垂直的空间。\n",
    "\n",
    "缺点：当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.OneHotEncoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下：通过fit方法，估计器拟合我们提供的数据。 在该步骤中，估计器根据数据估计参数（这里是平均值和标准差）。 然后，如果我们转换数据，这些参数将用于转换数据集。 （请注意，transform方法不会更新这些参数）。\n",
    "\n",
    "重要的是要注意，**相同的转换应用于训练和测试集**。 这导致通常在缩放后测试数据的平均值不为零（因为标注化的transformer是基于训练数据构造的）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下一节，我们将会使用一些数据集来研究机器学习的基本规则。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
