{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习的基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们将介绍机器学习的基本原理，以及怎么通过 Scikit-Learn 的API去实现这些算法。\n",
    "\n",
    "在简单的介绍scikit-learn的*Estimator*对象后，我们将介绍**监督学习**，包括*分类*问题和*回归*问题。我们还将介绍**无监督学习**，包括*降维*和*聚类*问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn 中的 Estimator 对象\n",
    "\n",
    "每一个在scikit-learn中实现的算法都是表示为一个''Estimator''的对象。比如，一个线性回归的算法如下这样实现的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression?\n",
    "# 超参数  hypoparameter： 模型初始化时设置的参数\n",
    "# 模型的参数： 模型训练完，模型里面的一些系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator 构造参数（超参数、hyperparameter）**：一个 Estimator 对象的所有系数可以在它初始化的时候设置，这些系数拥有普适性的初始值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(normalize=True) #超参数 人指定的 normallize\n",
    "print(model.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator 模型参数（parameter）**：当Estimator用数据来拟合系数时，模型的系数是根据数据一步步得到的。所有的模型系数都是Estimator对象的属性，并且以一个下划线(underscore)结尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10) # 按照一个多项式函数人为生成数据， 然后通过机器学习模型训练，看是否可以根据数据来反推出这个模型的参数\n",
    "y = 5* x + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, '^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 的输入数据要求是2维的: (samples == 10 x features == 1)\n",
    "X = x[:, np.newaxis]  # 原来是1维，转换为2维，但是没有增加数据\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用数据来拟合系数(fit the model)\n",
    "model.fit(X, y)  #模型训练\n",
    "\n",
    "#model这个变量就代表了我们模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下划线结尾，代表了一个拟合的系数 ， 这个系数就是模型的参数，是训练出来的\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "print(\"y=%s *x + %s\" %(model.coef_[0],model.intercept_))  #经过训练得到的多项式的 线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[5]])  #predict传的参数 也是一个 二维矩阵,  fit  predict传进去的是相同维度的特征矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个模型按照我们所预期的，找出了一条斜率为5，截距为0.1的线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 监督学习：分类问题的例子\n",
    "K个最近邻(kNN)算法是最简单的机器学习算法之一：**给出新的、未知的观测结果，在参考数据库中找出与这些特征最接近的分类，然后把这个对象归入最主要的分类中**。\n",
    "\n",
    "我们在iris分类问题上试一试我们这个算法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 建立模型\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# 训练模型 (fit the model)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# 一个 3cm x 5cm 萼片 and 4cm x 2cm 花瓣 的 iris 花，属于哪一个分类？\n",
    "# 调用\"predict\"方法:\n",
    "result = knn.predict([[1, 8, 4, 2],])\n",
    "\n",
    "print(iris.target_names[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您还可以去做分类概率预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba([[3, 5, 4, 2],]) #这种能告诉概率的性质，不是每个模型都有\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fig_code import plot_iris_knn\n",
    "plot_iris_knn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 练习\n",
    "\n",
    "对上述的问题，用一个不同的Esitimator: ``sklearn.svm.SVC``.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(gamma='scale')\n",
    "\n",
    "大家尝试在下面的代码框，模仿上面的KNN的代码 写出来 SVM的代码\n",
    "\n",
    "*请注意你并不需要知道Estimator内部的实现细节，我们在这里只是试着使用这些接口*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#小练习    在下面的代码框，模仿上面的KNN的代码 写出来 SVM的代码,到预测那一步就行\n",
    "# svm没有 predict_proba，这是knn特有的； plot_iris_knn()这个也是为knn专门开发的\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 监督学习：回归算法的例子\n",
    "\n",
    "回归算法的最简单的例子之一就是和我们之前看到的一样，根据给出的点去拟合一条线。\n",
    "Scikit-learn 也包含很多更复杂的回归算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一些简单的数据\n",
    "import numpy as np\n",
    "from numpy import random as nr\n",
    "\n",
    "np.random.seed(11)  \n",
    "X = nr.random(size=(20, 1))\n",
    "print(X)\n",
    "print(X.squeeze())  #和我们之前 X = x[:, np.newaxis]相反\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(20)\n",
    "print(y)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像上面说的一样，我们还画出一条最佳的拟合曲线："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #先还用线性回归模型看看是否可行\n",
    "model.fit(X, y)\n",
    "\n",
    "# 画出数据和模型预测之间的图形\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')    #训练数据实际值\n",
    "plt.plot(X_fit.squeeze(), y_fit); #模型预测值\n",
    "\n",
    "model.score(X, y) #用训练好的模型来 进行评价预测准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn 还有一些更加复杂的算法，可以根据模型中的特征进行更好的拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个随机森林(Random Forest)模型\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100,max_depth=50)\n",
    "print(model)\n",
    "model.fit(X, y)\n",
    "\n",
    "# 画出数据和模型预测之间的图形\n",
    "X_fit = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_fit = model.predict(X_fit)\n",
    "\n",
    "plt.plot(X.squeeze(), y, 'o')\n",
    "plt.plot(X_fit.squeeze(), y_fit);\n",
    "\n",
    "model.score(X, y) #用训练好的模型来 进行评价预测准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些是不是一个好的拟合取决于很多因素；我们之后在教程中会去讨论如何去选择一个模型的细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 练习\n",
    "\n",
    "通过Ipython的帮助，探索``RandomForestRegressor``对象（比如在对象加一个问号）。\n",
    "``RandomForestRegressor``的参数有哪些？\n",
    "如果您改变这些参数的话，对上面的图片会有哪些影响？\n",
    "\n",
    "这些类参数就是我们所说的 *hyperparameters*， 我们之后会在验证一节中讨论如何选择这些 hyperparameters。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无监督学习：降维(Dimensionality Reduction)和聚类(Clustering)\n",
    "\n",
    "**无监督学习**从问题的另一个角度去考虑。数据在这里是没有标签的，我们感兴趣的是问题中对象的相似点。从某种程度上说，您可以认为无监督学习是一种从数据本身去发掘标签的方法。无监督学习有*降维*(dimensionality reduction)\n",
    "、聚类(clustering)和密度估计(density estimation)等算法。举个例子，在我们之前所说的iris数据中，我们可以利用无监督学习去发现哪一种测量的组合最能体现数据的结构。我们接下来会看到，这样一种数据的投影能够被用来在二维空间中展现四维的数据集。还有一些更贴切的无监督学习的问题：\n",
    "\n",
    "- 给出遥远星系的详细的观测数据，决定哪一个或哪几个特征是最能体现其数据的\n",
    "\n",
    "- 给出两种声源的混合（比如一个人在音乐声中谈话），分离出两种声音（这叫做 [盲源分离](http://en.wikipedia.org/wiki/Blind_signal_separation) ）\n",
    "\n",
    "- 给出一段视频，分离出其中运动的物体，并且与其他运动物体比较，给出分类\n",
    "\n",
    "有些时候监督学习和无监督学习会组合在一起：无监督学习可以用来从有多个类别的数据中找到有用的特征，而获取的这些特征可以被用在一个监督学习的框架中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维：PCA\n",
    "\n",
    "主成分分析（Principle Component Analysis, PCA）是一个降维算法，它可以找到能达到最大方差的最佳变量的组合。\n",
    "\n",
    "考虑iris数据集。因为它有4种特征，所以并不可以在一个简单的2D图中显示出来。我们即将从中抓取出萼片和花瓣特征的组合去显示它们："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "'''\n",
    "n_components：这个参数可以帮我们指定希望PCA降维后的特征维度数目。最常用的做法是直接指定降维到的维度数目，\n",
    "此时n_components是一个大于等于1的整数。当然，我们也可以指定主成分的方差和所占的最小比例阈值，\n",
    "让PCA类自己去根据样本特征方差来决定降维到的维度数，此时n_components是一个（0，1]之间的数。\n",
    "当然，我们还可以将参数设置为\"mle\", 此时PCA类会用MLE算法根据特征的方差分布情况自己去选择一定数量的主成分特征来降维。\n",
    "我们也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)。\n",
    "'''\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X) # 对 鸢尾花数据集进行PCA转换\n",
    "print(\"Reduced dataset shape:\", X_reduced.shape) #最后特征数由4 消减为 2个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "            c=y, cmap='RdYlBu')\n",
    "\n",
    "print(\"降维为2维，这里是有明确含义的，是原来多个特征运算得来\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 聚类：K-means\n",
    "\n",
    "聚类算法针对观测到的数据，根据给定的准则发现它们的共同点，在数据集中寻找“群”。\n",
    "\n",
    "注意，这些聚类(cluster)会从数据中发现一些隐藏的相关结构如果所使用到的标准展现出了这些结构\n",
    "(Note that these clusters will uncover relevent hidden structure of the data only if the criterion used highlights it.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_means = KMeans(n_clusters=3, random_state=0) # Fixing the RNG in kmeans\n",
    "k_means.fit(X)\n",
    "y_pred = k_means.predict(X)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_pred,\n",
    "           cmap='RdYlBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结: Scikit-learn 的 Estimator 接口\n",
    "\n",
    "Scikit-learn 致力于为所有的机器学习方法创造一个统一的接口，我们将会在下面的看到这些例子。对于一个叫`model`的scikit-learn的Estimator对象，我们有下面这些方法：\n",
    "\n",
    "- 适用**所有Estimators**：\n",
    "    + `model.fit()` : 用训练数据集来训练算法。 对监督学习，这个方法接受两个参数: 特征向量 `X`和对应的标签 `y` (例如: `model.fit(X, y)`)。 对于无监督学习，这个方法只接受一个参数，即特征向量 `X` (例如: `model.fit(X)`)。\n",
    "- 适用 **监督学习(supervised)**的 方法\n",
    "  + `model.predict()` : 给定一个训练模型，预测一个新的数据集的返回值。 这个方法接受一个参数，即新的数据集 `X_new` (例如: `model.predict(X_new)`), 方法返回特征向量中每一组值预测返回的结果。\n",
    "  + `model.predict_proba()` : 对于分类(classification)问题，一些模型提供了这个方法，用于返回给定一个新的数值可能返回不同分类的概率。 在这种情况下，`model.predict()` 返回概率最高的分类值。\n",
    "  + `model.score()` : 对分类(classification)或者回归(regression)问题，绝大多数的模型都实现了这个得分(score) 方法。 得分数值介于 0 和 1 之间，数值越大说明模型训练的越好。\n",
    "- 适用**无监督Estimators**\n",
    "    + `model.predict()`: 在聚类算法中，预测数据的标签。\n",
    "  + `model.transform()` : 给定一个无监督模型，将一组新数据变换(transform)为新的基准(basis)。这个方法接受一个参数 `X_new`，将这组新数据根据无监督模型进行数据变换并返回。\n",
    "    + `model.fit_transform()`: 有一些estimators实现这个方法，它更有效的训练数据并根据模型进行数据转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证\n",
    "\n",
    "机器学习的一个重要部分就是**模型验证**: 就是得出你的训练模型的优劣，即从训练数据中去预测没有被标签的数据的性能和准确性。我们看一个采用*最近邻分类器*的例子。这是一个非常简单的分类器：它简单的存储了所有的训练数据，对于任何未知的数据的标签，仅仅采用与之最近的一个数据标签。\n",
    "\n",
    "对于iris数据，它非常简便的返回了正确的预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print(np.all(y == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个查看数据更有用的方法是查看**混淆矩阵**，或者是一个可以显示输入和输出频率的矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50个训练的数据都被正确的分类了。这是训练数据测试数据是一个数据，但是这**并不代表我们的模型就是完美的！**特别的，这个模型应用于新的数据时会效果会很糟糕。我们可以把数据集划分成*训练集*(training set)和*测试集*(testing set)，并再次进行训练和测试。 Scikit-learn 拥有方便的划分流程:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import train_test_split\n",
    "# 从 v0.18 起，使用 sklearn.model_selection 中 train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)#默认25%测试集\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)\n",
    "print(confusion_matrix(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面真是的反映了我们的分类器的性能：显然对于第二类和第三类出现了一点混淆，我们也许可以通过上面的数据看出这一点。\n",
    "\n",
    "这就是为什么将模型划分为训练集和测试集**非常重要**。我们将会在后续的教程中深入讨论模型的验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速案例：光学字符识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了在一个更有趣的问题中实践我们的这些准则，让我们考虑光学字符识别问题——也就是，识别手写数字。\n",
    "从更广的层面来说，这个问题包含了数字定位技术和识别数字的技术。在这里我们会走一个捷径，scikit-learn的库中已经包含了已经预处理好的数字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载并显示数字\n",
    "\n",
    "我们用scikit-learn的数据获取接口去看一下数字数据规模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "digits.images.shape  #图像形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data.shape #数据展开为64个特征，每个特征表示一个像素点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们打印出其中的一部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这些图像的数字\n",
    "print(digits.images.shape)\n",
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以我们有了1797个样本，其中每个样本有64个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对应的标签(label)\n",
    "print(digits.target)  #target y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data[0]   # data  X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 无监督学习：降维\n",
    "\n",
    "我们想在64维空间中去查看我们的数据，但是在64维空间中画图是一件无法想象的事情！\n",
    "我们可以使用无监督学习的方法将维度减少到2。在这里我们采用一种功能强大的机器学习算法 *Isomap (PCA类似功能)* ， 将这些数据转换成两维，就可以通过图像查看数据分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = Isomap(n_components=2)#另外一种降维的方法\n",
    "data_projected = iso.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_projected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,\n",
    "            edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10));\n",
    "plt.colorbar(label='digit label', ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在这里可以看到，这些数字在参数域被非常好的分隔开了。这说明了我们的监督学习分类算法也应当取得非常好的结果。让我们试试看。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数字分类\n",
    "\n",
    "让我们在这些数字上尝试我们的分类工作。我们首先需要做的是将数字分离成训练集和测试集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 sklearn.cross_validation 中引入 train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target, \n",
    "                                                test_size=0.25)# 25%做测试集\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用一个简单的逻辑回归算法（logistic regression，尽管名称中含有'回归'，但它是一个分类算法）去完成分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty='l2',solver='lbfgs',multi_class='auto',max_iter=30000)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过比较预测结果和真实结果来得到我们的分类准确率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, ypred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单纯的数字没有告诉我们哪里出错了，一个比较推荐的方法是使用混淆矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(confusion_matrix(ytest, ypred)),\n",
    "           cmap='Blues', interpolation='nearest')\n",
    "plt.grid(False)\n",
    "plt.ylabel('true')\n",
    "plt.xlabel('predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以查看我们的输出和其对应的预测的数字。我们会将预测错误的标签置红："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(Xtest[i].reshape(8, 8), cmap='binary')\n",
    "    ax.text(0.05, 0.05, str(ypred[i]),\n",
    "            transform=ax.transAxes,\n",
    "            color='blue' if (ytest[i] == ypred[i]) else 'red')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个有趣的现象是，即使我们采用的是最简单的逻辑回归算法，很多它预测错误的图片我们自己也很有可能预测错误！\n",
    "\n",
    "我们有很多方法去提高这个分类器，但是我们现在没有时间了。我们可以通过使用更成熟的模型，运用交叉验证或者采用其他技术去提高这个分类器。\n",
    "我们会在之后的教程中详细的说明这些。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
