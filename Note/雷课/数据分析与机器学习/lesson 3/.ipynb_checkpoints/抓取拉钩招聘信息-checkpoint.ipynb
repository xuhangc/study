{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pymongo\n",
    "# client = pymongo.MongoClient(host='localhost',port=27017)\n",
    "# db = client.test\n",
    "# collection = db.books\n",
    "\n",
    "# import selenium\n",
    "# from selenium import webdriver\n",
    "# import time\n",
    "# options = webdriver.FirefoxOptions()\n",
    "# options.headless = True\n",
    "# browser = webdriver.Firefox(options=options)\n",
    "# page = 5\n",
    "# keyword = '爬虫'\n",
    "\n",
    "# while True:\n",
    "#     url = 'https://book.douban.com/subject_search?search_text='+ keyword + '&cat=1001&start=' + str(page*15)\n",
    "#     browser.get(url)\n",
    "#     print(url)\n",
    "#     detail_elements = browser.find_elements_by_class_name('detail')\n",
    "#     if detail_elements != []:\n",
    "#         page += 1\n",
    "#     else:\n",
    "#         print(\"【没有更多的数据了，抓取结束】\")\n",
    "#         print(\"已抓取数据：\", collection.count_documents({}), \"条\")\n",
    "#         break\n",
    "#     for detail_element in detail_elements:\n",
    "#         title = detail_element.find_element_by_class_name('title-text').text\n",
    "#         rate = detail_element.find_elements_by_class_name('rating_nums')\n",
    "#         if rate == []:\n",
    "#             rate = '暂无评分'\n",
    "#         else:\n",
    "#             rate = rate[0].text\n",
    "#         profile = detail_element.find_element_by_xpath(\"//div[@class='meta abstract']\").text\n",
    "# #         print(title, rate, profile)\n",
    "#         result = collection.insert_one({'title':title, 'rate':rate, 'profile':profile})\n",
    "# #         print(result.inserted_id)\n",
    "\n",
    "# browser.quit()\n",
    "\n",
    "# for doc in collection.find({}):\n",
    "#     print(doc)\n",
    "# collection.delete_many({})\n",
    "# if collection.count_documents({}) == 0:\n",
    "#     print(\"【好了，数据库已经被删个精光了】\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185686\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# options = webdriver.FirefoxOptions()\n",
    "# options.headless = True\n",
    "# browser = webdriver.Firefox(options=options)\n",
    "browser = webdriver.Firefox()\n",
    "\n",
    "url = 'https://www.lagou.com/jobs/list_%E7%88%AC%E8%99%AB?labelWords='\n",
    "# data = browser.page_source\n",
    "\n",
    "#【测试Cookies是否能规避反爬验证】\n",
    "# browser.get(url)\n",
    "# print(\"【第一波：未验证的cookies】\", browser.get_cookies())\n",
    "# time.sleep(15)\n",
    "# b = browser.get_cookies()\n",
    "# print(\"【第二波：通过验证后的cookies】\", b)\n",
    "# browser.delete_all_cookies()\n",
    "# print(\"【第三波：删除后的空白cookies】\", browser.get_cookies())\n",
    "# for i in b:\n",
    "#     browser.add_cookie(i)\n",
    "# print(\"【第四波：伪造验证cookies】\", browser.get_cookies())  # 如果不伪造cookies，则下一步依然需要验证，以此证明cookies具有规避验证码的能力\n",
    "# browser.maximize_window()\n",
    "# browser.get(url)\n",
    "# print(\"【第五波：二次请求后的cookies】\", browser.get_cookies())\n",
    "\n",
    "\n",
    "#【Cookies文件读写测试】←从这里开始\n",
    "def save_cookies(cookies):\n",
    "    fw = open(\"test.txt\",'w+')\n",
    "    fw.write(str(cookies))\n",
    "    fw.close()\n",
    "\n",
    "def load_cookies():\n",
    "    fr = open(\"test.txt\",'r+')\n",
    "    cookies = eval(fr.read())\n",
    "    fr.close()\n",
    "    return cookies\n",
    "\n",
    "# browser.get(url)\n",
    "# print(\"【第一波：未验证的cookies】\", browser.get_cookies())\n",
    "# time.sleep(15)\n",
    "# b = browser.get_cookies()\n",
    "# save_cookies(b)  # 保存通过验证的cookies\n",
    "# print(\"【第二波：通过验证后的cookies】\", b)\n",
    "# browser.delete_all_cookies()\n",
    "# print(\"【第三波：删除后的空白cookies】\", browser.get_cookies())\n",
    "# c = load_cookies()\n",
    "# for i in c:\n",
    "#     browser.add_cookie(i)\n",
    "# print(\"【第四波：伪造验证cookies】\", browser.get_cookies())  # 如果不伪造cookies，则下一步依然需要验证，以此证明cookies具有规避验证码的能力\n",
    "# browser.maximize_window()\n",
    "# browser.get(url)\n",
    "# print(\"【第五波：二次请求后的cookies】\", browser.get_cookies())\n",
    "\n",
    "\n",
    "#【读取本地安全Cookies直接绕开反爬】\n",
    "browser.get(\"https://www.lagou.com\")\n",
    "# print(\"【初始Cookies】\", browser.get_cookies())\n",
    "browser.delete_all_cookies()  # 先访问lagou.com，然后抢在js重定向之前把Cookies给覆盖掉\n",
    "c = load_cookies()\n",
    "# print(\"【本地Cookies】\", c)\n",
    "for i in c:\n",
    "    browser.add_cookie(i)\n",
    "browser.get(url)\n",
    "# print(\"【重载后的Cookies】\", browser.get_cookies())\n",
    "data = browser.page_source\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "# #【问题描述】\n",
    "# 首先，我们拿到的Cookies域是.lagou.com，但我们的driver只能访问sec.lagou.com\n",
    "# 结果Selenium不能在sec.lagou.com域下读取.lagou.com域下的Cookies\n",
    "#【解决方案】1. 想办法把driver定位到.lagou.com的域下（这个思路ok） \n",
    "# 2. 想办法在sec.lagou.com域下加载.lagou.com域下的Cookies（这个暂时不行）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#【这里零零碎碎地记录了一些拉钩网招聘列表页面的解析式】\n",
    "from scrapy.selector import Selector\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.headless = True\n",
    "browser = webdriver.Firefox(options=options)\n",
    "\n",
    "url = 'https://www.lagou.com/zhaopin/CTO/?filterOption=3'\n",
    "browser.get(url)\n",
    "data = browser.page_source\n",
    "browser.quit()\n",
    "\n",
    "list_elements = Selector(text=data).xpath(\"//ul[@class='item_con_list']/li\")\n",
    "for list_element in list_elements:\n",
    "#     print(list_element.get())\n",
    "#     print(\"_______________________________________________________________________\")\n",
    "    print(list_element.xpath(\".//span[@class='add']/em/text()\").get())\n",
    "    print(list_element.xpath('./@data-positionname').get())\n",
    "    print(list_element.xpath('./@data-salary').get())\n",
    "    print(list_element.xpath(\".//a[@class='position_link']/@href\").get())\n",
    "\n",
    "list_elements = browser.find_elements_by_xpath(\"//ul[@class='item_con_list']/li\")\n",
    "for list_element in list_elements:\n",
    "#     print(list_element.text)\n",
    "    location = list_element.find_element_by_xpath(\".//span[@class='add']/em\").text\n",
    "    title = list_element.get_attribute('data-positionname')\n",
    "    salary = list_element.get_attribute('data-salary')\n",
    "    href = list_element.find_element_by_xpath(\".//a[@class='position_link']\").get_attribute('href')\n",
    "    print(location, title, salary, href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157058 2443\n"
     ]
    }
   ],
   "source": [
    "## 【下面这一坨是测试代码，用来测试Scrapy对接Splash的】\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.headless = True\n",
    "browser = webdriver.Firefox(options=options)\n",
    "\n",
    "# url = 'https://movie.douban.com/annual/2018'\n",
    "url = 'https://www.baidu.com/'\n",
    "\n",
    "browser.get(url)\n",
    "data = browser.page_source\n",
    "browser.quit()\n",
    "\n",
    "data2 = requests.get(url).text\n",
    "\n",
    "print(len(data), len(data2))\n",
    "\n",
    "# list_elements = Selector(text=data).xpath(\"//ul[@class='item_con_list']/li\")\n",
    "# for list_element in list_elements:\n",
    "# #     print(list_element.get())\n",
    "# #     print(\"_______________________________________________________________________\")\n",
    "#     print(list_element.xpath(\".//span[@class='add']/em/text()\").get())\n",
    "#     print(list_element.xpath('./@data-positionname').get())\n",
    "#     print(list_element.xpath('./@data-salary').get())\n",
    "#     print(list_element.xpath(\".//a[@class='position_link']/@href\").get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'BAIDUID', 'value': '2D8E6373B3EA601A49221737BD2B8131:FG=1', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False, 'expiry': 3704288702}, {'name': 'BIDUPSID', 'value': '2D8E6373B3EA601A49221737BD2B8131', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False, 'expiry': 3704288702}, {'name': 'PSTM', 'value': '1556805053', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False, 'expiry': 3704288702}, {'name': 'delPer', 'value': '0', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False}, {'name': 'BD_HOME', 'value': '0', 'path': '/', 'domain': 'www.baidu.com', 'secure': False, 'httpOnly': False}, {'name': 'H_PS_PSSID', 'value': '1423_21124_18560_28775_28722_28963_28831_28585_26350_28604', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False}, {'name': 'BD_UPN', 'value': '13314752', 'path': '/', 'domain': 'www.baidu.com', 'secure': False, 'httpOnly': False, 'expiry': 1557669056}]\n",
      "[]\n",
      "True\n",
      "[{'name': 'BAIDUID', 'value': '2D8E6373B3EA601A49221737BD2B8131:FG=1', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False, 'expiry': 3704288702}, {'name': 'BIDUPSID', 'value': '2D8E6373B3EA601A49221737BD2B8131', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False, 'expiry': 3704288702}, {'name': 'PSTM', 'value': '1556805053', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False, 'expiry': 3704288702}, {'name': 'delPer', 'value': '0', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False}, {'name': 'BD_HOME', 'value': '0', 'path': '/', 'domain': '.www.baidu.com', 'secure': False, 'httpOnly': False}, {'name': 'H_PS_PSSID', 'value': '1423_21124_18560_28775_28722_28963_28831_28585_26350_28604', 'path': '/', 'domain': '.baidu.com', 'secure': False, 'httpOnly': False}, {'name': 'BD_UPN', 'value': '13314752', 'path': '/', 'domain': '.www.baidu.com', 'secure': False, 'httpOnly': False, 'expiry': 1557669056}]\n"
     ]
    }
   ],
   "source": [
    "#【在这里测试Cookies读写】\n",
    "\n",
    "#先建立读写函数\n",
    "def save_cookies(cookies):\n",
    "    fw = open(\"test.txt\",'w+')\n",
    "    fw.write(str(cookies))\n",
    "    fw.close()\n",
    "\n",
    "def load_cookies():\n",
    "    fr = open(\"test.txt\",'r+')\n",
    "    cookies = eval(fr.read())\n",
    "    fr.close()\n",
    "    return cookies\n",
    "\n",
    "#测试读写函数：测试通过\n",
    "# test_data = [{ 'age': 23, 'city': 'beijing', 'skill': 'python' },{ 'age': 25, 'city': 'shanghai', 'skill': 'js' }]\n",
    "# save_cookies(test_data)\n",
    "# data_loaded = load_cookies()\n",
    "# print(test_data == data_loaded)\n",
    "\n",
    "#存储Cookies\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.headless = True\n",
    "browser = webdriver.Firefox(options=options)\n",
    "url = 'https://www.baidu.com/'\n",
    "browser.get(url)\n",
    "cookies = browser.get_cookies()\n",
    "print(cookies)  # 打印初始Cookies\n",
    "save_cookies(cookies)\n",
    "\n",
    "#读取Cookies\n",
    "browser.delete_all_cookies()\n",
    "print(browser.get_cookies())  # 清空Cookies确认\n",
    "cookies_loaded = load_cookies()\n",
    "print(cookies_loaded == cookies)  # 确保正确读取Cookies\n",
    "for cookie in cookies_loaded:\n",
    "    browser.add_cookie(cookie)\n",
    "print(browser.get_cookies())  # 确保正确加载Cookies\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'andy': {'age': 23, 'city': 'beijing', 'skill': 'python'}, 'william': {'age': 25, 'city': 'shanghai', 'skill': 'js'}}, {'andy': {'age': 23, 'city': 'beijing', 'skill': 'python'}, 'william': {'age': 25, 'city': 'shanghai', 'skill': 'js'}}]\n",
      "[{'andy': {'age': 23, 'city': 'beijing', 'skill': 'python'}, 'william': {'age': 25, 'city': 'shanghai', 'skill': 'js'}}, {'andy': {'age': 23, 'city': 'beijing', 'skill': 'python'}, 'william': {'age': 25, 'city': 'shanghai', 'skill': 'js'}}]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#【变量读写示例】\n",
    "dic = { 'andy':{ 'age': 23, 'city': 'beijing', 'skill': 'python' }, 'william': { 'age': 25, 'city': 'shanghai', 'skill': 'js' } } \n",
    "dic = [dic,dic]\n",
    "print(dic)\n",
    "\n",
    "fw = open(\"test.txt\",'w+')\n",
    "fw.write(str(dic))      #把字典转化为str\n",
    "fw.close()\n",
    "\n",
    "fr = open(\"test.txt\",'r+')\n",
    "dic2 = eval(fr.read())   #读取的str转换为字典\n",
    "fr.close()\n",
    "\n",
    "print(dic2)\n",
    "print(dic == dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'name': 'user_trace_token', 'value': '20190503104717-c3e1ca80-6d4d-11e9-b315-525400332722', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1588387637}, {'name': 'JSESSIONID', 'value': 'ABAAABAAAIAACBI2FA34BB7D7182DD8007A8390B94C425B', 'path': '/', 'domain': 'www.lagou.com', 'secure': False, 'httpOnly': True}, {'name': 'SEARCH_ID', 'value': '3efe3082e9c24a7c9c0aa15b6d057c22', 'path': '/', 'domain': 'www.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1556938049}, {'name': 'X_HTTP_TOKEN', 'value': '1d1fa82497246c3e846158655158a1e9b3ff3916d9', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False}, {'name': '_ga', 'value': 'GA1.2.2100074342.1556851651', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1619923650}, {'name': '_gat', 'value': '1', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1556852250}, {'name': 'Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6', 'value': '1556851651', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1588387650}, {'name': 'Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6', 'value': '1556851651', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False}, {'name': '_gid', 'value': 'GA1.2.1868893822.1556851651', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1556938050}, {'name': 'LGSID', 'value': '20190503104729-ca9f56bc-6d4d-11e9-9df8-5254005c3644', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1556853450}, {'name': 'PRE_UTM', 'value': '', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1556853450}, {'name': 'PRE_HOST', 'value': '', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1556853450}, {'name': 'PRE_SITE', 'value': 'https%3A%2F%2Fsec.lagou.com%2Fverify.html%3Fe%3D2%26f%3Dhttps%3A%2F%2Fwww.lagou.com%2Fjobs%2Flist_%25E7%2588%25AC%25E8%2599%25AB%3FlabelWords%3D', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1556853450}, {'name': 'PRE_LAND', 'value': 'https%3A%2F%2Fwww.lagou.com%2Fjobs%2Flist_%25E7%2588%25AC%25E8%2599%25AB%3FlabelWords%3D', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1556853450}, {'name': 'LGRID', 'value': '20190503104729-ca9f59a4-6d4d-11e9-9df8-5254005c3644', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False}, {'name': 'LGUID', 'value': '20190503104729-ca9f5a1c-6d4d-11e9-9df8-5254005c3644', 'path': '/', 'domain': '.lagou.com', 'secure': False, 'httpOnly': False, 'expiry': 1872211650}]\n"
     ]
    }
   ],
   "source": [
    "#【测试绝对地址文件读取】\n",
    "local = \"D:\\\\python project\\\\spider\\\\practice\\\\WebSpiderPractice\\\\test.txt\"\n",
    "with open(local, 'r+') as fr:\n",
    "    cookies = eval(fr.read())\n",
    "print(type(cookies))\n",
    "print(cookies)\n",
    "\n",
    "# 【测试正则替换】\n",
    "import re\n",
    "with open(local, 'r') as f:\n",
    "    data = f.read()\n",
    "# data = re.findall(\"'(\\S*?)':\", data)\n",
    "data = re.sub(\"'(\\S*?)': \",\"\\g<1>=\",data)\n",
    "data = re.sub(\"(\\W)False(\\W)\",\"\\g<1>false\\g<2>\",data)\n",
    "data = re.sub(\"(\\W)True(\\W)\",\"\\g<1>true\\g<2>\",data)\n",
    "print(data)\n",
    "local2 = \"D:\\\\python project\\\\spider\\\\practice\\\\WebSpiderPractice\\\\test2.txt\"\n",
    "with open(local2, 'w') as f:\n",
    "    f.write(str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
