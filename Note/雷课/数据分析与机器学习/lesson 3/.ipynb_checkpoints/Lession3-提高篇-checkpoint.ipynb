{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>网络数据获取——提高篇</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬虫框架\n",
    "* Scrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架，用途非常广泛，可以用于数据挖掘、监测和自动化测试。\n",
    "* 框架的力量，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。\n",
    "* Scrapy 使用了 Twisted异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 各组件作用\n",
    "![](img/1.jpg)\n",
    "#### Scrapy Engine\n",
    "引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 详细内容查看下面的数据流(Data Flow)部分。\n",
    "\n",
    "此组件相当于爬虫的“大脑”，是整个爬虫的调度中心。\n",
    "\n",
    "#### 调度器(Scheduler)\n",
    "调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。\n",
    "\n",
    "初始的爬取URL和后续在页面中获取的待爬取的URL将放入调度器中，等待爬取。同时调度器会自动去除重复的URL（如果特定的URL不需要去重也可以通过设置实现，如post请求的URL）\n",
    "\n",
    "#### 下载器(Downloader)\n",
    "下载器负责获取页面数据并提供给引擎，而后提供给spider。\n",
    "\n",
    "#### Spiders\n",
    "Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。\n",
    "\n",
    "#### Item Pipeline\n",
    "Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。\n",
    "\n",
    "当页面被爬虫解析所需的数据存入Item后，将被发送到项目管道(Pipeline)，并经过几个特定的次序处理数据，最后存入本地文件或存入数据库。\n",
    "\n",
    "#### 下载器中间件(Downloader middlewares)\n",
    "下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。\n",
    "\n",
    "通过设置下载器中间件可以实现爬虫自动更换user-agent、IP等功能。\n",
    "\n",
    "#### Spider中间件(Spider middlewares)\n",
    "Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。\n",
    "\n",
    "#### 数据流(Data flow)\n",
    "引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。\n",
    "\n",
    "引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。\n",
    "\n",
    "引擎向调度器请求下一个要爬取的URL。\n",
    "\n",
    "调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。\n",
    "\n",
    "一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。\n",
    "\n",
    "引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。\n",
    "\n",
    "Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。\n",
    "\n",
    "引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。\n",
    "\n",
    "(从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 建立Scrapy爬虫项目\n",
    "\n",
    "* 使用Scrapy创建项目\n",
    "* 使用Scrapy爬取整个网页\n",
    "* 使用Scrapy爬取所需元素\n",
    "* 使用Scrapy保存数据到json文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建项目\n",
    "在开始爬取之前，您必须创建一个新的Scrapy项目。 打开**Anaconda Prompt**进入您打算存储代码的目录中，运行下列命令:\n",
    "```\n",
    "scrapy startproject tutorial\n",
    "```\n",
    "该命令将会创建包含下列内容的 tutorial 目录:\n",
    "<pre>\n",
    "tutorial/\n",
    "    scrapy.cfg\n",
    "    tutorial/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py\n",
    "            ...\n",
    "</pre>\n",
    "这些文件分别是:\n",
    "\n",
    "* scrapy.cfg: 项目的配置文件\n",
    "* tutorial/: 该项目的python模块。之后您将在此加入代码。\n",
    "* tutorial/items.py: 项目中的item文件.\n",
    "* tutorial/pipelines.py: 项目中的pipelines文件.\n",
    "* tutorial/settings.py: 项目的设置文件.\n",
    "* tutorial/spiders/: 放置spider代码的目录."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建爬虫的模板文件\n",
    "```\n",
    "cd /tutorial\n",
    "scrapy genspider quotes quotes.toscrape.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义Item\n",
    "Item 是保存爬取到的数据的容器；其使用方法和python字典类似， 并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n",
    "\n",
    "类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field 的类属性来定义一个Item。 (如果不了解ORM, 不用担心，您会发现这个步骤非常简单)\n",
    "\n",
    "首先根据需要从quotes.toscrape.com获取到的数据对item进行建模。 我们需要从quotes中获取名字，url，以及网站的描述。 对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件:\n",
    "```py\n",
    "import scrapy\n",
    "\n",
    "class CourseItem(scrapy.Item):\n",
    "    text = scrapy.Field()\n",
    "    author = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "```\n",
    "一开始这看起来可能有点复杂，但是通过定义item， 您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编写第一个爬虫(Spider)\n",
    "Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。\n",
    "\n",
    "其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方法。\n",
    "\n",
    "为了创建一个Spider，您必须继承 scrapy.Spider 类， 且定义以下三个属性:\n",
    "\n",
    "* name: 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。\n",
    "* start_urls: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。\n",
    "* parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。\n",
    "\n",
    "以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 quotes.py 文件中:\n",
    "\n",
    "```py\n",
    "import scrapy\n",
    "\n",
    "class QuotespiderSpider(scrapy.spiders.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 爬取\n",
    "进入项目的根目录，执行下列命令启动spider:\n",
    "```\n",
    "scrapy crawl quotes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 提取Item\n",
    "通过爬取整页，我们掌握了最基本的爬虫，但是实际上，通常我们需要的只是网页中一部分对我们有用的信息，那么就涉及到了元素的过滤和筛选。\n",
    "\n",
    "* 从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制: Scrapy Selectors 。 关于selector和其他提取机制的信息请参考 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在我们的spider中加入这段代码:\n",
    "```\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.css('small.author::text').extract_first(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在尝试再次爬取，您将看到爬取到的网站信息被成功输出:\n",
    "\n",
    "scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用item\n",
    "一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:\n",
    "```\n",
    "import scrapy\n",
    "\n",
    "from tutorial.items import CourseItem\n",
    "\n",
    "\n",
    "class QuotespiderSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            item = CourseItem()\n",
    "            item['text'] = quote.css('span.text::text').extract_first()\n",
    "            item['author'] = quote.css('small.author::text').extract_first()\n",
    "            item['tags'] = quote.css('div.tags a.tag::text').extract()\n",
    "            yield item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存爬取到的数据\n",
    "* json\n",
    "```\n",
    "scrapy crawl quotes -o quotes.json\n",
    "```\n",
    "* csv\n",
    "```\n",
    "scrapy crawl quotes -o quotes.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### scrapy中查找HTML元素\n",
    "* 在前面我们已经知道使用BeautifulSoup能查找HTML中的元素，scrapy中也有强大的查找HTML元素的功能，那就是使用xpath方法。xpath方法使用XPath语法，比BeautifulSoup的select要灵活而且速度快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用xpath查找HTML中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "htmlText='''\n",
    "<html><body>\n",
    "<bookstore>\n",
    "<book>\n",
    "  <title lang=\"eng\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book>\n",
    "  <title lang=\"eng\">Learning XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "print(htmlText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用xpath查找HTML中的元素\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "selector=Selector(text=htmlText)\n",
    "print(type(selector));\n",
    "print(selector)\n",
    "s=selector.xpath(\"//title\")\n",
    "print(type(s))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) from scrapy.selector import Selector\n",
    "\n",
    "从scrapy中引入Selector类，这个类就是选择查找类。\n",
    "\n",
    "(2) selector=Selector(text=htmlText)\n",
    "\n",
    "使用htmlText的文字建立Selector类，就是装载HTML文档，文档装载后就形成一个Selector对象，就可以使用xpath查找元素。\n",
    "\n",
    "(3) print(type(selector)\n",
    "\n",
    "可看到selector是一个类型为scrapy.selector.unified.Selector，这个类型是一个有xpath方法的类型。\n",
    "\n",
    "(4) s=selector.xpath(\"//title\")\n",
    "\n",
    "这个方法在文档中查找所有的&#60;title&#62;的元素，其中\"//\"表示文档中的任何位置。\n",
    "一般地：selector.xpath(\"//tagName\")表示在权文档中搜索&#60;tagName&#62;的tags，形成一个Selector的列表。\n",
    "\n",
    "(5) print(type(s))\n",
    "\n",
    "由于&#60;title&#62;有两个元素，因此我们看到这是一个scrapy.selector.unified.SelectorList类，类似scrapy.selector.unified.Selector的列表。\n",
    "\n",
    "(6) print(s)\n",
    "\n",
    "我们看到s包含两个Selector对象，一个是&#60;Selector xpath='//title' data='&#60;title lang=\"eng\"&#62;Harry Potter&#60;/title&#62;'&#62;， 另外一个是&#60;Selector xpath='//title' data='&#60;title lang=\"eng\"&#62;Learning XML&#60;/title&#62;'&#62;。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见一般selector搜索一个&#60;tagName&#62;的HTML元素的方法是：\n",
    "selector.xpath(“//tagName”)，在装载HTML文档后selector=Selector(text=htmlText)得到的selector是对应全文档顶层的元素&#60;html&#62;的，其中\"//\"表示全文档搜索，结果是一个Selector的列表，哪怕只有一个元素也成一个列表，例如：\n",
    "\n",
    "* selector.xpath(\"//body\") 搜索到&#60;body&#62;元素，结果是一个Selector的列表，包含一个Selector元素；\n",
    "\n",
    "* selector.xpath(\"//title\")搜索到两个&#60;title&#62;元素，结果是Selector的列表，包含2个Selector元素；\n",
    "* selector.xpath(\"//book\")搜索到两个&#60;book&#62;元素，结果是Selector的列表，包含2个Selector元素；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy中查找HTML元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* XPath常用规则\n",
    "![](img/xpath1.png)\n",
    "\n",
    "* XPath查找路径\n",
    "![](img/xpath2.png)\n",
    "\n",
    "* XPath查找节点\n",
    "![](img/xpath3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、使用\"//\"表示文档下面的所有节点元素，用\"/\"表示当前节点的下一级节点元素。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "htmlText='''\n",
    "<html><body>\n",
    "<bookstore>\n",
    "<book>\n",
    "  <title lang=\"eng\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book>\n",
    "  <title lang=\"eng\">Learning XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### \"//\"与\"/\"的使用\n",
    "selector.xpath(\"//bookstore/book\") \n",
    "搜索&#60;bookstore&#62;下一级的&#60;book>元素，找到2个；\n",
    "\n",
    "selector.xpath(\"//body/book\") \n",
    "搜索&#60;body>下一级的&#60;book>元素，结果为空；\n",
    "\n",
    "selector.xpath(\"//body//book\") \n",
    "搜索&#60;body>下&#60;book>元素，找到2个；\n",
    "\n",
    "selector.xpath(\"/body//book\") \n",
    "搜索文档下一级的&#60;body>下的&#60;book>元素，找结果为空，因为文档的下一级是&#60;html>元素，不是&#60;body>元素；\n",
    "\n",
    "selector.xpath(\"/html/body//book\")或者selector.xpath(\"/html//book\") \n",
    "搜索&#60;book>元素，找到2个；\n",
    "\n",
    "selector.xpah(\"//book/title\") \n",
    "搜索文档中所有&#60;book>下一级的&#60;title>元素，找到2个，结果与selector.xpah(\"//title\")、selector.xpath(\"//bookstore//title\")一样；\n",
    "\n",
    "selector.xpath(\"//book//price\")与selector.xpath(\"//price\")\n",
    "结果一样，都是找到2个&#60;price>元素；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selector.xpath(\"//body//book\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2、使用\".\"表示当前节点元素，使用xpath可以连续调用，如果前一个xpath返回一个Selector的列表，那么这个列表可以继续调用xpath，功能是为每个列表元素调用xpath，最后结果是全部元素调用xpath的汇总。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用\".\"进行xpath连续调用\n",
    "from scrapy.selector import Selector\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<title>books</title>\n",
    "<book>\n",
    "  <title>Novel</title>\n",
    "  <title lang=\"eng\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book>\n",
    "  <title>TextBook</title>\n",
    "  <title lang=\"eng\">Learning XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//book\").xpath(\"./title\")\n",
    "for e in s:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到selector.xpath(\"//book\")首先搜索到文档中所有&#60;book>元素，总共有2个，然后再次调用xpath(\"./title\")，就是从当前元素&#60;book>开始往下一级搜索&#60;title>，每个&#60;book>都找到2个&#60;title>，因此结果有4个&#60;title>。\n",
    "\n",
    "注意如果xpath连续调用时不指定是从前一个xpath的结果元素开始的，那么默认是从全文档开始的，结果会不一样，例如：\n",
    "s=selector.xpath(\"//book\").xpath(\"/title\")\n",
    "\n",
    "结果是空的，因为后面的xpath(\"/title\")从文档开始搜索&#60;title>。\n",
    "\n",
    "s=selector.xpath(\"//book\").xpath(\"//title\")\n",
    "\n",
    "结果有10个元素，因为每个&#60;book>都驱动xpath(\"//title\")在全文档搜索&#60;title>元素，每次都搜索到5个元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、如果xpath返回的Selector对象列表，再次调用extract()函数会得到这些对象的元素文本的列表，使用extract_first()获取列表中第一个元素值，如果列表为空extract_first()的值为None。而对于单一的一个Selector对象，调用extract()函数就可以得到Selector对象对应的元素的文本值。单一的Selector对象没有extract_first()函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract与extract_first函数使用\n",
    "from scrapy.selector import Selector\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<book id=\"b1\">\n",
    "  <title lang=\"english\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book id=\"b2\">\n",
    "  <title lang=\"chinese\">学习XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//book/price\")\n",
    "print(type(s),s)\n",
    "s=selector.xpath(\"//book/price\").extract()\n",
    "print(type(s),s)\n",
    "s=selector.xpath(\"//book/price\").extract_first()\n",
    "print(type(s),s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s=selector.xpath(\"//book/price\") 得到的是SelectorList列表\n",
    "\n",
    "s=selector.xpath(\"//book/price\").extract() 得到的是&#60;price>元素的Selector对象对应的&#60;price>元素的文本组成的列表\n",
    "\n",
    "s=selector.xpath(\"//book/price\").extrac_first() 得到的是&#60;price>元素的文本组成的列表的第一个元素，是一个文本\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、xpath使用\"/@attrName\"得到一个Selector元素的attrName属性节点对象，属性节点对象也是一个Selector对象，通过extract()获取属性值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取元素属性值\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<book id=\"b1\">\n",
    "  <title lang=\"english\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book id=\"b2\">\n",
    "  <title lang=\"chinese\">学习XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//book/@id\")\n",
    "print(s)\n",
    "print(s.extract())\n",
    "for e in s:\n",
    "    print(e.extract())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s=selector.xpath(\"//book/@id\")\n",
    "\n",
    "结果是2个&#60;book>的id属性组成的SelectorList列表，即属性也是一个Selector对象；\n",
    "\n",
    "print(s.extract())\n",
    "结果是&#60;book>的id属性的两个Selector对象的属性文本值的列表，即['b1', 'b2']；\n",
    "\n",
    "for e in s:\n",
    "    print(e.extract())\n",
    "每个e是一个Selector对象，因此extract()获取对象的属性值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5、xpath使用\"/text()\"得到一个Selector元素包含的文本值，文本值节点对象也是一个Selector对象，通过extract()获取文本值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取节点的文本值\n",
    "from scrapy.selector import Selector\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<book id=\"b1\">\n",
    "  <title lang=\"english\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book id=\"b2\">\n",
    "  <title lang=\"chinese\">学习XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//book/title/text()\")\n",
    "print(s)\n",
    "print(s.extract())\n",
    "for e in s:\n",
    "    print(e.extract())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见：\n",
    "s=selector.xpath(\"//book/title/text()\")\n",
    "结果也是SelectorList列表，即文本也是一个节点；\n",
    "\n",
    "print(s.extract())\n",
    "结果是文本节点的字符串值的列表，即['Harry Potter', '学习XML']；\n",
    "\n",
    "for e in s:\n",
    "    print(e.extract())\n",
    "\n",
    "每个e是一个Selector对象，因此extract()获取对象的属性值。\n",
    "值得注意的是如果一个element的元素包含的文本不是单一的文本，那么可能会产生多个文本值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 多个文本节点值\n",
    "from scrapy.selector import Selector\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<book id=\"b1\">\n",
    "  <title lang=\"english\"><b>H</b>arry <b>P</b>otter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//book/title/text()\")\n",
    "print(s)\n",
    "print(s.extract())\n",
    "for e in s:\n",
    "    print(e.extract())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见&#60;title>中的文本值包含arry与otter两个。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6、xpath使用\"tag[condition]\"来限定一个tag元素，其中condition是由这个tag的属性、文本等计算出的一个逻辑值。如果有多个条件，那么可以写成：\"tag[condition1][condition2]...[conditionN]\" 或者：\"tag[condition1 and condition2 and ... and conditionN]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用condition限定tag元素\n",
    "from scrapy.selector import Selector\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<book id=\"b1\">\n",
    "  <title lang=\"english\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book id=\"b2\">\n",
    "  <title lang=\"chinese\">学习XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//book/title[@lang='chinese']/text()\")\n",
    "print(s.extract_first())\n",
    "s=selector.xpath(\"//book[@id='b1']/title\")\n",
    "print(s.extract_first())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见：\n",
    "s=selector.xpath(\"//book/title[@lang='chinese']/text()\")\n",
    "\n",
    "搜索&#60;book>下面属性lang=\"chinese\"的&#60;title>\n",
    "\n",
    "s=selector.xpath(\"//book[@id='b1']/title\")\n",
    "\n",
    "搜索属性id=\"b1\"的&#60;book>下面的&#60;title>。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7、xpath可以使用position()来确定其中一个元素的限制，这个选择序号是从1开始的，不是从0开始编号的，还可以通过 and、or等构造复杂的表达式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用position()序号来确定锁选择的元素\n",
    "from scrapy.selector import Selector\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<book id=\"b1\">\n",
    "  <title lang=\"english\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book id=\"b2\">\n",
    "  <title lang=\"chinese\">学习XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//book[position()=1]/title\")\n",
    "print(s.extract_first())\n",
    "s=selector.xpath(\"//book[position()=2]/title\")\n",
    "print(s.extract_first())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中\n",
    "\n",
    "s=selector.xpath(\"//book[position()=1]/title\")\n",
    "\n",
    "s=selector.xpath(\"//book[position()=2]/title\")\n",
    "\n",
    "分别选择第一、二个&#60;book>元素。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8、xpath使用星号\"*\"代表任何Element节点，不包括Text、Comment的节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用\"*\"代表任何element元素\n",
    "from scrapy.selector import Selector\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<book id=\"b1\">\n",
    "  <title lang=\"english\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book id=\"b2\">\n",
    "  <title lang=\"chinese\">学习XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//bookstore/*/title\")\n",
    "print(s.extract())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中s=selector.xpath(\"//bookstore/*/title\")是搜索&#60;bookstore>的孙子节点&#60;title>，中间隔开一层任何元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9、xpath使用\"@*\"代表任何属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用@*代表属性\n",
    "from scrapy.selector import Selector\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<book>\n",
    "  <title lang=\"english\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book id=\"b2\">\n",
    "  <title lang=\"chinese\">学习XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//book[@*]/title\")\n",
    "print(s.extract())\n",
    "s=selector.xpath(\"//@*\")\n",
    "print(s.extract())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "s=selector.xpath(\"//book[@*]/title\")\n",
    "\n",
    "是搜索任何包含属性的&#60;book>元素下面的&#60;title>，结果搜索到第二个&#60;book>\n",
    "\n",
    "s=selector.xpath(\"//@*\")\n",
    "\n",
    "是搜索文档中所有属性节点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10、xpath使用\"element/parent::*\"选择element的父节点，这个节点只有一个。如果写成element/parent::tag，就指定element的tag父节点，除非element的父节点正好为>&#60;tag>节点，不然就为None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xpath搜索元素的父节点\n",
    "from scrapy.selector import Selector\n",
    "htmlText='''\n",
    "<html>\n",
    "<body>\n",
    "<bookstore>\n",
    "<book>\n",
    "  <title lang=\"english\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book id=\"b2\">\n",
    "  <title lang=\"chinese\">学习XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>\n",
    "</body></html>\n",
    "'''\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//title[@lang='chinese']/parent::*\")\n",
    "print(s.extract())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中s=selector.xpath(\"//title[@lang='chinese']/parent::*\")是查找属性为lang='chinese'的&#60;title>元素的父节点，就是id=\"b2\"的&#60;book>元素节点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11、xpath使用\"element/folllowing-sibling::*\"搜索element后面的同级的所有兄弟节点，使用\"element/folllowing-sibling::*[position()=1]\" 搜索element后面的同级的第一个兄弟节点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 搜索后面的兄弟节点\n",
    "from scrapy.selector import Selector\n",
    "htmlText=\"<a>A1</a><b>B1</b><c>C1</c><d>D<e>E</e></d><b>B2</b><c>C2</c>\"\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//a/following-sibling::*\")\n",
    "print(s.extract())\n",
    "s=selector.xpath(\"//a/following-sibling::*[position()=1]\")\n",
    "print(s.extract())\n",
    "s=selector.xpath(\"//b[position()=1]/following-sibling::*\")\n",
    "print(s.extract())\n",
    "s=selector.xpath(\"//b[position()=1]/following-sibling::*[position()=1]\")\n",
    "print(s.extract())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s=selector.xpath(\"//b[position()=1]/following-sibling::*[position()=1]\")\n",
    "\n",
    "是搜索第一个&#60;b>节点后面的第一个兄弟节点，即&#60;c>C1&#60;/c>节点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12、xpath使用\"element/preceding-sibling::*\"搜索element前面的同级的所有兄弟节点，使用\"element/preceding-sibling::*[position()=1]\" 搜索element前面的同级的第一个兄弟节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 搜索前面的兄弟节点\n",
    "from scrapy.selector import Selector\n",
    "htmlText=\"<a>A1</a><b>B1</b><c>C1</c><d>D<e>E</e></d><b>B2</b><c>C2</c>\"\n",
    "selector=Selector(text=htmlText)\n",
    "s=selector.xpath(\"//a/preceding-sibling::*\")\n",
    "print(s.extract())\n",
    "s=selector.xpath(\"//b/preceding-sibling::*[position()=1]\")\n",
    "print(s.extract())\n",
    "s=selector.xpath(\"//b[position()=2]/preceding-sibling::*\")\n",
    "print(s.extract())\n",
    "s=selector.xpath(\"//b[position()=2]/preceding-sibling::*[position()=1]\")\n",
    "print(s.extract())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s=selector.xpath(\"//b/preceding-sibling::*[position()=1]\")\n",
    "\n",
    "是所有&#60;b>前面的第一个兄弟节点，因为有2个&#60;b>节点，因此结果是['&#60;a>A1&#60;/a>', '&#60;d>D&#60;e>E&#60;/e>&#60;/d>']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
