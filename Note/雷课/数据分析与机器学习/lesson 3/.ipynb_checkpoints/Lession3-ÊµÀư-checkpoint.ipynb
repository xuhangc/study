{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>网络数据获取——实例</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习1：下载链接文件\n",
    "* 爬取会议论文集：https://conference.bj.bcebos.com/ccks2019/eval/webpage/index.html 中文知识图谱问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习\n",
    "# 爬取会议论文集：https://conference.bj.bcebos.com/ccks2019/eval/webpage/index.html 中文知识图谱问答\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "cur_dir = '/'.join(os.path.abspath('__file__').split('/')[:-1])\n",
    "paper_dir = os.path.join(cur_dir, 'papers')\n",
    "if not os.path.exists(paper_dir):\n",
    "    os.makedirs(paper_dir)\n",
    "\n",
    "for page in range(1, 5):\n",
    "    try:\n",
    "        _page = str(page)\n",
    "        url = 'https://conference.bj.bcebos.com/ccks2019/eval/webpage/pdfs/eval_paper_6_{}.pdf'.format(_page)\n",
    "        file_name = os.path.join(paper_dir, '6_'+_page + '.pdf')        \n",
    "        print(file_name)        \n",
    "        r = requests.get(url)\n",
    "        f = open(file_name, 'wb')\n",
    "        f.write(r.content)\n",
    "        f.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2：调用API爬取数据\n",
    "* 爬取豆瓣电影中选电影的华语推荐电影数据 https://movie.douban.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习\n",
    "# 爬取豆瓣电影中选电影的华语推荐电影数据 https://movie.douban.com\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# 定义请求url\n",
    "url = \"https://movie.douban.com/j/search_subjects\"\n",
    "# 定义请求头\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36\"\n",
    "}\n",
    "fp = open('tmp/豆瓣华语电影.csv', 'w')\n",
    "# 循环构建请求参数并且发送请求\n",
    "for page_start in range(0, 20, 10):\n",
    "    params = {\n",
    "        \"type\": \"movie\",\n",
    "        \"tag\": \"华语\",\n",
    "        \"sort\": \"recommend\",\n",
    "        \"page_limit\": \"10\",\n",
    "        \"page_start\": page_start\n",
    "    }\n",
    "    response = requests.get(\n",
    "        url=url,\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = response.json()\n",
    "    # 解析结果\n",
    "    for movie in results[\"subjects\"]:\n",
    "        print(movie[\"title\"], movie[\"rate\"])\n",
    "        fp.write(('%s,%s\\n' % (movie[\"title\"], movie[\"rate\"])))\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习3：解析网页爬取数据\n",
    "* 爬取一个城市近7天的天气情况 http://www.weather.com.cn/weather/101010100.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.weather.com.cn/weather/101010100.shtml'\n",
    "# 设置头部信息\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\"} \n",
    "response = requests.get(url, headers=headers)\n",
    "# 使用utf-8进行编码，不重新编码就会成乱码\n",
    "response.encoding = 'uft-8'\n",
    "#将应答包里面的内容读取出来\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = []   #初始化一个空的list，将最终的的数据保存到list\n",
    "bs = BeautifulSoup(html,\"html.parser\")   # 创建BeautifulSoup对象\n",
    "body = bs.body  # 获取body部分\n",
    "data = body.find('div',{'id':'7d'})  # 找到id为7d的div\n",
    "ul = data.find('ul')  # 获取ul部分\n",
    "li = ul.find_all('li')  # 获取所有的li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for day in li:  # 对每个li标签中的内容进行遍历\n",
    "    if i < 7:\n",
    "        temp = []\n",
    "        date = day.find('h1').string # 找到日期\n",
    "        temp.append(date)  # 添加到temp中\n",
    "        inf = day.find_all('p')  # 找到li中的所有p标签\n",
    "        if inf[1].find('span') is None:\n",
    "            temperature_highest = None # 天气预报可能没有当天的最高气温（到了傍晚，就是这样），需要加个判断语句,来输出最低气温\n",
    "        else:\n",
    "            temperature_highest = inf[1].find('span').string # 找到最高温度\n",
    "            temperature_highest = temperature_highest.replace('℃', '') # 到了晚上网站会变，最高温度后面也有个℃\n",
    "        temperature_lowest = inf[1].find('i').string  #找到最低温度\n",
    "        temperature_lowest = temperature_lowest.replace('℃', '')  # # 最低温度后面有个℃，去掉这个符号\n",
    "        temp.append(temperature_highest)\n",
    "        temp.append(temperature_lowest)\n",
    "        final.append(temp)\n",
    "        i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "name = ['日期', '最高温', '最低温']\n",
    "df = pd.DataFrame(columns=name, data=final)\n",
    "df.to_csv('tmp/北京.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习4：动态网页数据爬取\n",
    "* 水木社区每日十大话题 http://www.newsmth.net/nForum/#!board/ShiDa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习：requests方法\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "base_url = 'http://www.newsmth.net/nForum/#!board/ShiDa'\n",
    "response = requests.get(base_url).text\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "result = soup.find_all('td', recursive=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习：Selenium方法\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "base_url = 'http://www.newsmth.net/nForum/#!board/ShiDa'\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "browser.maximize_window()\n",
    "browser.get(base_url)\n",
    "time.sleep(10)\n",
    "# 获取html\n",
    "html = browser.page_source\n",
    "browser.quit()\n",
    "\n",
    "# print(html)\n",
    "soup = BS(html, 'html.parser')\n",
    "result = soup.find_all('td')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = list()\n",
    "for j in range(0, len(result), 9):\n",
    "        item = {}\n",
    "        item['状态'] = result[j].get_text().strip()\n",
    "        item['主题'] = result[j+1].get_text().strip()\n",
    "        item['发帖时间'] = result[j+2].get_text().strip()\n",
    "        item['作者'] = result[j+3].a.get_text().strip()\n",
    "        item['评分'] = result[j+4].get_text().strip()\n",
    "        item['Like'] = result[j+5].get_text().strip()\n",
    "        item['回复'] = result[j+6].get_text().strip()\n",
    "        item['最新回复'] = result[j+7].get_text().strip()\n",
    "        item['回复作者'] = result[j+8].a.get_text().strip()\n",
    "        topics.append(item)\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'tmp/topics.json'\n",
    "with open(file_name, 'w', encoding='utf-8') as file_object:\n",
    "    for item in topics:\n",
    "        file_object.write(json.dumps(item, ensure_ascii=False))\n",
    "        file_object.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(file_name, 'r', encoding='utf-8') as file_object:\n",
    "    for line in file_object:\n",
    "        item = json.loads(line)\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例1：中国大学排名定向爬虫\n",
    "\n",
    "中国大学排名：http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习：使用CSS选择器\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "\n",
    "url=\"http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html\"\n",
    "r=requests.get(url,timeout=30)\n",
    "r.encoding=r.apparent_encoding\n",
    "soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "# print(soup.prettify())\n",
    "\n",
    "uinfo=[]\n",
    "for tr in soup.select(\".hidden_zhpm tr\"):\n",
    "    if isinstance(tr,bs4.element.Tag):\n",
    "        tds=tr('td')\n",
    "        uinfo.append([tds[0].string,tds[1].string,tds[2].string,tds[3].string])\n",
    "uinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例2：豆瓣电影排行榜\n",
    "\n",
    "豆瓣电影排行榜：https://movie.douban.com/chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小练习：抓取豆瓣电影排行榜\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "\n",
    "url=\"https://movie.douban.com/chart\"\n",
    "r=requests.get(url,headers = {\"User-Agent\":\"Mozilla/5.0 Chrome/58.0.3029.110 Safari/537.36\"}, timeout=30)\n",
    "soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "# print(soup.prettify())\n",
    "\n",
    "uinfo=[]\n",
    "for item in soup.select(\".item\"):\n",
    "    uinfo.append([item.div.a.text.strip(), item.div.p.string, item.find(\"span\", \"rating_nums\").text,item.find(\"span\", \"pl\").text])\n",
    "uinfo"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
