{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>网络数据获取——课程实践</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 课程内容\n",
    "\n",
    "* 下载链接文件\n",
    "* 调用API爬取数据\n",
    "* 解析网页爬取数据\n",
    "* 动态网页数据爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 下载链接文件\n",
    "\n",
    "* 2019全国知识图谱与语义计算大会评测论文集 https://conference.bj.bcebos.com/ccks2019/eval/webpage/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "cur_dir = '/'.join(os.path.abspath('__file__').split('/')[:-1])\n",
    "paper_dir = os.path.join(cur_dir, 'papers')\n",
    "if not os.path.exists(paper_dir):\n",
    "    os.makedirs(paper_dir)\n",
    "\n",
    "for page in range(1, 4):\n",
    "    try:\n",
    "        _page = str(page)\n",
    "        url = 'https://conference.bj.bcebos.com/ccks2019/eval/webpage/pdfs/eval_paper_1_1_{}.pdf'.format(_page)\n",
    "        file_name = os.path.join(paper_dir, _page + '.pdf')\n",
    "        print(file_name)\n",
    "        urllib.request.urlretrieve(url, file_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 普通下载\n",
    "```\n",
    "r = requests.get(url)\n",
    "f = open(file_name, 'wb')\n",
    "f.write(r.content)\n",
    "f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有效率的下载\n",
    "直接使用f.write的话，是先把r.content全部写到内存里，在写到硬盘当中，显然这样既不效率且占用内存，因此另一种更有效率的下载方式是以文件流的形式下载\n",
    "* 把get()里的stream参数设置为True\n",
    "* 使用for...in的形式写文件\n",
    "```\n",
    "r = requests.get(url, stream=True)\n",
    "f = open(file_name, 'wb')\n",
    "for a in r.iter_content(chunk_size=32):  # iter是iter\n",
    "    f.write(a)\n",
    "f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#小练习\n",
    "# 爬取会议论文集：https://conference.bj.bcebos.com/ccks2019/eval/webpage/index.html 中文知识图谱问答\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "cur_dir = '/'.join(os.path.abspath('__file__').split('/')[:-1])\n",
    "paper_dir = os.path.join(cur_dir, 'papers')\n",
    "if not os.path.exists(paper_dir):\n",
    "    os.makedirs(paper_dir)\n",
    "\n",
    "for page in range(1, 5):\n",
    "    try:\n",
    "        _page = str(page)\n",
    "        url = 'https://conference.bj.bcebos.com/ccks2019/eval/webpage/pdfs/eval_paper_6_{}.pdf'.format(_page)\n",
    "        file_name = os.path.join(paper_dir, _page + '_6.pdf')\n",
    "        print(file_name)\n",
    "        urllib.request.urlretrieve(url, file_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取静态数据\n",
    "* 调用API爬取数据\n",
    "* 解析网页爬取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调用API爬取数据\n",
    "* 爬取**豆瓣电影**中选电影的热门推荐电影数据 https://movie.douban.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 请求API地址\n",
    "* https://movie.douban.com/j/search_subjects?type=movie&tag=热门&sort=recommend&page_limit=50&page_start=0\n",
    "* 可以根据提供的标签、排序方法、每页数量、每页开始编号等参数返回相应的电影数据，这里是按推荐程度排名，从0号开始，返回热门标签下的50条电影数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 返回Json数据\n",
    "* 在浏览器中访问以上链接，得到的也是一个json格式字符串，同样转成Python字典再处理即可。\n",
    "```\n",
    "{\n",
    "\t\"subjects\": [{\n",
    "\t\t\"rate\": \"8.3\",\n",
    "\t\t\"cover_x\": 5906,\n",
    "\t\t\"title\": \"调音师\",\n",
    "\t\t\"url\": \"https:\\/\\/movie.douban.com\\/subject\\/30334073\\/\",\n",
    "\t\t\"playable\": true,\n",
    "\t\t\"cover\": \"https://img1.doubanio.com\\/view\\/photo\\/s_ratio_poster\\/public\\/p2551995207.webp\",\n",
    "\t\t\"id\": \"30334073\",\n",
    "\t\t\"cover_y\": 8268,\n",
    "\t\t\"is_new\": false\n",
    "\t}, {\n",
    "\t\t\"rate\": \"8.9\",\n",
    "\t\t\"cover_x\": 2000,\n",
    "\t\t\"title\": \"绿皮书\",\n",
    "\t\t\"url\": \"https:\\/\\/movie.douban.com\\/subject\\/27060077\\/\",\n",
    "\t\t\"playable\": true,\n",
    "\t\t\"cover\": \"https://img3.doubanio.com\\/view\\/photo\\/s_ratio_poster\\/public\\/p2549177902.webp\",\n",
    "\t\t\"id\": \"27060077\",\n",
    "\t\t\"cover_y\": 3167,\n",
    "\t\t\"is_new\": false\n",
    "\t}]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# 定义请求url\n",
    "url = \"https://movie.douban.com/j/search_subjects\"\n",
    "# 定义请求头\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36\"\n",
    "}\n",
    "# 循环构建请求参数并且发送请求\n",
    "for page_start in range(0, 100, 50):\n",
    "    params = {\n",
    "        \"type\": \"movie\",\n",
    "        \"tag\": \"热门\",\n",
    "        \"sort\": \"recommend\",\n",
    "        \"page_limit\": \"50\",\n",
    "        \"page_start\": page_start\n",
    "    }\n",
    "    response = requests.get(\n",
    "        url=url,\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    # 方式一:直接转换json方法\n",
    "    results = response.json()\n",
    "    # 方式二: 手动转换\n",
    "#     # 获取字节串\n",
    "#     content = response.content\n",
    "#     # 转换成字符串\n",
    "#     string = content.decode('utf-8')\n",
    "#     # 把字符串转成python数据类型\n",
    "#     results = json.loads(string)\n",
    "    # 解析结果\n",
    "    for movie in results[\"subjects\"]:\n",
    "        print(movie[\"title\"], movie[\"rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#小练习\n",
    "#爬取豆瓣电影中选电影的华语推荐电影数据 https://movie.douban.com\n",
    "\n",
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# 定义请求url\n",
    "url = \"https://movie.douban.com/j/search_subjects\"\n",
    "# 定义请求头\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36\"\n",
    "}\n",
    "# 循环构建请求参数并且发送请求\n",
    "for page_start in range(0, 10, 5):\n",
    "    params = {\n",
    "        \"type\": \"movie\",\n",
    "        \"tag\": \"华语\",\n",
    "        \"sort\": \"recommend\",\n",
    "        \"page_limit\": \"5\",\n",
    "        \"page_start\": page_start\n",
    "    }\n",
    "    response = requests.get(\n",
    "        url=url,\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    # 方式一:直接转换json方法\n",
    "    results = response.json()\n",
    "    # 解析结果\n",
    "    for movie in results[\"subjects\"]:\n",
    "        print(movie[\"title\"], movie[\"rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解析网页爬取数据\n",
    "\n",
    "* 简书 https://www.jianshu.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分析页面获取数据\n",
    "```\n",
    "<div id=\"list-container\">\n",
    "<!-- 文章列表模块 -->\n",
    "<ul class=\"note-list\" infinite-scroll-url=\"/\">  \n",
    "<li id=\"note-42616373\" data-note-id=\"42616373\" class=\"have-img\">\n",
    "    <a class=\"wrap-img\" href=\"/p/85732e50fbf8\" target=\"_blank\">\n",
    "      <img class=\"  img-blur-done\" src=\"//upload-images.jianshu.io/upload_images/2376211-5ae568461a3c3308.jpg?imageMogr2/auto-orient/strip|imageView2/1/w/360/h/240\" alt=\"120\">\n",
    "    </a>\n",
    "  <div class=\"content\">\n",
    "    <a class=\"title\" target=\"_blank\" href=\"/p/85732e50fbf8\">随想：卖基金浅谈＂投资＂</a>\n",
    "    <p class=\"abstract\">\n",
    "      今天，我把唯一养的基金给卖了。这只基金，我从2016年11月就开始定投了，一月定投100元，期间全部卖出过几次，但我没有终止定投就又继续定投，累...\n",
    "    </p>\n",
    "    <div class=\"meta\">\n",
    "        <span class=\"jsd-meta\">\n",
    "          <i class=\"iconfont ic-paid1\"></i> 15.5\n",
    "        </span>\n",
    "      <a class=\"nickname\" target=\"_blank\" href=\"/u/c2969a4ab893\">沐滢</a>\n",
    "        <a target=\"_blank\" href=\"/p/85732e50fbf8#comments\">\n",
    "          <i class=\"iconfont ic-list-comments\"></i> 23\n",
    "</a>      <span><i class=\"iconfont ic-list-like\"></i> 43</span>\n",
    "    </div>\n",
    "  </div>\n",
    "</li>  \n",
    "\n",
    "</ul>\n",
    "<!-- 文章列表模块 -->\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0\"}\n",
    "url = 'https://www.jianshu.com/'\n",
    "response = requests.get(url, headers=headers)\n",
    "html = response.text\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('<a class=\"title\" target=\"_blank\" href=(.*?)>(.*?)</a>.*?<p class=\"abstract\">(.*?)</p>', re.S)\n",
    "items = re.findall(pattern, str(html))\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items:\n",
    "    print({'url': item[0].strip(), 'name': item[1].strip(), 'text': item[2].replace(\"\\n\", \"\").strip()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0\"}\n",
    "url = 'https://www.jianshu.com/'\n",
    "r = requests.get(url=url, headers=headers)\n",
    "html = r.text        \n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "con = soup.find(id='list-container')\n",
    "con_list = con.find_all('div', class_=\"content\")\n",
    "con_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in con_list:\n",
    "    url = i.find('a', class_='title')['href']\n",
    "    name = i.find('a', class_='title').string\n",
    "    text = i.find('p', class_='abstract').get_text()\n",
    "    print({'url': url.strip(), 'name': name.strip(), 'text': text.strip()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 爬取动态数据\n",
    "* 爬取空气质量数据 https://www.aqistudy.cn/historydata/daydata.php?city=北京&month=2015-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```py\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "base_url = 'https://www.aqistudy.cn/historydata/daydata.php?city='\n",
    "city = '北京'\n",
    "\n",
    "month_set = list()\n",
    "for i in range(1, 10):\n",
    "    month_set.append(('2018-0%s' % i))\n",
    "for i in range(10, 13):\n",
    "    month_set.append(('2018-%s' % i))\n",
    "print(month_set)\n",
    "\n",
    "file_name = city + '.csv'\n",
    "fp = open('tmp/' + file_name, 'w')\n",
    "for i in range(len(month_set)):\n",
    "    str_month = month_set[i]\n",
    "    weburl = ('%s%s&month=%s' % (base_url, city, str_month))\n",
    "    response = requests.get(weburl).content\n",
    "    soup = BeautifulSoup(response, 'html.parser', from_encoding='utf-8')\n",
    "    result = soup.find_all('td', attrs={'align': 'center'}, recursive=True)\n",
    "\n",
    "    for j in range(0, len(result), 9):\n",
    "        record_day = result[j].get_text().strip()\n",
    "        record_aqi = result[j + 1].get_text().strip()\n",
    "        fp.write(('%s,%s\\n' % (record_day, record_aqi)))\n",
    "    print('%s,%s---DONE' % (city, str_month))\n",
    "fp.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "base_url = 'https://www.aqistudy.cn/historydata/daydata.php?city='\n",
    "city = '北京'\n",
    "\n",
    "month_set = list()\n",
    "for i in range(1, 10):\n",
    "    month_set.append(('2018-0%s' % i))\n",
    "for i in range(10, 13):\n",
    "    month_set.append(('2018-%s' % i))\n",
    "print(month_set)\n",
    "\n",
    "file_name = city + '.csv'\n",
    "fp = open('tmp/' + file_name, 'w')\n",
    "for i in range(len(month_set)):\n",
    "    str_month = month_set[i]\n",
    "    weburl = ('%s%s&month=%s' % (base_url, city, str_month))\n",
    "    response = requests.get(weburl).content\n",
    "    soup = BeautifulSoup(response, 'html.parser', from_encoding='utf-8')\n",
    "    result = soup.find_all('td', attrs={'align': 'center'}, recursive=True)\n",
    "\n",
    "    for j in range(0, len(result), 9):\n",
    "        record_day = result[j].get_text().strip()\n",
    "        record_aqi = result[j + 1].get_text().strip()\n",
    "        fp.write(('%s,%s\\n' % (record_day, record_aqi)))\n",
    "    print('%s,%s---DONE' % (city, str_month))\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法二"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "base_url = 'https://www.aqistudy.cn/historydata/daydata.php?city='\n",
    "city = '北京'\n",
    "\n",
    "month_set = list()\n",
    "for i in range(1, 10):\n",
    "    month_set.append(('2018-0%s' % i))\n",
    "for i in range(10, 13):\n",
    "    month_set.append(('2018-%s' % i))\n",
    "print(month_set)\n",
    "\n",
    "file_name = city + '.csv'\n",
    "fp = open('tmp/' + file_name, 'w', newline='', encoding='utf-8')\n",
    "fieldnames = ['city', 'date', 'AQI', 'LEVEL', 'PM2_5', 'PM10', 'SO2', 'CO', 'NO2', 'O3_8h']\n",
    "writer = csv.DictWriter(fp, fieldnames=fieldnames)\n",
    "writer.writeheader()\n",
    "browser = webdriver.Firefox()\n",
    "browser.maximize_window()\n",
    "for i in range(len(month_set)):\n",
    "    str_month = month_set[i]\n",
    "    weburl = ('%s%s&month=%s' % (base_url, city, str_month))\n",
    "    browser.get(weburl)\n",
    "    time.sleep(1)\n",
    "    # 获取html\n",
    "    html = browser.page_source\n",
    "    soup = BS(html, 'html.parser', from_encoding='utf-8')\n",
    "    result = soup.find_all('td', attrs={'align': 'center'}, recursive=True)\n",
    "\n",
    "    for j in range(0, len(result), 9):\n",
    "        item = {}\n",
    "        # 城市\n",
    "        item['city'] = city\n",
    "        # 日期\n",
    "        item['date'] = result[j].get_text().strip()\n",
    "        # AQI\n",
    "        item['AQI'] = result[j+1].get_text().strip()\n",
    "        # 质量等级\n",
    "        item['LEVEL'] = result[j+2].get_text().strip()\n",
    "        # PM2.5\n",
    "        item['PM2_5'] = result[j+3].get_text().strip()\n",
    "        # PM10\n",
    "        item['PM10'] = result[j+4].get_text().strip()\n",
    "        # SO2\n",
    "        item['SO2'] = result[j+5].get_text().strip()\n",
    "        # CO\n",
    "        item['CO'] = result[j+6].get_text().strip()\n",
    "        # NO2\n",
    "        item['NO2'] = result[j+7].get_text().strip()\n",
    "        # O3_8h\n",
    "        item['O3_8h'] = result[j+8].get_text().strip()\n",
    "        writer.writerow(item)\n",
    "        print(item)\n",
    "    print('%s,%s---DONE' % (city, str_month))\n",
    "fp.close()\n",
    "browser.quit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "base_url = 'https://www.aqistudy.cn/historydata/daydata.php?city='\n",
    "city = '北京'\n",
    "\n",
    "month_set = list()\n",
    "for i in range(1, 10):\n",
    "    month_set.append(('2018-0%s' % i))\n",
    "for i in range(10, 13):\n",
    "    month_set.append(('2018-%s' % i))\n",
    "print(month_set)\n",
    "\n",
    "file_name = city + '.csv'\n",
    "fp = open('tmp/' + file_name, 'w', newline='', encoding='utf-8')\n",
    "fieldnames = ['city', 'date', 'AQI', 'LEVEL', 'PM2_5', 'PM10', 'SO2', 'CO', 'NO2', 'O3_8h']\n",
    "writer = csv.DictWriter(fp, fieldnames=fieldnames)\n",
    "writer.writeheader()\n",
    "browser = webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "for i in range(len(month_set)):\n",
    "    str_month = month_set[i]\n",
    "    weburl = ('%s%s&month=%s' % (base_url, city, str_month))\n",
    "    browser.get(weburl)\n",
    "    time.sleep(10)\n",
    "    # 获取html\n",
    "    html = browser.page_source\n",
    "    soup = BS(html, 'html.parser', from_encoding='utf-8')\n",
    "    result = soup.find_all('td', attrs={'align': 'center'}, recursive=True)\n",
    "\n",
    "    for j in range(0, len(result), 9):\n",
    "        item = {}\n",
    "        # 城市\n",
    "        item['city'] = city\n",
    "        # 日期\n",
    "        item['date'] = result[j].get_text().strip()\n",
    "        # AQI\n",
    "        item['AQI'] = result[j+1].get_text().strip()\n",
    "        # 质量等级\n",
    "        item['LEVEL'] = result[j+2].get_text().strip()\n",
    "        # PM2.5\n",
    "        item['PM2_5'] = result[j+3].get_text().strip()\n",
    "        # PM10\n",
    "        item['PM10'] = result[j+4].get_text().strip()\n",
    "        # SO2\n",
    "        item['SO2'] = result[j+5].get_text().strip()\n",
    "        # CO\n",
    "        item['CO'] = result[j+6].get_text().strip()\n",
    "        # NO2\n",
    "        item['NO2'] = result[j+7].get_text().strip()\n",
    "        # O3_8h\n",
    "        item['O3_8h'] = result[j+8].get_text().strip()\n",
    "        writer.writerow(item)\n",
    "        print(item)\n",
    "    print('%s,%s---DONE' % (city, str_month))\n",
    "fp.close()\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
