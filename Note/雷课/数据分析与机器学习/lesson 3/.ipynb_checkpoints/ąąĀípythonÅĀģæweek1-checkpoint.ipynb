{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "课程地址:https://www.icourse163.org/course/BIT-1001870001  \n",
    "笔记内容结合课件整理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对北理爬虫课程第一周内容回顾,本周主要介绍了requests库的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Request库入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先来看下request的基本使用,基本使用如下\n",
    "\n",
    "requests.get(url, params=None, **kwargs)\n",
    "\n",
    "- url : 拟获取页面的url链接\n",
    "\n",
    "- params : url中的额外参数，字典或字节流格式，可选\n",
    "\n",
    "- **kwargs: 12个控制访问的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"http://www.baidu.com\")\n",
    "print(r.status_code)\n",
    "r.text[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里可以看到r.text内容有乱码，这里是编码的问题，后续会处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看下r的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到r是Response类型的，这里再来看下Response对象的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "status_code是状态相应码,如果是200表示正常，否则为异常，所以后续处理的时候要使用try...except"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoding从HTTP header中猜测的响应内容编码方式。如果header中不存在charset，则认为编码为ISO‐8859‐1。\n",
    "r.text根据r.encoding显示网页内容。这也就是为什么之前的内容中有乱码，因为内容里有中文,而编码方式为'ISO-8859-1'。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.apparent_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apparent_encoding从内容中分析出的响应内容编码方式（备选编码方式）。根据网页内容分析出的编码方式\n",
    "可以看作是r.encoding的备选。所以一般处理的时候会让r.encoding=r.apparent_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里再来看下Request库具体的异常种类。\n",
    "\n",
    "| 异常        | 说明           |\n",
    "| ------------- |:-------------:|\n",
    "| requests.ConnectionError      | 网络连接错误异常，如DNS查询失败、拒绝连接等 |\n",
    "| requests.HTTPError      | HTTP错误异常      |\n",
    "| requests.URLRequired      | URL缺失异常      |\n",
    "| requests.TooManyRedirects      | 超过最大重定向次数，产生重定向异常      |\n",
    "| requests.ConnectTimeout       | 连接远程服务器超时异常      |\n",
    "| requests.Timeout      | 请求URL超时，产生超时异常      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合上述异常处理以及编码问题,这里老师给了一个爬取网页的通用代码框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r=requests.get(url,timeout=30)\n",
    "        r.raise_for_status()#如果状态码不是200,引发HTTPError异常\n",
    "        r.encoding=r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"产生异常\"\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    url=\"http://www.baidu.com\"\n",
    "    print(getHTMLText(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    url=\"www.baidu.com\"\n",
    "    print(getHTMLText(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后来看下Requests库的7个主要方法,其实一般也就get,head使用的比较多，更加具体的部分可以参考老师的课件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|方法| 说明|\n",
    "| ------------- |:-------------:|\n",
    "|requests.request() |构造一个请求，支撑以下各方法的基础方法|\n",
    "|requests.get() |获取HTML网页的主要方法，对应于HTTP的GET|\n",
    "|requests.head() |获取HTML网页头信息的方法，对应于HTTP的HEAD|\n",
    "|requests.post() |向HTML网页提交POST请求的方法，对应于HTTP的POST|\n",
    "|requests.put() |向HTML网页提交PUT请求的方法，对应于HTTP的PUT|\n",
    "|requests.patch() |向HTML网页提交局部修改请求，对应于HTTP的PATCH|\n",
    "|requests.delete() |向HTML页面提交删除请求，对应于HTTP的DELETE|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里老师留了一个思考题——Requests库的爬取性能分析,找一个网页，计算其爬取100次的时间。下面尝试下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r=requests.get(url,timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.encoding=r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"产生异常\"\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    url=\"https://www.baidu.com/\"\n",
    "    start=time.time()\n",
    "    for i in range(100):\n",
    "        getHTMLText(url)\n",
    "    end=time.time()\n",
    "    print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.网络爬虫的“盗亦有道”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先来看下网络爬虫引发的问题,主要有对服务器的性能骚扰,使用数据牟利的法律风险以及用户隐私泄露。\n",
    "\n",
    "\n",
    "\n",
    "再来看下网页堆网络爬虫的限制,主要有两种:\n",
    "\n",
    "- 来源审查：判断User‐Agent进行限制\n",
    "检查来访HTTP协议头的User‐Agent域，只响应浏览器或友好爬虫的访问\n",
    "- 发布公告：Robots协议\n",
    "告知所有爬虫网站的爬取策略，要求爬虫遵守"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robots协议\n",
    "Robots Exclusion Standard，网络爬虫排除标准\n",
    "\n",
    "作用：\n",
    "网站告知网络爬虫哪些页面可以抓取，哪些不行\n",
    "\n",
    "形式：\n",
    "在网站根目录下的robots.txt文件\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robots协议具体案例\n",
    "Robots协议基本语法:\n",
    "\n",
    " \\*   代表所有，/代表根目录\n",
    " \n",
    "具体形式为\n",
    "User‐agent: \\*\n",
    "Disallow: /\n",
    "\n",
    "后续来看几个具体案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 京东robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    url=\"https://www.jd.com/robots.txt\"\n",
    "    print(getHTMLText(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来分析下这段内容的含义:\n",
    "\n",
    "User-agent: \\* \n",
    "\n",
    "Disallow: /?\\* \n",
    "\n",
    "Disallow: /pop/\\*.html \n",
    "\n",
    "Disallow: /pinpai/\\*.html?\\* \n",
    "\n",
    "这一段的意思对于任何爬虫,均不能访问后缀为/?\\*,/pop/\\*.html,/pinpai/\\*.html?\\* 的网页\n",
    "\n",
    "User-agent: EtaoSpider \n",
    "\n",
    "Disallow: / \n",
    "\n",
    "User-agent: HuihuiSpider \n",
    "\n",
    "Disallow: / \n",
    "\n",
    "User-agent: GwdangSpider \n",
    "\n",
    "Disallow: / \n",
    "\n",
    "User-agent: WochachaSpider \n",
    "\n",
    "Disallow: /\n",
    "\n",
    "这几段的意思是EtaoSpider,HuihuiSpider,GwdangSpider,WochachaSpider这几个网络爬虫不能访问京东的任何页面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Request库网络爬取实战\n",
    "\n",
    "这一部分来看几个具体案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例1：京东商品页面的爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url=\"https://item.jd.com/2967929.html\"\n",
    "try:\n",
    "    r=requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    r.encoding=r.apparent_encoding\n",
    "    print(r.text[:1000])\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例2：亚马逊商品页面的爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://www.amazon.cn/gp/product/B01M8L5Z3Y\")\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分结果和老师的就有所不同了，老师那边返回的是503，查看结果之后发现是代理user-agent了问题，所以对user-agent进行了设置，这里两种均尝试下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#未修改user-agent\n",
    "import requests\n",
    "url=\"https://www.amazon.cn/gp/product/B01M8L5Z3Y\"\n",
    "try:\n",
    "    r=requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    r.encoding=r.apparent_encoding\n",
    "    print(r.text[2000:3000])\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#修改user-agent\n",
    "import requests\n",
    "url=\"https://www.amazon.cn/gp/product/B01M8L5Z3Y\"\n",
    "try:\n",
    "    kv={'user-agent':'Mozilla/5.0'}\n",
    "    r=requests.get(url,headers=kv)\n",
    "    r.raise_for_status()\n",
    "    r.encoding=r.apparent_encoding\n",
    "    print(r.text[2000:3000])\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分和之前结果竟然不一样，也是挺神奇的，后续再探索原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例3：百度/360搜索关键字提交\n",
    "\n",
    "百度的关键词接口：\n",
    "\n",
    "http://www.baidu.com/s?wd=keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "keyword=\"Python\"\n",
    "try:\n",
    "    kv={'wd':keyword}\n",
    "    r=requests.get(\"http://www.baidu.com/s\",params=kv)\n",
    "    print(r.request.url)\n",
    "    r.raise_for_status()\n",
    "    print(len(r.text))\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "360的关键词接口：\n",
    "\n",
    "http://www.so.com/s?q=keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "keyword=\"Python\"\n",
    "try:\n",
    "    kv={'q':keyword}\n",
    "    r=requests.get(\"http://www.so.com/s\",params=kv)\n",
    "    print(r.request.url)\n",
    "    r.raise_for_status()\n",
    "    print(len(r.text))\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例4：网络图片的爬取和存储\n",
    "\n",
    "网络图片链接的格式：\n",
    "\n",
    "http://www.example.com/picture.jpg\n",
    "\n",
    "国家地理：http://www.nationalgeographic.com.cn/\n",
    "\n",
    "选择一个图片Web页面：\n",
    "\n",
    "http://www.nationalgeographic.com.cn/photography/photo_of_the_day/3921.html\n",
    "\n",
    "右键复制图片地址\n",
    "\n",
    "http://image.nationalgeographic.com.cn/2017/0211/20170211061910157.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "url=\"http://image.nationalgeographic.com.cn/2017/0211/20170211061910157.jpg\"\n",
    "root=r\"E:/北理爬虫课程/\"\n",
    "path=root+url.split('/')[-1]\n",
    "try:\n",
    "    #这一步是防止目标路径不存在\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "    #判断路径下是否有该张图片\n",
    "    if not os.path.exists(path):\n",
    "        r=requests.get(url)\n",
    "        with open(path,'wb') as f:\n",
    "            f.write(r.content)\n",
    "            f.close()\n",
    "            print(\"文件保存成功\")\n",
    "    else:\n",
    "        print(\"文件已存在\")\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例5：IP地址归属地的自动查询\n",
    "\n",
    "这里老师提供了一个可以查询ip归属地的网站:\n",
    "\n",
    "http://m.ip138.com/ip.asp?ip=ipaddress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url=\"http://m.ip138.com/ip.asp?ip=\"\n",
    "try:\n",
    "    r=requests.get(url+'202.204.80.112')\n",
    "    r.raise_for_status()\n",
    "    r.encoding=r.apparent_encoding\n",
    "    print(r.text[-500:])\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
