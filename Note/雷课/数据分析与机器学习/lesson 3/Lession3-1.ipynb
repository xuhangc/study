{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>网络数据获取——基本操作</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 课程内容\n",
    "\n",
    "* 1.爬虫初探\n",
    "* 2.网络请求——requests\n",
    "* 3.文本解析——BeautifulSoup\n",
    "* 4.动态网页——selenium\n",
    "* 5.爬虫法律问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.爬虫初探"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 请求分析\n",
    "使用浏览器开发者工具查看网页结构与请求分析。\n",
    "\n",
    "![](./img/请求分析.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 爬取数据\n",
    "\n",
    "初步体验爬取数据的过程\n",
    "\n",
    "* 要求：从网站 https://movie.douban.com 爬取图片，并保存到本地\n",
    "* 步骤：1、构造URL；2、请求URL；3、解析数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 引入所需包\n",
    "import requests\n",
    "\n",
    "# 第1步：构造URL\n",
    "url = 'https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2626308994.jpg'\n",
    "\n",
    "# 第2步：请求URL\n",
    "r = requests.get(url)\n",
    "\n",
    "# 第3步：解析数据\n",
    "with open(\"tmp/心灵奇旅.jpg\", \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 HTML文件\n",
    "```html\n",
    "<html>\n",
    "<head>\n",
    "    <title>雷课教育</title>\n",
    "    <style type=\"text/css\">\n",
    "        .sp-title1 { font-size: 24px; line-height: 30px; font-weight: 300; }\n",
    "        .sp-title2 { font-size: 24px; line-height: 30px; font-weight: 300; }\n",
    "        .sub-text1 { color: red; }\n",
    "        .sub-text2 { color: blue; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "<div>\n",
    "    <span class=\"sp-title1\" id=\"title1\">我的第一个标题</span>\n",
    "    <p class=\"sub-text1\">我的第一个段落。</p>\n",
    "</div>\n",
    "<div>\n",
    "    <span class=\"sp-title2\" id=\"title2\">我的第二个标题</span>\n",
    "    <p class=\"sub-text2\">我的第二个段落。</p>\n",
    "</div>\n",
    "<script type=\"text/javascript\">\n",
    "    document.getElementById(\"title1\").innerHTML = '我是新改的'\n",
    "</script>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "* HTML教程：https://www.w3school.com.cn/html/index.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.网络请求——requests\n",
    "* Python 提供了很多模块来支持 HTTP 协议的网络编程，urllib、urllib2、urllib3、httplib、httplib2，都是和 HTTP 相关的模块，看名字觉得很反人类，更糟糕的是这些模块在 Python2 与 Python3 中有很大的差异，如果业务代码要同时兼容 2 和 3，写起来会让人崩溃。\n",
    "\n",
    "* 幸运地是，繁荣的 Python 社区给开发者带来了一个非常惊艳的 HTTP 库 requests，一个真正给人用的HTTP库。它是 GitHUb 关注数最多的 Python 项目之一，requests 的作者是 Kenneth Reitz 大神。\n",
    "\n",
    "* requests 实现了 HTTP 协议中绝大部分功能，它提供的功能包括 Keep-Alive、连接池、Cookie持久化、内容自动解压、HTTP代理、SSL认证、连接超时、Session等很多特性，最重要的是它同时兼容 python2 和 python3。\n",
    "\n",
    "* 让 HTTP 服务人类 https://requests.readthedocs.io/zh_CN/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 发送请求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取response对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过GET请求访问一个页面\n",
    "response = requests.get(\"https://www.baidu.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请求返回 Response 对象，Response 对象是 对 HTTP 协议中服务端返回给浏览器的响应数据的封装，响应的中的主要元素包括：状态码status_code、内容编码方式encoding与apparent_encoding、原因短语reason、响应首部headers等等，这些属性都封装在Response 对象中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### response对象的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http请求的返回状态，200表示连接成功，400代表失败\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从HTTP header中猜测的响应内容编码方式，若header中不存在charset,则默认为'ISO-8859-1'\n",
    "response.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从内容中分析出的响应内容编码方式\n",
    "response.apparent_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 原因短语\n",
    "response.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 响应首部\n",
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requests库的异常\n",
    "\n",
    "| 异常        | 说明           |\n",
    "| ------------- |:-------------:|\n",
    "| requests.ConnectionError      | 网络连接错误异常，如DNS查询失败、拒绝连接等 |\n",
    "| requests.HTTPError      | HTTP错误异常      |\n",
    "| requests.URLRequired      | URL缺失异常      |\n",
    "| requests.ConnectTimeout       | 连接远程服务器超时异常      |\n",
    "| requests.Timeout      | 请求URL超时，产生超时异常      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 网络连接错误异常\n",
    "requests.ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HTTP错误异常\n",
    "requests.HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# URL缺失异常\n",
    "requests.URLRequired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 连接远程服务器超时异常\n",
    "requests.ConnectTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 请求URL超时，产生超时异常\n",
    "requests.Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建请求查询参数\n",
    "* 很多URL都带有很长一串参数，称这些参数为URL的查询参数，用”?”附加在URL链接后面，多个参数之间用”&”隔开，比如：httpbin.org/get?key=val 。\n",
    "* Requests允许使用 params 关键字参数，以一个字符串字典来提供这些参数。举例来说，如果你想传递 key1=value1 和 key2=value2 到 httpbin.org/get ，那么你可以使用如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 方法一\n",
    "response = requests.get(\"http://baidu.com\", params={'wd': 'python'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 打印输出该 URL，你能看到 URL 已被正确编码：\n",
    "response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 方法二\n",
    "response = requests.get('http://baidu.com?wd=python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 打印输出该 URL，你能看到 URL 已被正确编码：\n",
    "response.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建请求首部 Headers\n",
    "* requests 可以很简单地指定请求首部字段`Headers`，比如有时要指定`User-Agent`伪装成浏览器发送请求，以此来蒙骗服务器。直接传递一个字典对象给参数 headers 即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 请求百度网页\n",
    "response = requests.get('https://www.baidu.com')\n",
    "response.text # 获取网页内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看相应状态码\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意: 所有的 header 值必须是 string、bytestring 或者 unicode。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 请求知乎网页\n",
    "response = requests.get(\"https://www.zhihu.com\")\n",
    "response.text # 获取网页内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看相应状态码\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为访问知乎需要头部信息，通过谷歌或火狐浏览器开发者模式就可以看到用户代理，将用户代理添加到头部信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设定浏览器User-Agent\n",
    "\n",
    "# 构建请求首部Headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 其他请求方法\n",
    "\n",
    "* requests 除了支持 GET 请求外，还支持 HTTP 规范中的其它所有方法，包括 POST、PUT、DELTET、HEADT、OPTIONS方法。\n",
    "\n",
    "|方法| 说明|\n",
    "| ------------- |:-------------:|\n",
    "|requests.request() |构造一个请求，支撑以下各方法的基础方法|\n",
    "|requests.get() |获取HTML网页的主要方法，对应于HTTP的GET|\n",
    "|requests.head() |获取HTML网页头信息的方法，对应于HTTP的HEAD|\n",
    "|requests.post() |向HTML网页提交POST请求的方法，对应于HTTP的POST|\n",
    "|requests.put() |向HTML网页提交PUT请求的方法，对应于HTTP的PUT|\n",
    "|requests.patch() |向HTML网页提交局部修改请求，对应于HTTP的PATCH|\n",
    "|requests.delete() |向HTML页面提交删除请求，对应于HTTP的DELETE|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* requests 可以非常灵活地构建 POST 请求需要的数据，如果服务器要求发送的数据是表单数据，则可以指定关键字参数 data，如果要求传递 json 格式字符串参数，则可以使用json关键字参数，参数的值都可以字典的形式传过去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 作为表单数据传输给服务器\n",
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.post(\"http://httpbin.org/post\", data=payload) # get params | post data, 两者都是JSON数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 作为 json 格式的字符串格式传输给服务器\n",
    "import json\n",
    "payload = {'key1': 'value1'}\n",
    "r = requests.post('http://httpbin.org/post', json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 响应内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response中的响应体\n",
    "* HTTP返回的响应消息中很重要的一部分内容是响应体，响应体在 requests 中处理非常灵活，与响应体相关的属性有：content、text、json()。\n",
    "* HTTP Content-type 对照表: https://www.runoob.com/http/http-content-type.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、content 是 **byte 类型**，适合直接将内容保存到文件系统或者传输到网络中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 请求一幅图片\n",
    "r = requests.get(\"https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2626308994.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看文件类型\n",
    "r.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看文件内容\n",
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看文本内容\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将文件内容另存为 test.jpg\n",
    "with open(\"tmp/test.jpg\", \"wb\") as f: # w写 b二进制\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、text 是 **str 类型**，比如一个普通的 HTML 页面，需要对文本进一步分析时，使用 text。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 请求一个网页\n",
    "r = requests.get(\"https://www.baidu.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看文件类型\n",
    "r.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看文本内容\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、如果使用第三方开放平台或者API接口爬取数据时，返回的内容是**json格式**的数据时，那么可以直接使用json()方法返回一个经过json.loads()处理后的对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 请求一个Json格式API\n",
    "hd = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "r = requests.get('https://movie.douban.com/j/search_subjects?type=movie&tag=%E7%83%AD%E9%97%A8&sort=recommend&page_limit=20&page_start=0',headers=hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看文件类型\n",
    "r.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看文本内容\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看json内容\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、如果是大文件下载，则需要使用原始套接字响应，即访问 r.raw，在初始请求中设置了 **stream=True**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://www.baidu.com') #普通文件的下载方法\n",
    "r.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.raw.read(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://www.baidu.com', stream=True) #大文件的下载方法\n",
    "r.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.raw.read(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 响应状态码\n",
    "\n",
    "* 200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\o/', '✓')\n",
    "\n",
    "* 400: ('bad_request', 'bad')\n",
    "\n",
    "* 401: ('unauthorized',)\n",
    "\n",
    "* 403: ('forbidden',)\n",
    "\n",
    "* 404: ('not_found', '-o-')\n",
    "\n",
    "* 500: ('internal_server_error', 'server_error', '/o\\', '✗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://www.baidu.com')\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果发送了一个错误请求(一个 4XX 客户端错误，或者 5XX 服务器错误响应)，我们可以通过 Response.raise_for_status() 来抛出异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_r = requests.get('http://httpbin.org/status/404')\n",
    "bad_r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_r.raise_for_status() # 如果出错，抛出异常信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://httpbin.org')\n",
    "r.raise_for_status() # 如果正确，返回空字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 超时设置\n",
    "* requests 发送请求时，默认请求下线程一直阻塞，直到有响应返回才处理后面的逻辑。如果遇到服务器没有响应的情况时，问题就变得很严重了，它将导致整个应用程序一直处于阻塞状态而没法处理其他请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"https://www.google.com\") # 服务器没有相应的话，会一直阻塞等待中，直到网络超时"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正确的方式的是给每个请求显示地指定一个超时时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.google.com\", timeout=1) # 服务器没有相应，1秒返回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 如何应对反爬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略1：伪装请求报头\n",
    "![](./img/fp1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略2：减轻访问频率，速度\n",
    "![](./img/fp2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略3：使用代理IP\n",
    "![](./img/fp3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代理设置\n",
    "* 当爬虫频繁地对服务器进行抓取内容时，很容易被服务器屏蔽掉，因此要想继续顺利的进行爬取数据，使用代理是明智的选择。如果你想爬取墙外的数据，同样设置代理可以解决问题，requests 完美支持代理。\n",
    "\n",
    "```py\n",
    "proxies = {\n",
    "  'http': 'http://10.10.1.10:3128',\n",
    "  'https': 'http://10.10.1.10:1080',\n",
    "}\n",
    "\n",
    "requests.get('http://example.org', proxies=proxies)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Request库网络爬取实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例1：苏宁商品页面的爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入工具包\n",
    "import requests\n",
    "\n",
    "# 指定URL\n",
    "url=\"https://product.suning.com/0000000000/11933394795.html\"\n",
    "\n",
    "# 请求数据\n",
    "r=requests.get(url)\n",
    "\n",
    "# 判断状态码，输出数据\n",
    "if r.status_code == 200:\n",
    "    print(r.text[:1000])\n",
    "else:\n",
    "    print(\"爬取失败\")\n",
    "    r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例2：百度搜索关键字提交\n",
    "\n",
    "百度的关键词接口：http://www.baidu.com/s?wd=keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入工具包\n",
    "import requests\n",
    "\n",
    "# 设定请求参数\n",
    "kv={'wd':'python'}\n",
    "\n",
    "# 请求数据\n",
    "r=requests.get(\"http://www.baidu.com/s\", params=kv) #构造参数请求\n",
    "\n",
    "# 打印URL\n",
    "print(r.request.url)\n",
    "\n",
    "# 判断状态码，输出数据\n",
    "if r.status_code == 200:\n",
    "    print(r.text[:1000])\n",
    "else:\n",
    "    print(\"爬取失败\")\n",
    "    r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例3：知乎Python中文社区\n",
    "\n",
    "知乎Python中文社区：https://www.zhihu.com/column/Python\n",
    "```py\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "```\n",
    "学习使用`User-Agent`伪装请求报头应对反爬。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入工具包\n",
    "import requests\n",
    "\n",
    "# 指定URL\n",
    "url=\"https://www.zhihu.com/column/Python\"\n",
    "\n",
    "# 请求数据\n",
    "r=requests.get(url, headers= headers)\n",
    "\n",
    "# 判断状态码，输出数据\n",
    "if r.status_code == 200:\n",
    "    print(r.text[:1000])\n",
    "else:\n",
    "    print(\"爬取失败\")\n",
    "    r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例4：中国大学2020排名\n",
    "\n",
    "中国大学2020排名：http://www.shanghairanking.cn/rankings/bcur/2020\n",
    "```py\n",
    "r.encoding = r.apparent_encoding\n",
    "```\n",
    "学习修改`encoding`处理中文编码网页。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入工具包\n",
    "import requests\n",
    "# 指定URL\n",
    "url=\"http://www.shanghairanking.cn/rankings/bcur/2020\"\n",
    "# 请求数据\n",
    "r=requests.get(url)\n",
    "# 判断状态码，输出数据\n",
    "if r.status_code == 200:\n",
    "    print(r.text[:1000])\n",
    "else:\n",
    "    print(\"爬取失败\")\n",
    "    r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.文本解析——BeautifulSoup\n",
    "* 网络请求库神器 Requests ，请求把数据返回来之后就要提取目标数据，不同的网站返回的内容通常有多种不同的格式，一种是 json 格式，这类数据对开发者来说最友好。另一种 XML 格式的，还有一种最常见格式的是 HTML 文档，今天就来讲讲如何从 HTML 中提取出感兴趣的数据\n",
    "\n",
    "* BeautifulSoup 是一个用于解析 HTML 文档的 Python 库，通过 BeautifulSoup，你只需要用很少的代码就可以提取出 HTML 中任何感兴趣的内容，此外，它还有一定的 HTML 容错能力，对于一个格式不完整的HTML 文档，它也可以正确处理。\n",
    "\n",
    "![](./img/BS2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML 标签\n",
    "学习 BeautifulSoup4 前有必要先对 HTML 文档有一个基本认识，如下代码，HTML 是一个树形组织结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<html>  \n",
    "    <head>\n",
    "     <title>hello, world</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>BeautifulSoup</h1>\n",
    "        <p>如何使用BeautifulSoup</p>\n",
    "    <body>\n",
    "</html>\n",
    "```\n",
    "* 它由很多**标签（Tag）**组成，比如 html、head、title等等都是标签\n",
    "\n",
    "* 一个标签对构成一个节点，比如 &lt;html&gt;...&lt;/html&gt;是一个根节点\n",
    "\n",
    "* 节点之间存在某种关系，比如 h1 和 p 互为邻居，他们是相邻的兄弟（sibling）节点\n",
    "\n",
    "* h1 是 body 的直接子（children）节点，还是 html 的子孙（descendants）节点\n",
    "\n",
    "* body 是 p 的父（parent）节点，html 是 p 的祖辈（parents）节点\n",
    "\n",
    "* 嵌套在标签之间的字符串是该节点下的一个特殊子节点，比如 “hello, world” 也是一个节点，只不过没名字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 BeautifulSoup\n",
    "构建一个 BeautifulSoup 对象需要两个参数，第一个参数是将要解析的 HTML 文本字符串，第二个参数告诉 BeautifulSoup  使用哪个解析器来解析 HTML。\n",
    "\n",
    "| 解析器\t| 使用方法\t| 优势\t| 劣势 |\n",
    "|--| -- |-- |-- |\n",
    "| Python标准库 |\tBeautifulSoup(markup, \"html.parser\")\t| Python的内置标准库、执行速度适中 、文档容错能力强 | Python 2.7.3 or 3.2.2前的版本中文容错能力差|\n",
    "| lxml HTML 解析器\t| BeautifulSoup(markup, \"lxml\")\t| 速度快、文档容错能力强 | 需要安装C语言库 |\n",
    "| lxml XML 解析器\t| BeautifulSoup(markup, \"xml\") | 速度快、唯一支持XML的解析器 | 需要安装C语言库 |\n",
    "| html5lib\t| BeautifulSoup(markup, \"html5lib\")\t | 最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 | 速度慢、不依赖外部扩展 |\n",
    "\n",
    "解析器负责把 HTML 解析成相关的对象，而 BeautifulSoup 负责操作数据（增删改查）。“html.parser” 是 Python 内置的解析器，“lxml” 则是一个基于c语言开发的解析器，它的执行速度更快，不过它需要额外安装\n",
    "\n",
    "通过 BeautifulSoup 对象可以定位到 HTML 中的任何一个标签节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title> 雷课教育-计算机前沿技术教育咨询 </title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>雷课与客户关系</h1>\n",
    "<p class=\"big\">客户满意度<span class=\"percent\">99</span></p>\n",
    "<p class=\"small\">服务质量度<span class=\"percent\">99</span></p>\n",
    "<div class=\"panel\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h2>雷课课程</h2>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <ul class=\"list\" id=\"list-1\" name=\"elements\">\n",
    "            <li class=\"element\"><a href=\"http://example.com/link1\">数据科学</a></li>\n",
    "            <li class=\"element\"><a href=\"http://example.com/link2\">人工智能</a></li>\n",
    "            <li class=\"element\"><a href=\"http://example.com/link3\">数字媒体</a></li>\n",
    "            <li class=\"element\"><a href=\"http://example.com/link4\">区块链</a></li>\n",
    "        </ul>\n",
    "        <ul class=\"list list-small\" id=\"list-2\">\n",
    "            <li class=\"element\">新基建</li>\n",
    "            <li class=\"element\">新需求</li>\n",
    "            <li class=\"element\">新机会</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeatifulSoup 将 HTML 抽象成为 4 类主要的数据类型，分别是**BeautifulSoup**，**Tag** , **NavigableString** , Comment 。每个标签节点就是一个Tag对象，NavigableString 对象一般是包裹在Tag对象中的字符串，BeautifulSoup 对象代表整个 HTML 文档。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看soup对象\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看数据类型\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# title 标签\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看数据类型\n",
    "type(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# title 标签的内容\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看数据类型\n",
    "type(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本概念介绍完，现在可以正式进入主题了，如何从 HTML 中找到我们关心的数据？BeautifulSoup 提供了两种方式，一种是**遍历**，一种是**搜索**，通常两者结合来完成查找任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 标签选择器\n",
    "\n",
    "#### 遍历文档树\n",
    "遍历文档树，顾名思义，就是是从根节点 html 标签开始遍历，直到找到目标元素为止，遍历的一个缺陷是，如果你要找的内容在文档的末尾，那么它要遍历整个文档才能找到它，速度上就慢了。遍历文档树的另一个缺点是只能获取到与之匹配的第一个子节点，例如，如果有两个相邻的 p 标签时，第二个标签就没法通过 .p 的方式获取，这是需要借用 next_sibling 属性获取相邻的节点。\n",
    "![](./img/BS1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1、选择元素**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 遍历head标签\n",
    "soup.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 逐层遍历标签\n",
    "soup.head.title "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2、获取属性**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 遍历a标签\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取a标签的href属性\n",
    "soup.a.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取a标签的href属性\n",
    "soup.a['href'] # 常用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3、获取内容**\n",
    "* string可以返回当前节点中的内容，但是当前节点包含子节点时，.string不知道要获取哪一个节点中的内容，故返回空\n",
    "* text（或者.get_text()）可以返回当前节点所包含的所有文本内容，包括当前节点的子孙节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 遍历title标签\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取title标签内容\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 遍历p标签\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取p标签内容\n",
    "soup.p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<>…</>构成了所属关系，形成了标签的树形结构。这里将介绍对标签树的遍历，有三种遍历方式，分为下行遍历，上行遍历以及，平行遍历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4、子节点和子孙节点**\n",
    "\n",
    "**标签树的下行遍历**\n",
    "\n",
    "属性| 说明\n",
    "--|--\n",
    ".contents| 子节点的列表，将<tag>所有儿子节点存入列表\n",
    ".children| 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点\n",
    ".descendants| 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 遍历p标签\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#方法一：获取p标签里的所有子节点，以list保存\n",
    "print(soup.p.contents)#获取p标签里的所有子节点，以list保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 方法二：获取p标签里的所有子节点，以迭代输出\n",
    "print(soup.p.children)\n",
    "for i, child in enumerate(soup.p.children):\n",
    "    print(i, child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 方法三：获取p标签里的所有子节点以及子孙节点（子节点对应的子节点）\n",
    "print(soup.p.descendants)\n",
    "for i, child in enumerate(soup.p.descendants):\n",
    "    print(i, child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5、父节点和祖先节点**\n",
    "\n",
    "**标签树的上行遍历**\n",
    "\n",
    "属性| 说明\n",
    "--|--\n",
    ".parent| 节点的父亲标签\n",
    ".parents| 节点先辈标签的迭代类型，用于循环遍历先辈节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6、兄弟节点**\n",
    "\n",
    "**标签树的平行遍历**\n",
    "\n",
    "属性| 说明\n",
    "--|--\n",
    ".next_sibling |返回按照HTML文本顺序的下一个平行节点标签\n",
    ".previous_sibling |返回按照HTML文本顺序的上一个平行节点标签\n",
    ".next_siblings| 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签\n",
    ".previous_siblings |迭代类型，返回按照HTML文本顺序的前续所有平行节点标签  \n",
    "注意平行遍历发生在同一个父节点下的各节点间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 标签搜索器\n",
    "\n",
    "#### 搜索文档树\n",
    "搜索文档树是通过指定标签名来搜索元素，还可以通过指定标签的属性值来精确定位某个节点元素，最常用的两个方法就是 find 和 find_all。这两个方法在 BeatifulSoup 和 Tag 对象上都可以被调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、find_all方法\n",
    "```py\n",
    "find_all( name , attrs , recursive , text , **kwargs )\n",
    "  name：对标签名称的检索字符串\n",
    "  attrs：对标签属性值的检索字符串，可标注属性检索\n",
    "  recursive：是否对子孙全部检索，默认True\n",
    "  text：<>...</>中字符串区域的检索字符串\n",
    "```\n",
    "* 按照节点名称、属性值、文字进行的搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**name（标签名字选择）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 搜索所有的ul标签\n",
    "soup.find_all('ul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 嵌套搜索，先遍历ul，后遍历li\n",
    "for ul in soup.find_all('ul'):\n",
    "    print(ul.find_all('li'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**attr（标签的属性选择）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用标签的的属性查找\n",
    "#方法一：使用字典参数查询\n",
    "print(soup.find_all(attrs={'id': 'list-1'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#方法二：使用属性名=属性值查询\n",
    "print(soup.find_all(id='list-1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、find方法\n",
    "```\n",
    "find( name , attrs , recursive , text , **kwargs )\n",
    "```\n",
    "find返回单个元素，find_all返回所有元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用方法和find_all类似\n",
    "print(soup.find(id='list-1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、其他的一些find方法\n",
    "```\n",
    "find_parents() find_parent()\n",
    "find_parents()返回所有祖先节点，find_parent()返回直接父节点。\n",
    "\n",
    "find_next_siblings() find_next_sibling()\n",
    "find_next_siblings()返回后面所有兄弟节点，find_next_sibling()返回后面第一个兄弟节点。\n",
    "\n",
    "find_previous_siblings() find_previous_sibling()\n",
    "find_previous_siblings()返回前面所有兄弟节点，find_previous_sibling()返回前面第一个兄弟节点。\n",
    "\n",
    "find_all_next() find_next()\n",
    "find_all_next()返回节点后所有符合条件的节点, find_next()返回第一个符合条件的节点\n",
    "\n",
    "find_all_previous() 和 find_previous()\n",
    "find_all_previous()返回节点后所有符合条件的节点, find_previous()返回第一个符合条件的节点\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 CSS选择器\n",
    "通过select()直接传入CSS选择器即可完成选择\n",
    "* 通过标签名查找 ```soup.select(head)```\n",
    "* 通过类名查找 ```soup.select('.sister')```\n",
    "* 通过id名查找 ```soup.select('#link1')```\n",
    "* 通过属性查找 ```soup.select('p[name=dromouse]')```\n",
    "* 组合查找 ```soup.select(\"body p\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.select('ul li')) #选择ul标签下面的li标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.select('.panel .panel-body'))#.代表class，中间需要空格来分隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.select('#list-1 .element')) #'#'代表id。这句的意思是查找id为\"list-2\"的标签下的，class=element的元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总结\n",
    "BeatifulSoup 是一个用于操作 HTML 文档的 Python 库，初始化 BeatifulSoup 时，需要指定 HTML 文档字符串和具体的解析器。它有3类常用的数据类型，分别是 **BeautifulSoup**、**Tag**和**NavigableString** 。查找 HTML元素有两种方式，分别是**遍历文档树**和**搜索文档树**，通常快速获取数据需要二者结合。\n",
    "\n",
    "Beautiful Soup4中文文档 https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 BeautifulSoup库网络爬取实战\n",
    "\n",
    "爬取数据步骤\n",
    "* 步骤1：从网络上获取网页内容 \n",
    "* 步骤2：提取网页内容中信息到合适的数据结构\n",
    "* 步骤3：利用数据结构展示并输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例1：中国大学排名定向爬虫\n",
    "\n",
    "中国大学排名：http://www.shanghairanking.cn/rankings/bcur/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 抓取数据\n",
    "\n",
    "import requests\n",
    "url = \"http://www.shanghairanking.cn/rankings/bcur/2020\"\n",
    "r = requests.get(url)\n",
    "r.encoding = r.apparent_encoding #处理中文信息\n",
    "if r.status_code == 200:\n",
    "    html = r.text\n",
    "    print(html)\n",
    "else:\n",
    "    print(\"爬取失败\")\n",
    "    r.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 解析数据\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "tb = soup.find('tbody')\n",
    "trs = tb.find_all('tr')\n",
    "datas = []\n",
    "for tr in trs: #循环遍历每一行\n",
    "    tds = tr.find_all('td')\n",
    "    print(tds[0].text.strip(),tds[1].text.strip(),tds[2].text.strip(),tds[3].text.strip(),tds[5].text.strip())\n",
    "    datas.append({'排名':tds[0].text.strip(),'学校':tds[1].text.strip(),'省市':tds[2].text.strip(),'总分':tds[5].text.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存数据\n",
    "\n",
    "import pandas as pd\n",
    "# 构造DataFrame\n",
    "df = pd.DataFrame(columns=['排名','学校','省市','总分'], data= datas)\n",
    "# 生成csv数据\n",
    "df.to_csv('tmp/大学.csv', encoding='utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步思考，如果使用使用CSS选择器如何抓取数据。\n",
    "* 小提示 ```\n",
    "soup.select(\".rk-table tr\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.动态网页——selenium\n",
    "* Selenium是一个用于测试网站的自动化测试工具，支持各种浏览器包括Chrome、Firefox、Safari等主流界面浏览器，同时也支持phantomJS无界面浏览器，爬虫中主要用来解决JavaScript渲染问题。\n",
    "\n",
    "* Selenium中文文档：https://python-selenium-zh.readthedocs.io/zh_CN/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 看看Selenium.Webdriver支持哪些浏览器\n",
    "from selenium import webdriver\n",
    "help(webdriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 驱动下载\n",
    "\n",
    "安装ChromeDriver, 该工具供selenium使用Chrome.\n",
    "\n",
    "ChromeDriver: http://npm.taobao.org/mirrors/chromedriver/\n",
    "\n",
    "下载前先查看本地环境的Chrome版本, 然后去上面的link中下载对应的ChromeDriver版本."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selenium声明浏览器对象\n",
    "browser = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 访问页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# webdriver.Firefox()\n",
    "driver = webdriver.Chrome()     # 创建Chrome对象.\n",
    "# 操作这个对象.\n",
    "driver.get('https://www.baidu.com')     # get方式访问百度.\n",
    "time.sleep(5)\n",
    "driver.quit()   # 使用完, 记得关闭浏览器, 不然chromedriver.exe进程为一直在内存中."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Selenium库网络爬取实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例1：百度搜索页面元素交互操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "driver = webdriver.Chrome()     # 创建Chrome对象.\n",
    "# 操作这个对象.\n",
    "driver.get('https://www.baidu.com')     # get方式访问百度.\n",
    "\n",
    "print('Before search================')\n",
    "time.sleep(5)\n",
    "# 打印当前页面title\n",
    "title = driver.title\n",
    "print(title)\n",
    "\n",
    "# 打印当前页面URL\n",
    "now_url = driver.current_url\n",
    "print(now_url)\n",
    "\n",
    "driver.find_element_by_id(\"kw\").send_keys(\"python\")\n",
    "driver.find_element_by_id(\"su\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "print('After search================')\n",
    "\n",
    "# 再次打印当前页面title\n",
    "title = driver.title\n",
    "print(title)\n",
    "\n",
    "# 打印当前页面URL\n",
    "now_url = driver.current_url\n",
    "print(now_url)\n",
    "\n",
    "# 获取结果数目\n",
    "user = driver.find_element_by_class_name('nums').text\n",
    "print(user)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#关闭所有窗口\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例2：知乎Python中文社区页面的爬取\n",
    "\n",
    "知乎Python中文社区： https://www.zhihu.com/column/Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "url=\"https://www.zhihu.com/column/Python\"\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(url)\n",
    "html = browser.page_source\n",
    "print(html)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.爬虫法律问题\n",
    "网络爬虫的“盗亦有道”，网页对网络爬虫的限制，主要有两种:\n",
    "\n",
    "- 来源审查：判断User‐Agent进行限制\n",
    "检查来访HTTP协议头的User‐Agent域，只响应浏览器或友好爬虫的访问\n",
    "- 发布公告：Robots协议\n",
    "告知所有爬虫网站的爬取策略，要求爬虫遵守"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robots协议\n",
    "Robots Exclusion Standard，网络爬虫排除标准\n",
    "\n",
    "作用：\n",
    "网站告知网络爬虫哪些页面可以抓取，哪些不行\n",
    "\n",
    "形式：\n",
    "在网站根目录下的robots.txt文件\n",
    "\n",
    "Robots协议基本语法:\n",
    "\n",
    " \\*   代表所有，/代表根目录\n",
    " \n",
    "具体形式为\n",
    "User‐agent: \\*\n",
    "Disallow: /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 京东robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url=\"https://www.jd.com/robots.txt\"\n",
    "r=requests.get(url,timeout=30)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来分析下这段内容的含义:\n",
    "\n",
    "User-agent: \\* \n",
    "\n",
    "Disallow: /?\\* \n",
    "\n",
    "Disallow: /pop/\\*.html \n",
    "\n",
    "Disallow: /pinpai/\\*.html?\\* \n",
    "\n",
    "这一段的意思对于任何爬虫,均不能访问后缀为/?\\*,/pop/\\*.html,/pinpai/\\*.html?\\* 的网页\n",
    "\n",
    "User-agent: EtaoSpider \n",
    "\n",
    "Disallow: / \n",
    "\n",
    "User-agent: HuihuiSpider \n",
    "\n",
    "Disallow: / \n",
    "\n",
    "User-agent: GwdangSpider \n",
    "\n",
    "Disallow: / \n",
    "\n",
    "User-agent: WochachaSpider \n",
    "\n",
    "Disallow: /\n",
    "\n",
    "这几段的意思是EtaoSpider,HuihuiSpider,GwdangSpider,WochachaSpider这几个网络爬虫不能访问京东的任何页面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
