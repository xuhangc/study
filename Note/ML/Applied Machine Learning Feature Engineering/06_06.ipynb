{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models To Compare Features: Compare And Evaluate All Models\n",
    "\n",
    "In this section, we will do the following:\n",
    "1. Evaluate all of our saved models on the validation set\n",
    "2. Select the best model based on performance on the validation set\n",
    "3. Evaluate that model on the holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "\n",
    "val_features_raw = pd.read_csv('../../../data/final_data/val_features_raw.csv')\n",
    "val_features_original = pd.read_csv('../../../data/final_data/val_features_original.csv')\n",
    "val_features_all = pd.read_csv('../../../data/final_data/val_features_all.csv')\n",
    "val_features_reduced = pd.read_csv('../../../data/final_data/val_features_reduced.csv')\n",
    "\n",
    "val_labels = pd.read_csv('../../../data/final_data/val_labels.csv')\n",
    "\n",
    "val_features_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read In Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in models\n",
    "models = {}\n",
    "\n",
    "for mdl in ['raw_original', 'cleaned_original', 'all', 'reduced']:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models On The Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    print('{} -- \\tAccuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
    "                                                                                     accuracy,\n",
    "                                                                                     precision,\n",
    "                                                                                     recall,\n",
    "                                                                                     round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all of our models on the validation set\n",
    "evaluate_model('Raw Features', models['raw_original'], val_features_raw, val_labels)\n",
    "evaluate_model('Cleaned Features', models['cleaned_original'], val_features_original, val_labels)\n",
    "evaluate_model('All Features', models['all'], val_features_all, val_labels)\n",
    "evaluate_model('Reduced Features', models['reduced'], val_features_reduced, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Best Model On Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our test features\n",
    "test_features = pd.read_csv('../../../data/final_data/')\n",
    "test_labels = pd.read_csv('../../../data/final_data/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our final model on the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
