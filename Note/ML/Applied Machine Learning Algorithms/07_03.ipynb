{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Compare model results and final model selection\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will do the following:\n",
    "1. Evaluate all of our saved models on the validation set\n",
    "2. Select the best model based on performance on the validation set\n",
    "3. Evaluate that model on the holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "\n",
    "val_features = pd.read_csv('./val_features.csv')\n",
    "val_labels = pd.read_csv('./val_labels.csv')\n",
    "\n",
    "te_features = pd.read_csv('./test_features.csv')\n",
    "te_labels = pd.read_csv('./test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB']:\n",
    "    models[mdl] = joblib.load('./{}_model.pkl'.format(mdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'LR': LogisticRegression(C=1),\n",
       " 'SVM': SVC(C=0.1, kernel='linear'),\n",
       " 'MLP': MLPClassifier(activation='tanh', hidden_layer_sizes=(50,),\n",
       "               learning_rate='adaptive'),\n",
       " 'RF': RandomForestClassifier(max_depth=8, n_estimators=50),\n",
       " 'GB': GradientBoostingClassifier(learning_rate=0.01, n_estimators=500)}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on the validation set\n",
    "\n",
    "![Evaluation Metrics](../../img/eval_metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
    "                                                                                   accuracy,\n",
    "                                                                                   precision,\n",
    "                                                                                   recall,\n",
    "                                                                                   round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LR -- Accuracy: 0.775 / Precision: 0.712 / Recall: 0.646 / Latency: 1.3ms\nSVM -- Accuracy: 0.747 / Precision: 0.672 / Recall: 0.6 / Latency: 1.7ms\nMLP -- Accuracy: 0.781 / Precision: 0.717 / Recall: 0.662 / Latency: 1.7ms\nRF -- Accuracy: 0.803 / Precision: 0.788 / Recall: 0.631 / Latency: 5.6ms\nGB -- Accuracy: 0.815 / Precision: 0.808 / Recall: 0.646 / Latency: 2.5ms\n"
     ]
    }
   ],
   "source": [
    "for name, mdl in models.items():\n",
    "    evaluate_model(name, mdl, val_features, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LR -- Accuracy: 0.827 / Precision: 0.846 / Recall: 0.724 / Latency: 1.6ms\nSVM -- Accuracy: 0.799 / Precision: 0.794 / Recall: 0.711 / Latency: 1.8ms\nMLP -- Accuracy: 0.816 / Precision: 0.831 / Recall: 0.711 / Latency: 1.6ms\nRF -- Accuracy: 0.821 / Precision: 0.867 / Recall: 0.684 / Latency: 5.5ms\nGB -- Accuracy: 0.816 / Precision: 0.852 / Recall: 0.684 / Latency: 2.8ms\n"
     ]
    }
   ],
   "source": [
    "for name, mdl in models.items():\n",
    "    evaluate_model(name, mdl, te_features, te_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}